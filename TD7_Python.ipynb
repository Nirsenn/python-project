{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "id": "KxfHH4z4trML"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: praw in c:\\users\\albar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (7.8.1)\n",
            "Requirement already satisfied: prawcore<3,>=2.4 in c:\\users\\albar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from praw) (2.4.0)\n",
            "Requirement already satisfied: update_checker>=0.18 in c:\\users\\albar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in c:\\users\\albar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from praw) (1.9.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in c:\\users\\albar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from prawcore<3,>=2.4->praw) (2.32.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\albar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\albar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\albar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\albar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.10.5)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xmltodict in c:\\users\\albar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.0.2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        " #Requirements :\n",
        "!pip install praw\n",
        "!pip install xmltodict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqCQ4cg0pghw"
      },
      "source": [
        "Le sujet choisi est : Machine Learning\n",
        "\n",
        "Partie 1  :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VePKbQON_JCq"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<module 'Document' from 'c:\\\\Users\\\\albar\\\\Desktop\\\\TD-Python\\\\Projet_Python\\\\TD7\\\\TD7\\\\Document.py'>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Importer le module\n",
        "import Document\n",
        "\n",
        "# Recharger le module si nécessaire (utile si déjà importé avant)\n",
        "import importlib\n",
        "importlib.reload(Document)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4D-zmju5trMO"
      },
      "outputs": [],
      "source": [
        "#Reddit\n",
        "import praw\n",
        "from datetime import datetime\n",
        "import Author\n",
        "\n",
        "#instanciation de l'authentification aux outils API de Reddit\n",
        "reddit = praw.Reddit(client_id='WhOeUX8xCa_LSqqzHogeNA', client_secret='IouRe-5putFpKpyr11rOW7Wzh2Rpmw', user_agent='Web Scrapping')\n",
        "\n",
        "#Les 10 posts les plus tendance du subreddit MachineLearning :\n",
        "hot_posts = reddit.subreddit('MachineLearning').hot(limit=10)\n",
        "\n",
        "#variables principales\n",
        "docs = []                 #stocke les données d'un article (indice 0 → titre, 1 → auteur, 2 → date de publication, 3 → url, 4 → contenu textuel)\n",
        "origines = []             #tableau indiquant l'origine du document (Reddit ou Arxiv)\n",
        "id2doc = {}               #Clés : id du document, Valeurs : objet Document\n",
        "id2aut = {}               #Clés : noms des auteurs, Valeurs : id des documents publiés\n",
        "id = 1                    #id du document\n",
        "\n",
        "#pour chaque post Reddit\n",
        "for posts in hot_posts:\n",
        "  docs.append(posts.title)                #ajout du titre dans docs\n",
        "  auteurs = posts.author.name             #ajout du nom d'auteur dans docs\n",
        "  docs.append(auteurs)\n",
        "\n",
        "  texte = posts.selftext                  #contenu texte\n",
        "  texte = texte.replace(\"\\n\", \" \")                #formatage du texte pour remplacer les sauts de ligne \\n par un espace\n",
        "  texte = texte.replace(\"\\t\", \" \")\n",
        "  texte = texte.replace(\";\", \" \")\n",
        "  texte = texte.replace(\"&#x200B\", \"\")\n",
        "  if texte == \"\":                         #Pour les contenu textuels vides, ils seront rreprésentés par un espace afin de pouvoir être traités par les fonctions du corpus\n",
        "    texte = \" \"\n",
        "\n",
        "\n",
        "  if auteurs not in id2aut.keys():        #vérification de la présence de l'auteur dans id2aut\n",
        "    id2aut[auteurs] = Author.Author(auteurs)  #instanciation de l'objet Author avec le nouvel auteur\n",
        "    id2aut[auteurs].add(id, texte)\n",
        "  else:\n",
        "    id2aut[auteurs].add(id, texte)           #ajout du document à la production de l'auteur\n",
        "\n",
        "  dateP = posts.created_utc               #ajout de la date dans docs au format unix\n",
        "  docs.append(datetime.fromtimestamp(dateP))\n",
        "\n",
        "  docs.append(posts.url)                  #ajout de l'url dans docs\n",
        "\n",
        "  docs.append(texte)                      #ajout du contenu texte dans docs\n",
        "  docs.append(posts.num_comments)         #ajout du nombre commentaire dans docs\n",
        "\n",
        "  docs.append(\"Reddit\")                   #récupération de l'origine du post\n",
        "\n",
        "  #Une fois toutes les données du post récupérées, on instancie l'objet Document dans id2doc avec un id unique\n",
        "  id2doc[id] = Document.RedditDocument(docs[0], docs[1], docs[2], docs[3], docs[4], docs[5], docs[6])\n",
        "  id+=1                                   #incrémentation de l'id\n",
        "  docs = []                               #on vide le tableau pour pouvoir ajouter les données du document suivant\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqDPvnS7trMQ"
      },
      "source": [
        "Champs disponibles pour Reddit API :\n",
        "* 'title' (titre du post)\n",
        "* 'score' (nombre d'upvotes)\n",
        "* 'id' (identifiant du post, contenu dans l'url)\n",
        "* 'subreddit' (communauté)\n",
        "* 'url' (lien dans le post)\n",
        "* 'num_comments' (nb commentaires)\n",
        "* 'body' (contenu commentaire)\n",
        "* 'selftext' (contenu du post si c'est textuel)\n",
        "* 'author' (nom de l'utilisateur auteur du post)\n",
        "* created_utc (heure de publication du post)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "diMBjea_trMS"
      },
      "outputs": [],
      "source": [
        "#Arxiv\n",
        "import requests\n",
        "import xmltodict\n",
        "import dateutil.parser as dp\n",
        "\n",
        "#url contenant la requete (ici, 10 articles résultant de la recherche des mots Machine et Learning).\n",
        "url = 'http://export.arxiv.org/api/query?search_query=all:machine+learning&start=0&max_results=10'\n",
        "\n",
        "data = requests.get(url)                      #Obtention des données au format XML à partir de la requête\n",
        "articles = xmltodict.parse(data.text)         #données converties du format XML à dictionnaire\n",
        "\n",
        "#parcours des articles\n",
        "entries = articles['feed']['entry']\n",
        "for entry in entries:\n",
        "  a2 = [] #tableau des coauteurs\n",
        "  docs.append(entry.get(\"title\"))             #ajout du titre dans docs\n",
        "\n",
        "  summary = entry['summary']                  #contenu textuel\n",
        "  summary = summary.replace(\"\\n\", \" \")\n",
        "  summary = summary.replace(\"\\t\", \" \")                  #formatage du texte pour remplacer les sauts de ligne \\n par un espace\n",
        "  summary = summary.replace(\";\", \" \")\n",
        "  summary = summary.replace(\"&#x200B\", \"\")\n",
        "\n",
        "  authors = entry.get(\"author\", [])           #ajout de l'auteur dans docs\n",
        "  if type(authors) is dict:                   #si un seul auteur\n",
        "    a = authors.get(\"name\") #a = premier auteur\n",
        "    if a not in id2aut.keys():                #vérification de la présence de l'auteur dans id2aut\n",
        "      id2aut[a] = Author.Author(a)  #instanciation de l'objet AUthor avec le nouvel auteur\n",
        "      id2aut[a].add(id, summary)\n",
        "    else:\n",
        "      id2aut[a].add(id, summary)\n",
        "    docs.append(a)\n",
        "  else:                                       #si plusieurs auteurs\n",
        "    docs.append(authors[0].get(\"name\"))\n",
        "    for au in authors:\n",
        "      a2.append(au.get(\"name\"))\n",
        "      if au.get(\"name\") not in id2aut.keys(): #vérification de la présence de l'auteur dans id2aut\n",
        "        id2aut[au.get(\"name\")] = Author.Author(a2)\n",
        "        id2aut[au.get(\"name\")].add(id, summary)\n",
        "      else:\n",
        "        id2aut[au.get(\"name\")].add(id, summary)\n",
        "    a2 = a2[1:]                              #on met la lste des co-auteurs dans a2 (donc tous sauf le premier element)\n",
        "\n",
        "  dateP = entry.get(\"published\")              #ajout de la date de publication dans docs au format unix\n",
        "  dateP = dp.parse(dateP)\n",
        "  dateP = dateP.timestamp()\n",
        "  docs.append(datetime.fromtimestamp(dateP))\n",
        "\n",
        "  url = entry.get(\"id\")                       #ajout de l'url dans docs\n",
        "  docs.append(url)\n",
        "\n",
        "  docs.append(summary)                        #ajout du contenu textuel dans docs\n",
        "\n",
        "  docs.append(a2)\n",
        "\n",
        "  docs.append(\"Arxiv\")                        #récupération de l'origine du post\n",
        "\n",
        "  #Une fois toutes les données du post récupérées, on instancie l'objet Document dans id2doc avec un id unique\n",
        "  id2doc[id] = Document.ArxivDocument(docs[0], docs[1], docs[2], docs[3], docs[4], docs[5], docs[6])\n",
        "  docs = []                                   ##on vide le tableau pour pouvoir ajouter les données du document suivant\n",
        "  id+=1                                       #incrémentation de l'id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjmzF6OrK084"
      },
      "source": [
        "Champs disponibles pour Arxiv :\n",
        "\n",
        "* author/name (nom d'un auteur de l'article)\n",
        "* title (titre de l'article)\n",
        "* id (identifiant de l'article)\n",
        "* published (date de première publication)\n",
        "* link (lien de l'article)\n",
        "* update (date de mise à jour de l'article)\n",
        "* summary (sommaire du contenu de l'article)\n",
        "\n",
        "\n",
        "Classe Document :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQYY1I5s-u3H",
        "outputId": "41aef19d-2190-459f-d2c3-fd1c1d24d5a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reddit\n"
          ]
        }
      ],
      "source": [
        "#test avec le 5ème document sur 20\n",
        "d1 = id2doc[5]\n",
        "print(d1.type)   #affiche les attributs de l'objet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bHwaU5sNA-4"
      },
      "source": [
        "Classe Author :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        },
        "id": "2cm9iL1d3pLY",
        "outputId": "a1b47b7a-e5c1-4c12-dac1-6ffab749cd4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Liste complète des auteurs :\n",
            "- Alejandro Guerra-Manzanares\n",
            "- Anindya Iqbal\n",
            "- AutoModerator\n",
            "- Binxin Ru\n",
            "- Catholijn M. Jonker\n",
            "- Cedric De Boom\n",
            "- CommunityTough1\n",
            "- Cool-Statistician880\n",
            "- Dario Garcia-Gasulla\n",
            "- David J Wales\n",
            "- Davide Cacciarelli\n",
            "- Diego Granziol\n",
            "- Dmytro Chernyshov\n",
            "- Dmytro Fishman\n",
            "- Dmytro Sytnikov\n",
            "- Emergency-Cobbler137\n",
            "- Farah E. Shamout\n",
            "- Felix Mohr\n",
            "- Foreign_Fee_5859\n",
            "- Fotis E. Psomopoulos\n",
            "- Gianluca Pollastri\n",
            "- Gias Uddin\n",
            "- Ian Walsh\n",
            "- Jan N. van Rijn\n",
            "- Jen Harrow\n",
            "- Joost Broekens\n",
            "- Junaed Younus Khan\n",
            "- L. Julian Lechuga Lopez\n",
            "- Maximilian P Niroomand\n",
            "- Md. Tawkat Islam Khondaker\n",
            "- Michael Osborne\n",
            "- Michael Reusens\n",
            "- Michail Maniatakos\n",
            "- Murat Kulahci\n",
            "- Norqj\n",
            "- Olga Cherednichenko\n",
            "- PhotographOld9150\n",
            "- Polina Sytnikova\n",
            "- Sadia Afroz\n",
            "- Silvio C. E. Tosatto\n",
            "- Stefan Zohren\n",
            "- Stephen Roberts\n",
            "- Substantial_Ring_895\n",
            "- The ELIXIR Machine Learning focus group\n",
            "- Thomas M. Moerland\n",
            "- Tiina Titma\n",
            "- Xiaowen Doing\n",
            "- tensorpool_tycho\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Nombre de documents produits : 1 - Taille moyenne des documents : 1394 caractères'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"Liste complète des auteurs :\")                                #affiche la liste des noms d'auteurs\n",
        "#tri des auteurs par ordre alphabétique\n",
        "auts = []\n",
        "for key in id2aut:\n",
        "  auts.append(key)\n",
        "auts.sort()\n",
        "\n",
        "#affichage de la liste\n",
        "for elt in auts:\n",
        "  print(\"-\", elt)\n",
        "\n",
        "verif = False\n",
        "print()\n",
        "while verif is False:                                                #boucle pour que l'utilisateur entre un nom d'auteur valide\n",
        "  nom  = str(input(\"Entrez un nom d'auteur parmis ceux proposés : \"))\n",
        "  if nom in id2aut.keys():\n",
        "    verif = True\n",
        "  else:\n",
        "    verif = False\n",
        "    print(\"Veuillez entrer un nom d'auteur valide parmi la liste proposée.\")\n",
        "    print()\n",
        "\n",
        "a1 = id2aut[nom]                                             #instanciation de l'auteur choisi\n",
        "a1.get_taille_moyenne_documents()                            #affichage des informations concernées"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UhA9-yPy_JCt"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<module 'Corpus' from 'c:\\\\Users\\\\albar\\\\Desktop\\\\TD-Python\\\\Projet_Python\\\\TD7\\\\TD7\\\\Corpus.py'>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Importer le module\n",
        "import Corpus\n",
        "\n",
        "# Recharger le module si nécessaire (utile si déjà importé avant)\n",
        "import importlib\n",
        "importlib.reload(Corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtyrR4uQgJrt",
        "outputId": "4bfa757b-d60b-4cd2-e93b-6769eb56ef39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corpus 'Machine Learning' : 20 documents, 48 auteurs\n",
            "\n",
            "Tri par date :\n",
            "- 2025-11-25 | [P] Knowledge Distillation: 97% Cost Reduction Distilling Claude Sonnet 4 → GPT-4.1-nano (98% Fidelity Retained) (Emergency-Cobbler137)\n",
            "- 2025-11-25 | [D] I built a reasoning pipeline that boosts 8B models using structured routing + verification (Cool-Statistician880)\n",
            "- 2025-11-25 | [R] Novel Relational Cross-Attention appears to best Transformers in spatial reasoning tasks (CommunityTough1)\n",
            "- 2025-11-25 | [R] is there a way to decide on a model architecture using pruning without using NAS? (PhotographOld9150)\n",
            "- 2025-11-25 | [P] Feedback/Usage of SAM (Segment Anything) (Norqj)\n",
            "\n",
            "Tri par titre\n",
            "- [D] I built a reasoning pipeline that boosts 8B models using structured routing + verification (Cool-Statistician880, 2025-11-25)\n",
            "- [D] ML conferences need to learn from AISTATS (Rant/Discussion) (Foreign_Fee_5859, 2025-11-23)\n",
            "- [D] Monthly Who's Hiring and Who wants to be Hired? (AutoModerator, 2025-10-31)\n",
            "- [D] Self-Promotion Thread (AutoModerator, 2025-11-02)\n",
            "- [P] Feedback/Usage of SAM (Segment Anything) (Norqj, 2025-11-25)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Instanciation de corpus\n",
        "corpus = Corpus.Corpus(\"Machine Learning\", id2aut, id2doc)\n",
        "print(corpus)\n",
        "print()\n",
        "corpus.afficher_par_date(5)\n",
        "print()\n",
        "corpus.afficher_par_titre(5)\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JL-v6EtR_JCu"
      },
      "source": [
        "Tester le singleton"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8-z9aKT_JCu",
        "outputId": "29383d5d-0bc0-4d15-a3b3-db583615fd00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "corpus2 = Corpus.Corpus(\"Machine_Learning_2\", id2aut, id2doc)\n",
        "print(corpus is corpus2) # retourne True si la même instance est crée (verifie qu'il est singleton)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-J2curF_JCu"
      },
      "source": [
        "Générateur de documents grâce à un patron de conception d’usine (factory pattern)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mvZqTVCV_JCu"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<module 'DocumentFactory' from 'c:\\\\Users\\\\albar\\\\Desktop\\\\TD-Python\\\\Projet_Python\\\\TD7\\\\TD7\\\\DocumentFactory.py'>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Importer le module\n",
        "import DocumentFactory\n",
        "\n",
        "# Recharger le module si nécessaire (utile si déjà importé avant)\n",
        "import importlib\n",
        "importlib.reload(DocumentFactory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2yXFbJG_JCu"
      },
      "source": [
        "Tester le generateur avec reddit et arxiv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "GakETHBc_JCu"
      },
      "outputs": [],
      "source": [
        "from DocumentFactory import DocumentFactory  # importe la classe\n",
        "\n",
        "#créer un nouveau document Arxiv\n",
        "id2doc[id] = DocumentFactory.createDocument(\"arxiv\", titre=\"Le sujet du monde IA\", auteur=\"Aissatou\",\n",
        "                                            date=datetime.today(),url=\"https://arxiv.com/xyz\",\n",
        "                                            texte=\"loremmmmmmmmmm, je fais le tour du monde Texte du post.lom..\",\n",
        "                                            extra=[\"Bob\", \"Charlie\"])\n",
        "id+=1                                       #incrémentation de l'id\n",
        "\n",
        "#créer un nouveau document Reddit\n",
        "id2doc[id] = DocumentFactory.createDocument(\"reddit\", titre=\"Le tour du monde IA\", auteur=\"Barry\",\n",
        "                                            date=datetime.today(),url=\"https://reddit.com/xyz\",\n",
        "                                            texte=\"je fais le tour du monde Texte du post.lom..\",\n",
        "                                            extra=50)\n",
        "id+=1                                       #incrémentation de l'id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "rzxYlgrA_JCv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Titre : [R] Novel Relational Cross-Attention appears to best Transformers in spatial reasoning tasks\n",
            "Auteur : CommunityTough1\n",
            "Date : 2025-11-25\n",
            "Url : https://www.reddit.com/r/MachineLearning/comments/1p6cz0f/r_novel_relational_crossattention_appears_to_best/\n",
            "Texte : Repo (MIT): [https://github.com/clowerweb/relational-cross-attention](https://github.com/clowerweb/r...\n",
            "Nombre de commentaires : 0\n",
            "Type : Reddit\n"
          ]
        }
      ],
      "source": [
        "d2 = corpus.id2doc[5]     #test avec le dernier document créé\n",
        "print(d2)   #affiche les attributs de l'objet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JztROrFK62w0",
        "outputId": "88fecedb-f69a-4911-cdd2-0fec7a303015"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corpus 'Machine Learning' : 20 documents, 48 auteurs\n",
            "La Chaine de caractere unique contenant tous les documents : \n",
            " Please post your personal projects, startups, product placements, collaboration needs, blogs etc.  Please mention the payment and pricing requirements for products and services.  Please do not post link shorteners, link aggregator websites , or auto-subscribe links.  \\--  Any abuse of trust will lead to bans.  Encourage others who create new posts for questions to post here instead!  Thread will stay alive until next one so keep posting after the date in the title.  \\--  Meta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads.**For Job Postings** please use this template  >Hiring: \\[Location\\], Salary:\\[\\], \\[Remote | Relocation\\], \\[Full Time | Contract | Part Time\\]    and \\[Brief overview, what you're looking for\\]  **For Those looking for jobs** please use this template  >Want to be Hired: \\[Location\\], Salary Expectation:\\[\\], \\[Remote | Relocation\\], \\[Full Time | Contract | Part Time\\]  Resume: \\[Link to resume\\] and \\[Brief overview, what you're looking for\\]     Please remember that this community is geared towards those with experience.**TL DR**: Fine-tuned GPT-4.1-nano achieved 98% of Claude Sonnet 4's quality (0.784 vs 0.795) on structured reasoning tasks while reducing inference cost from $45/1k to $1.30/1k and P90 latency from 25s to 2.5s. Open-source alternatives (Qwen3-Coder-30B, Llama-3.1-8B) underperformed despite larger parameter counts, primarily due to instruction-following weaknesses.  # Problem  Transforming algorithmic problems into structured JSON interview scenarios. Claude Sonnet 4 delivered 0.795 quality but cost $45/1k requests with 25s P90 latency.  **Challenge**: Maintain quality while achieving production-viable economics.  # Approach  **Teacher Selection**:  * Tested: Claude Sonnet 4, GPT-5, Gemini 2.5 Pro * Winner: Claude Sonnet 4 (0.795) due to superior parsing quality (0.91) and algorithmic correctness (0.95) * Evaluation: LLM-as-a-judge ensemble across 6 dimensions * *Note: Circular evaluation bias exists (Claude as both teacher/judge), but judges scored independently*  **Data Generation**:  * Generated 7,500 synthetic examples (combinatorial: 15 companies × 100 problems × 5 roles) * **Critical step**: Programmatic validation rejected 968 examples (12.7%) * Rejection criteria: schema violations, hallucinated constraints, parsing failures * Final training set: 6,532 examples  **Student Comparison**:  |Model|Method|Quality|Cost/1k|Key Failure Mode| |:-|:-|:-|:-|:-| |Qwen3-Coder-30B|LoRA (r=16)|0.710|$5.50|Negative constraint violations| |Llama-3.1-8B|LoRA (r=16)|0.680|$2.00|Catastrophic forgetting (24% parse failures)| |**GPT-4.1-nano**|**API Fine-tune**|**0.784**|**$1.30**|**Role specificity weakness**|  # Results  **GPT-4.1-nano Performance**:  * Quality: 0.784 (98% of teacher's 0.795) * Cost: $1.30/1k (97% reduction from $45/1k) * Latency: 2.5s P90 (10x improvement from 25s) * Parsing success: 92.3%  **Performance by Dimension**:  * Algorithmic correctness: 0.98 (exceeds teacher) * Parsing quality: 0.92 (matches teacher) * Technical accuracy: 0.89 (exceeds teacher) * Company relevance: 0.75 * Role specificity: 0.57 (main weakness) * Scenario realism: 0.60  # Key Insights  1. **Model Size ≠ Quality**: GPT-4.1-nano (rumored \\~7B parameters) beat 30B Qwen3-Coder by 7.4 points. Pre-training for instruction-following matters more than parameter count. 2. **Data Quality Critical**: 12.7% rejection rate was essential. Without data filtering, parsing failures jumped to 35% (vs 7.7% with filtering). A 4.5× increase. 3. **Code-Completion vs Instruction-Following**: Qwen3-Coder's pre-training bias toward code completion interfered with strict constraint adherence, despite larger size. 4. **Catastrophic Forgetting**: Llama-3.1-8B couldn't maintain JSON syntax knowledge while learning new task (24% parse failures).  # Economics  * Setup: $351 (data generation + fine-tuning) * Break-even: \\~8K inferences (achieved in \\~3 weeks) * **12-month cumulative savings**: >$10,000 (volume scaling from 10K to 75K/month)  # Questions for Community  1. How do you handle circular evaluation when teacher is part of judge ensemble? 2. Any architectural techniques to improve negative constraint adherence in fine-tuned models? 3. Why do code-specialized models struggle with strict instruction-following?  **Reproducibility**: Full methodology + charts: [https://www.algoirl.ai/engineering-notes/distilling-intelligence](https://www.algoirl.ai/engineering-notes/distilling-intelligence)  Happy to discuss evaluation methodology, training details, or failure modes!This is a project I’ve been working on quietly for a while, and I finally feel confident enough to share the core idea. It’s a lightweight reasoning and verification pipeline designed to make small local models (7B–13B) behave much more reliably by giving them structure, not scale.  The architecture has three main parts:  1. **Intent understanding**    Before the model does anything, an intent classifier figures out what type of request the user is making: news, explanation, or problem-solving.    Instead of treating all prompts the same, the model is routed into the correct mode from the beginning.  2. **Structured execution paths**    Each “mode” has its own reasoning pipeline:    • For news → multi-source search + aggregation      • For explanations → layered reasoning chain      • For problem solving → step-by-step logic + symbolic checks      This removes ambiguity and forces predictable behavior – a big deal for small models.  3. **Verification + automatic correction**    After generating an answer, the pipeline verifies it against external signals:    • Cross-source consistency      • Internal reasoning coherence      • Pattern-based self-checks      If verification fails, it automatically regenerates a corrected answer.  The goal isn’t to “trick” models into looking smart.   The goal is to give small models the *software architecture* they need to behave like bigger models: dedicated routes, clear roles, and a second layer of quality control.  Early testers reported that a basic 8B model felt noticeably “larger” when run through this pipeline — not because the model changed, but because the *surrounding system* did.  I’ll post the full code, examples, and benchmarks in the first comment (to comply with Rule 5).   If anyone here tries it, I’d genuinely love to know how it behaves with your local LLM setups. Feedback, improvements, or edge cases are all welcome.  Happy to answer any technical questions about the routing logic, verification design, or implementation details.Repo (MIT): [https://github.com/clowerweb/relational-cross-attention](https://github.com/clowerweb/relational-cross-attention)  Quick rundown:  A novel neural architecture for few-shot learning of transformations that outperforms standard transformers by **30% relative improvement** while being **17% faster**.  ## Key Results  | Model | Unseen Accuracy | Speed | Gap vs Standard | |-------|----------------|-------|-----------------| | **Relational (Ours)** | **16.12%** | **24.8s** | **+3.76%** | | Standard Transformer | 12.36% | 29.7s | baseline |  ### Per-Transform Breakdown (Unseen)  | Transform | Standard | Relational | Improvement | |-----------|----------|------------|-------------| | flip_vertical | 10.14% | **16.12%** | +5.98% | | rotate_180 | 10.33% | **15.91%** | +5.58% | | translate_down | 9.95% | **16.20%** | +6.25% | | invert_colors | 20.07% | **20.35%** | +0.28% |  **The relational model excels at spatial reasoning while maintaining strong color transform performance.**  7M params model scores 2.5% on epoch 1 and 2.8% in 5 epochs on ARC-AGI. After 5 epochs, performance starts to slip, likely due to overfitting (I think the model is just too small, and I don't have the hardware to run ARC-AGI with a bigger one). I'd also love to see what this algorithm might do for LLMs, so I may train a TinyStories SLM over the weekend (it'll probably take several days on my hardware). Welcoming any feedback!I have a data of size 16k where each sample is a matrix of 4*8 mapping to two values as output and the output of the model will be regression. I want to find an  architecture  which max contains 2 conv2d layer and 3 dense layer with max 80 nodes er layer, won't pruning the overparameterized model help?  How will you fix a model architecture without over fitting it? How will I decide how many conv2d layer needed and dense layer needed without using NAS? Coz NAS even for slightest improvement will give the model with max number of cov2d layers and max number of dense layers. I don't want NAS to select the one with the highest number of attribute. I want to select a model which has approx 1600 attributes with not very high drop in frequency compared to a model with 35k attribute.Hi folks!  I'm one of the maintainers of [Pixeltable](https://github.com/pixeltable/pixeltable) and we are looking to provide a built-in support for [SAM](https://github.com/facebookresearch/segment-anything) (Segment Anything) and I'd love to chat with people who are using it on a daily/weekly basis and what their workflows look like.  Pixeltable is quite unique in the way that we can provide an API/Dataframe/Engine to manipulate video/frames/arrays/json as first-class data types to work with among other things which makes it very unique programmatically to work with SAM outputs/masks.  Feel free to reply here/DM me or others :)  Thanks and really appreciated!Pretty crazy feat. the zELO approach is super impressive. thoughts?  [https://tensorpool.dev/blog/zeroentropy-zerank-training?utm\\_source=reddit](https://tensorpool.dev/blog/zeroentropy-zerank-training?utm_source=reddit)If anyone used PP-OCR VL could you help me with installation ? I tried several times with different ways and I faced a lot of issues that can not solve.   Also I created new environment and tried, but failed, tried on Colab, but failed, even with AWS EC2 but there are a lot of not understandable issues.  My machine is Ubuntu 24.04 with GTX 1660TI and 16 GB RAM.  I really appreciate your helpQuick rant. As many have noticed and experienced, the quality of reviews at large conferences such as ICLR, ICML. AAAI, NIPS, has generally been very inconsistent with several people getting low quality or even AI written reviews. While this is not too shocking given the number of submissions and lack of reviewers changes need to be made.  Based on my experience and a general consensus by other researchers, AISTATS is the ML conference with the highest quality of reviews. Their approach to reviewing makes a lot more sense and is more similar to other scientific fields and i believe the other ML conferences should learn from them.  For example: 1) they dont allow for any LLMs when writing reviews and they flag any reviews that have even a small chance of being AI written (i think everyone should do this) 2) they follow a structured reviewing format making it much easier to compare the different reviewers points. 3) Reviews are typically shorter and focus on key concerns making it easier to pin point what you should adress.   While AISTATS also isn't perfect in my experience it feels less \"random\" than other venues and usually I'm sure the reviewers have actually read my work. Their misunderstandingd are also usually more \"acceptable\".Data science has become increasingly essential for the production of official statistics, as it enables the automated collection, processing, and analysis of large amounts of data. With such data science practices in place, it enables more timely, more insightful and more flexible reporting. However, the quality and integrity of data-science-driven statistics rely on the accuracy and reliability of the data sources and the machine learning techniques that support them. In particular, changes in data sources are inevitable to occur and pose significant risks that are crucial to address in the context of machine learning for official statistics.   This paper gives an overview of the main risks, liabilities, and uncertainties associated with changing data sources in the context of machine learning for official statistics. We provide a checklist of the most prevalent origins and causes of changing data sources  not only on a technical level but also regarding ownership, ethics, regulation, and public perception. Next, we highlight the repercussions of changing data sources on statistical reporting. These include technical effects such as concept drift, bias, availability, validity, accuracy and completeness, but also the neutrality and potential discontinuation of the statistical offering. We offer a few important precautionary measures, such as enhancing robustness in both data sourcing and statistical techniques, and thorough monitoring. In doing so, machine learning-based official statistics can maintain integrity, reliability, consistency, and relevance in policy-making, decision-making, and public discourse.Modern biology frequently relies on machine learning to provide predictions and improve decision processes. There have been recent calls for more scrutiny on machine learning performance and possible limitations. Here we present a set of community-wide recommendations aiming to help establish standards of supervised machine learning validation in biology. Adopting a structured methods description for machine learning based on data, optimization, model, evaluation (DOME) will aim to help both reviewers and readers to better understand and assess the performance and limitations of a method or outcome. The recommendations are formulated as questions to anyone wishing to pursue implementation of a machine learning algorithm. Answers to these questions can be easily included in the supplementary material of published papers.Learning curves are a concept from social sciences that has been adopted in the context of machine learning to assess the performance of a learning algorithm with respect to a certain resource, e.g., the number of training examples or the number of training iterations. Learning curves have important applications in several machine learning contexts, most notably in data acquisition, early stopping of model training, and model selection. For instance, learning curves can be used to model the performance of the combination of an algorithm and its hyperparameter configuration, providing insights into their potential suitability at an early stage and often expediting the algorithm selection process. Various learning curve models have been proposed to use learning curves for decision making. Some of these models answer the binary decision question of whether a given algorithm at a certain budget will outperform a certain reference performance, whereas more complex models predict the entire learning curve of an algorithm. We contribute a framework that categorises learning curve approaches using three criteria: the decision-making situation they address, the intrinsic learning curve question they answer and the type of resources they use. We survey papers from the literature and classify them into this framework.Online active learning is a paradigm in machine learning that aims to select the most informative data points to label from a data stream. The problem of minimizing the cost associated with collecting labeled observations has gained a lot of attention in recent years, particularly in real-world applications where data is only available in an unlabeled form. Annotating each observation can be time-consuming and costly, making it difficult to obtain large amounts of labeled data. To overcome this issue, many active learning strategies have been proposed in the last decades, aiming to select the most informative observations for labeling in order to improve the performance of machine learning models. These approaches can be broadly divided into two categories: static pool-based and stream-based active learning. Pool-based active learning involves selecting a subset of observations from a closed pool of unlabeled data, and it has been the focus of many surveys and literature reviews. However, the growing availability of data streams has led to an increase in the number of approaches that focus on online active learning, which involves continuously selecting and labeling observations as they arrive in a stream. This work aims to provide an overview of the most recently proposed approaches for selecting the most informative observations from data streams in real time. We review the various techniques that have been proposed and discuss their strengths and limitations, as well as the challenges and opportunities that exist in this area of research.The ability to explain decisions made by machine learning models remains one of the most significant hurdles towards widespread adoption of AI in highly sensitive areas such as medicine, cybersecurity or autonomous driving. Great interest exists in understanding which features of the input data prompt model decision making. In this contribution, we propose a novel approach to identify relevant features of the input data, inspired by methods from the energy landscapes field, developed in the physical sciences. By identifying conserved weights within groups of minima of the loss landscapes, we can identify the drivers of model decision making. Analogues to this idea exist in the molecular sciences, where coordinate invariants or order parameters are employed to identify critical features of a molecule. However, no such approach exists for machine learning loss landscapes. We will demonstrate the applicability of energy landscape methods to machine learning models and give examples, both synthetic and from the real world, for how these methods can help to make models more interpretable.Machine Learning (ML) has recently shown tremendous success in modeling various healthcare prediction tasks, ranging from disease diagnosis and prognosis to patient treatment. Due to the sensitive nature of medical data, privacy must be considered along the entire ML pipeline, from model training to inference. In this paper, we conduct a review of recent literature concerning Privacy-Preserving Machine Learning (PPML) for healthcare. We primarily focus on privacy-preserving training and inference-as-a-service, and perform a comprehensive review of existing trends, identify challenges, and discuss opportunities for future research directions. The aim of this review is to guide the development of private and efficient ML models in healthcare, with the prospects of translating research efforts into real-world settings.The proliferation of fake news and its propagation on social media has become a major concern due to its ability to create devastating impacts. Different machine learning approaches have been suggested to detect fake news. However, most of those focused on a specific type of news (such as political) which leads us to the question of dataset-bias of the models used. In this research, we conducted a benchmark study to assess the performance of different applicable machine learning approaches on three different datasets where we accumulated the largest and most diversified one. We explored a number of advanced pre-trained language models for fake news detection along with the traditional and deep learning ones and compared their performances from different aspects for the first time to the best of our knowledge. We find that BERT and similar pre-trained models perform the best for fake news detection, especially with very small dataset. Hence, these models are significantly better option for languages with limited electronic contents, i.e., training data. We also carried out several analysis based on the models' performance, article's topic, article's length, and discussed different lessons learned from them. We believe that this benchmark study will help the research community to explore further and news sites/blogs to select the most appropriate fake news detection method.This article provides the first survey of computational models of emotion in reinforcement learning (RL) agents. The survey focuses on agent/robot emotions, and mostly ignores human user emotions. Emotions are recognized as functional in decision-making by influencing motivation and action selection. Therefore, computational emotion models are usually grounded in the agent's decision making architecture, of which RL is an important subclass. Studying emotions in RL-based agents is useful for three research fields. For machine learning (ML) researchers, emotion models may improve learning efficiency. For the interactive ML and human-robot interaction (HRI) community, emotions can communicate state and enhance user investment. Lastly, it allows affective modelling (AM) researchers to investigate their emotion theories in a successful AI agent class. This survey provides background on emotion theory and RL. It systematically addresses 1) from what underlying dimensions (e.g., homeostasis, appraisal) emotions can be derived and how these can be modelled in RL-agents, 2) what types of emotions have been derived from these dimensions, and 3) how these emotions may either influence the learning efficiency of the agent or be useful as social signals. We also systematically compare evaluation criteria, and draw connections to important RL sub-domains like (intrinsic) motivation and model-based RL. In short, this survey provides both a practical overview for engineers wanting to implement emotions in their RL agents, and identifies challenges and directions for future emotion-RL research.Efficient approximation lies at the heart of large-scale machine learning problems. In this paper, we propose a novel, robust maximum entropy algorithm, which is capable of dealing with hundreds of moments and allows for computationally efficient approximations. We showcase the usefulness of the proposed method, its equivalence to constrained Bayesian variational inference and demonstrate its superiority over existing approaches in two applications, namely, fast log determinant estimation and information-theoretic Bayesian optimisation.This research paper delves into the innovative integration of Shannon entropy and rough set theory, presenting a novel approach to generalize the evaluation approach in machine learning. The conventional application of entropy, primarily focused on information uncertainty, is extended through its combination with rough set theory to offer a deeper insight into data's intrinsic structure and the interpretability of machine learning models. We introduce a comprehensive framework that synergizes the granularity of rough set theory with the uncertainty quantification of Shannon entropy, applied across a spectrum of machine learning algorithms. Our methodology is rigorously tested on various datasets, showcasing its capability to not only assess predictive performance but also to illuminate the underlying data complexity and model robustness. The results underscore the utility of this integrated approach in enhancing the evaluation landscape of machine learning, offering a multi-faceted perspective that balances accuracy with a profound understanding of data attributes and model dynamics. This paper contributes a groundbreaking perspective to machine learning evaluation, proposing a method that encapsulates a holistic view of model performance, thereby facilitating more informed decision-making in model selection and application.loremmmmmmmmmm, je fais le tour du monde Texte du post.lom..je fais le tour du monde Texte du post.lom..\n"
          ]
        }
      ],
      "source": [
        "#Enregistre le corpus dans un fichier\n",
        "corpus.save(\"data.csv\")\n",
        "\n",
        "#Charge un fichier csv pour créer Dataframe du corpus\n",
        "newcorps = corpus.load('data.csv')\n",
        "print(newcorps)\n",
        "\n",
        "texte_join=\"\"\n",
        "#Créé une chaine de caractères unique contenant tous les documents grâce à la fonction join et le séparateur \" \"\n",
        "for texte in newcorps.id2doc.values():\n",
        "  texte_join += texte.texte #rassemble le texte de chaque document\n",
        "\n",
        "print('La Chaine de caractere unique contenant tous les documents : \\n', texte_join)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_vqpMCEHBtw",
        "outputId": "8f2ee6a4-5617-43ac-a578-bcf03838c542"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 : e getting low quality or even AI written reviews. While this i\n",
            "2 :  even a small chance of being AI written (i think everyone sho\n",
            "3 : owards widespread adoption of AI in highly sensitive areas suc\n",
            "4 : tion theories in a successful AI agent class. This survey prov\n",
            "                    contexte gauche motif trouvé                    contexte droit\n",
            "0  ...e getting low quality or even           AI  written reviews. While this i...\n",
            "1  ... even a small chance of being           AI  written (i think everyone sho...\n",
            "2  ...owards widespread adoption of           AI  in highly sensitive areas suc...\n",
            "3  ...tion theories in a successful           AI  agent class. This survey prov...\n"
          ]
        }
      ],
      "source": [
        "# tester la recherche d'un mot\n",
        "corpus.search(texte_join, \"AI\")\n",
        "\n",
        "# tester concorde\n",
        "corpus.concorde(texte_join, \"AI\", 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbL72fTVMhES"
      },
      "source": [
        "2.1 et 2.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3Cj0oGXMhES",
        "outputId": "719d53ec-a2a3-4d01-e46d-b03134329af8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nombre de mots différents : 1236\n",
            "Les 10 mots les plus fréquents :\n",
            "        mot  term frequency  document frequency\n",
            "0       the             132                  17\n",
            "1        of             104                  18\n",
            "2       and              98                  19\n",
            "3        to              90                  18\n",
            "4         a              66                  17\n",
            "5        in              55                  17\n",
            "6  learning              47                  12\n",
            "7       for              43                  17\n",
            "8      with              34                  15\n",
            "9      data              32                  11\n"
          ]
        }
      ],
      "source": [
        "# Tester les mots les plus frequents\n",
        "corpus.stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'please': {'id': 1, 'occurences': 6, 'docFreq': np.int64(2)}, 'post': {'id': 2, 'occurences': 6, 'docFreq': np.int64(4)}, 'your': {'id': 3, 'occurences': 3, 'docFreq': np.int64(3)}, 'personal': {'id': 4, 'occurences': 1, 'docFreq': np.int64(1)}, 'projects': {'id': 5, 'occurences': 1, 'docFreq': np.int64(1)}, 'startups': {'id': 6, 'occurences': 1, 'docFreq': np.int64(1)}, 'product': {'id': 7, 'occurences': 1, 'docFreq': np.int64(1)}, 'placements': {'id': 8, 'occurences': 1, 'docFreq': np.int64(1)}, 'collaboration': {'id': 9, 'occurences': 1, 'docFreq': np.int64(1)}, 'needs': {'id': 10, 'occurences': 1, 'docFreq': np.int64(1)}, 'blogs': {'id': 11, 'occurences': 2, 'docFreq': np.int64(2)}, 'etc': {'id': 12, 'occurences': 1, 'docFreq': np.int64(1)}, 'mention': {'id': 13, 'occurences': 1, 'docFreq': np.int64(1)}, 'the': {'id': 14, 'occurences': 132, 'docFreq': np.int64(17)}, 'payment': {'id': 15, 'occurences': 1, 'docFreq': np.int64(1)}, 'and': {'id': 16, 'occurences': 98, 'docFreq': np.int64(19)}, 'pricing': {'id': 17, 'occurences': 1, 'docFreq': np.int64(1)}, 'requirements': {'id': 18, 'occurences': 1, 'docFreq': np.int64(1)}, 'for': {'id': 19, 'occurences': 43, 'docFreq': np.int64(17)}, 'products': {'id': 20, 'occurences': 1, 'docFreq': np.int64(1)}, 'services': {'id': 21, 'occurences': 1, 'docFreq': np.int64(1)}, 'do': {'id': 22, 'occurences': 5, 'docFreq': np.int64(4)}, 'not': {'id': 23, 'occurences': 10, 'docFreq': np.int64(7)}, 'link': {'id': 24, 'occurences': 3, 'docFreq': np.int64(2)}, 'shorteners': {'id': 25, 'occurences': 1, 'docFreq': np.int64(1)}, 'aggregator': {'id': 26, 'occurences': 1, 'docFreq': np.int64(1)}, 'websites': {'id': 27, 'occurences': 1, 'docFreq': np.int64(1)}, 'or': {'id': 28, 'occurences': 12, 'docFreq': np.int64(9)}, 'auto': {'id': 29, 'occurences': 1, 'docFreq': np.int64(1)}, 'subscribe': {'id': 30, 'occurences': 1, 'docFreq': np.int64(1)}, 'links': {'id': 31, 'occurences': 1, 'docFreq': np.int64(1)}, 'any': {'id': 32, 'occurences': 6, 'docFreq': np.int64(5)}, 'abuse': {'id': 33, 'occurences': 1, 'docFreq': np.int64(1)}, 'of': {'id': 34, 'occurences': 104, 'docFreq': np.int64(18)}, 'trust': {'id': 35, 'occurences': 1, 'docFreq': np.int64(1)}, 'will': {'id': 36, 'occurences': 11, 'docFreq': np.int64(6)}, 'lead': {'id': 37, 'occurences': 1, 'docFreq': np.int64(1)}, 'to': {'id': 38, 'occurences': 90, 'docFreq': np.int64(18)}, 'bans': {'id': 39, 'occurences': 1, 'docFreq': np.int64(1)}, 'encourage': {'id': 40, 'occurences': 2, 'docFreq': np.int64(1)}, 'others': {'id': 41, 'occurences': 2, 'docFreq': np.int64(2)}, 'who': {'id': 42, 'occurences': 2, 'docFreq': np.int64(2)}, 'create': {'id': 43, 'occurences': 2, 'docFreq': np.int64(2)}, 'new': {'id': 44, 'occurences': 3, 'docFreq': np.int64(3)}, 'posts': {'id': 45, 'occurences': 1, 'docFreq': np.int64(1)}, 'questions': {'id': 46, 'occurences': 5, 'docFreq': np.int64(4)}, 'here': {'id': 47, 'occurences': 4, 'docFreq': np.int64(4)}, 'instead': {'id': 48, 'occurences': 2, 'docFreq': np.int64(2)}, 'thread': {'id': 49, 'occurences': 1, 'docFreq': np.int64(1)}, 'stay': {'id': 50, 'occurences': 1, 'docFreq': np.int64(1)}, 'alive': {'id': 51, 'occurences': 1, 'docFreq': np.int64(1)}, 'until': {'id': 52, 'occurences': 1, 'docFreq': np.int64(1)}, 'next': {'id': 53, 'occurences': 2, 'docFreq': np.int64(2)}, 'one': {'id': 54, 'occurences': 6, 'docFreq': np.int64(6)}, 'so': {'id': 55, 'occurences': 3, 'docFreq': np.int64(3)}, 'keep': {'id': 56, 'occurences': 1, 'docFreq': np.int64(1)}, 'posting': {'id': 57, 'occurences': 1, 'docFreq': np.int64(1)}, 'after': {'id': 58, 'occurences': 3, 'docFreq': np.int64(3)}, 'date': {'id': 59, 'occurences': 1, 'docFreq': np.int64(1)}, 'in': {'id': 60, 'occurences': 55, 'docFreq': np.int64(17)}, 'title': {'id': 61, 'occurences': 1, 'docFreq': np.int64(1)}, 'meta': {'id': 62, 'occurences': 1, 'docFreq': np.int64(1)}, 'this': {'id': 63, 'occurences': 30, 'docFreq': np.int64(14)}, 'is': {'id': 64, 'occurences': 24, 'docFreq': np.int64(15)}, 'an': {'id': 65, 'occurences': 13, 'docFreq': np.int64(8)}, 'experiment': {'id': 66, 'occurences': 1, 'docFreq': np.int64(1)}, 'if': {'id': 67, 'occurences': 4, 'docFreq': np.int64(3)}, 'community': {'id': 68, 'occurences': 7, 'docFreq': np.int64(6)}, 'doesnt': {'id': 69, 'occurences': 1, 'docFreq': np.int64(1)}, 'like': {'id': 70, 'occurences': 4, 'docFreq': np.int64(4)}, 'we': {'id': 71, 'occurences': 25, 'docFreq': np.int64(12)}, 'cancel': {'id': 72, 'occurences': 1, 'docFreq': np.int64(1)}, 'it': {'id': 73, 'occurences': 19, 'docFreq': np.int64(9)}, 'those': {'id': 74, 'occurences': 4, 'docFreq': np.int64(3)}, 'promote': {'id': 75, 'occurences': 1, 'docFreq': np.int64(1)}, 'their': {'id': 76, 'occurences': 9, 'docFreq': np.int64(7)}, 'work': {'id': 77, 'occurences': 5, 'docFreq': np.int64(4)}, 'by': {'id': 78, 'occurences': 11, 'docFreq': np.int64(7)}, 'spamming': {'id': 79, 'occurences': 1, 'docFreq': np.int64(1)}, 'main': {'id': 80, 'occurences': 4, 'docFreq': np.int64(4)}, 'threads': {'id': 81, 'occurences': 1, 'docFreq': np.int64(1)}, 'job': {'id': 82, 'occurences': 1, 'docFreq': np.int64(1)}, 'postings': {'id': 83, 'occurences': 1, 'docFreq': np.int64(1)}, 'use': {'id': 84, 'occurences': 4, 'docFreq': np.int64(2)}, 'template': {'id': 85, 'occurences': 2, 'docFreq': np.int64(1)}, 'hiring': {'id': 86, 'occurences': 1, 'docFreq': np.int64(1)}, 'location': {'id': 87, 'occurences': 2, 'docFreq': np.int64(1)}, 'salary': {'id': 88, 'occurences': 2, 'docFreq': np.int64(1)}, 'remote': {'id': 89, 'occurences': 2, 'docFreq': np.int64(1)}, 'relocation': {'id': 90, 'occurences': 2, 'docFreq': np.int64(1)}, 'full': {'id': 91, 'occurences': 4, 'docFreq': np.int64(3)}, 'time': {'id': 92, 'occurences': 7, 'docFreq': np.int64(3)}, 'contract': {'id': 93, 'occurences': 2, 'docFreq': np.int64(1)}, 'part': {'id': 94, 'occurences': 3, 'docFreq': np.int64(2)}, 'brief': {'id': 95, 'occurences': 2, 'docFreq': np.int64(1)}, 'overview': {'id': 96, 'occurences': 5, 'docFreq': np.int64(4)}, 'what': {'id': 97, 'occurences': 8, 'docFreq': np.int64(6)}, 'you': {'id': 98, 'occurences': 6, 'docFreq': np.int64(5)}, 're': {'id': 99, 'occurences': 2, 'docFreq': np.int64(1)}, 'looking': {'id': 100, 'occurences': 5, 'docFreq': np.int64(3)}, 'jobs': {'id': 101, 'occurences': 1, 'docFreq': np.int64(1)}, 'want': {'id': 102, 'occurences': 4, 'docFreq': np.int64(2)}, 'be': {'id': 103, 'occurences': 11, 'docFreq': np.int64(8)}, 'hired': {'id': 104, 'occurences': 1, 'docFreq': np.int64(1)}, 'expectation': {'id': 105, 'occurences': 1, 'docFreq': np.int64(1)}, 'resume': {'id': 106, 'occurences': 2, 'docFreq': np.int64(1)}, 'remember': {'id': 107, 'occurences': 1, 'docFreq': np.int64(1)}, 'that': {'id': 108, 'occurences': 19, 'docFreq': np.int64(11)}, 'geared': {'id': 109, 'occurences': 1, 'docFreq': np.int64(1)}, 'towards': {'id': 110, 'occurences': 2, 'docFreq': np.int64(2)}, 'with': {'id': 111, 'occurences': 34, 'docFreq': np.int64(15)}, 'experience': {'id': 112, 'occurences': 3, 'docFreq': np.int64(2)}, 'tl': {'id': 113, 'occurences': 1, 'docFreq': np.int64(1)}, 'dr': {'id': 114, 'occurences': 1, 'docFreq': np.int64(1)}, 'fine': {'id': 115, 'occurences': 4, 'docFreq': np.int64(1)}, 'tuned': {'id': 116, 'occurences': 2, 'docFreq': np.int64(1)}, 'gpt': {'id': 117, 'occurences': 5, 'docFreq': np.int64(1)}, 'nano': {'id': 118, 'occurences': 4, 'docFreq': np.int64(1)}, 'achieved': {'id': 119, 'occurences': 2, 'docFreq': np.int64(1)}, 'claude': {'id': 120, 'occurences': 5, 'docFreq': np.int64(1)}, 'sonnet': {'id': 121, 'occurences': 4, 'docFreq': np.int64(1)}, 's': {'id': 122, 'occurences': 15, 'docFreq': np.int64(6)}, 'quality': {'id': 123, 'occurences': 14, 'docFreq': np.int64(4)}, 'vs': {'id': 124, 'occurences': 4, 'docFreq': np.int64(2)}, 'on': {'id': 125, 'occurences': 25, 'docFreq': np.int64(13)}, 'structured': {'id': 126, 'occurences': 5, 'docFreq': np.int64(4)}, 'reasoning': {'id': 127, 'occurences': 6, 'docFreq': np.int64(3)}, 'tasks': {'id': 128, 'occurences': 2, 'docFreq': np.int64(2)}, 'while': {'id': 129, 'occurences': 8, 'docFreq': np.int64(4)}, 'reducing': {'id': 130, 'occurences': 1, 'docFreq': np.int64(1)}, 'inference': {'id': 131, 'occurences': 4, 'docFreq': np.int64(3)}, 'cost': {'id': 132, 'occurences': 5, 'docFreq': np.int64(2)}, 'from': {'id': 133, 'occurences': 20, 'docFreq': np.int64(9)}, 'k': {'id': 134, 'occurences': 11, 'docFreq': np.int64(2)}, 'p': {'id': 135, 'occurences': 3, 'docFreq': np.int64(1)}, 'latency': {'id': 136, 'occurences': 3, 'docFreq': np.int64(1)}, 'open': {'id': 137, 'occurences': 1, 'docFreq': np.int64(1)}, 'source': {'id': 138, 'occurences': 3, 'docFreq': np.int64(2)}, 'alternatives': {'id': 139, 'occurences': 1, 'docFreq': np.int64(1)}, 'qwen': {'id': 140, 'occurences': 4, 'docFreq': np.int64(1)}, 'coder': {'id': 141, 'occurences': 4, 'docFreq': np.int64(1)}, 'b': {'id': 142, 'occurences': 10, 'docFreq': np.int64(2)}, 'llama': {'id': 143, 'occurences': 3, 'docFreq': np.int64(1)}, 'underperformed': {'id': 144, 'occurences': 1, 'docFreq': np.int64(1)}, 'despite': {'id': 145, 'occurences': 2, 'docFreq': np.int64(1)}, 'larger': {'id': 146, 'occurences': 3, 'docFreq': np.int64(2)}, 'parameter': {'id': 147, 'occurences': 2, 'docFreq': np.int64(1)}, 'counts': {'id': 148, 'occurences': 1, 'docFreq': np.int64(1)}, 'primarily': {'id': 149, 'occurences': 3, 'docFreq': np.int64(3)}, 'due': {'id': 150, 'occurences': 5, 'docFreq': np.int64(4)}, 'instruction': {'id': 151, 'occurences': 4, 'docFreq': np.int64(1)}, 'following': {'id': 152, 'occurences': 4, 'docFreq': np.int64(1)}, 'weaknesses': {'id': 153, 'occurences': 1, 'docFreq': np.int64(1)}, 'problem': {'id': 154, 'occurences': 4, 'docFreq': np.int64(3)}, 'transforming': {'id': 155, 'occurences': 1, 'docFreq': np.int64(1)}, 'algorithmic': {'id': 156, 'occurences': 3, 'docFreq': np.int64(1)}, 'problems': {'id': 157, 'occurences': 3, 'docFreq': np.int64(2)}, 'into': {'id': 158, 'occurences': 9, 'docFreq': np.int64(6)}, 'json': {'id': 159, 'occurences': 3, 'docFreq': np.int64(2)}, 'interview': {'id': 160, 'occurences': 1, 'docFreq': np.int64(1)}, 'scenarios': {'id': 161, 'occurences': 1, 'docFreq': np.int64(1)}, 'delivered': {'id': 162, 'occurences': 1, 'docFreq': np.int64(1)}, 'but': {'id': 163, 'occurences': 9, 'docFreq': np.int64(5)}, 'requests': {'id': 164, 'occurences': 1, 'docFreq': np.int64(1)}, 'challenge': {'id': 165, 'occurences': 1, 'docFreq': np.int64(1)}, 'maintain': {'id': 166, 'occurences': 3, 'docFreq': np.int64(2)}, 'achieving': {'id': 167, 'occurences': 1, 'docFreq': np.int64(1)}, 'production': {'id': 168, 'occurences': 2, 'docFreq': np.int64(2)}, 'viable': {'id': 169, 'occurences': 1, 'docFreq': np.int64(1)}, 'economics': {'id': 170, 'occurences': 2, 'docFreq': np.int64(1)}, 'approach': {'id': 171, 'occurences': 8, 'docFreq': np.int64(5)}, 'teacher': {'id': 172, 'occurences': 7, 'docFreq': np.int64(1)}, 'selection': {'id': 173, 'occurences': 5, 'docFreq': np.int64(4)}, 'tested': {'id': 174, 'occurences': 2, 'docFreq': np.int64(2)}, 'gemini': {'id': 175, 'occurences': 1, 'docFreq': np.int64(1)}, 'pro': {'id': 176, 'occurences': 1, 'docFreq': np.int64(1)}, 'winner': {'id': 177, 'occurences': 1, 'docFreq': np.int64(1)}, 'superior': {'id': 178, 'occurences': 1, 'docFreq': np.int64(1)}, 'parsing': {'id': 179, 'occurences': 5, 'docFreq': np.int64(1)}, 'correctness': {'id': 180, 'occurences': 2, 'docFreq': np.int64(1)}, 'evaluation': {'id': 181, 'occurences': 9, 'docFreq': np.int64(4)}, 'llm': {'id': 182, 'occurences': 2, 'docFreq': np.int64(2)}, 'as': {'id': 183, 'occurences': 18, 'docFreq': np.int64(11)}, 'a': {'id': 184, 'occurences': 66, 'docFreq': np.int64(17)}, 'judge': {'id': 185, 'occurences': 3, 'docFreq': np.int64(1)}, 'ensemble': {'id': 186, 'occurences': 2, 'docFreq': np.int64(1)}, 'across': {'id': 187, 'occurences': 2, 'docFreq': np.int64(2)}, 'dimensions': {'id': 188, 'occurences': 3, 'docFreq': np.int64(2)}, 'note': {'id': 189, 'occurences': 1, 'docFreq': np.int64(1)}, 'circular': {'id': 190, 'occurences': 2, 'docFreq': np.int64(1)}, 'bias': {'id': 191, 'occurences': 4, 'docFreq': np.int64(3)}, 'exists': {'id': 192, 'occurences': 3, 'docFreq': np.int64(2)}, 'both': {'id': 193, 'occurences': 5, 'docFreq': np.int64(5)}, 'judges': {'id': 194, 'occurences': 1, 'docFreq': np.int64(1)}, 'scored': {'id': 195, 'occurences': 1, 'docFreq': np.int64(1)}, 'independently': {'id': 196, 'occurences': 1, 'docFreq': np.int64(1)}, 'data': {'id': 197, 'occurences': 32, 'docFreq': np.int64(11)}, 'generation': {'id': 198, 'occurences': 2, 'docFreq': np.int64(1)}, 'generated': {'id': 199, 'occurences': 1, 'docFreq': np.int64(1)}, 'synthetic': {'id': 200, 'occurences': 2, 'docFreq': np.int64(2)}, 'examples': {'id': 201, 'occurences': 6, 'docFreq': np.int64(4)}, 'combinatorial': {'id': 202, 'occurences': 1, 'docFreq': np.int64(1)}, 'companies': {'id': 203, 'occurences': 1, 'docFreq': np.int64(1)}, 'roles': {'id': 204, 'occurences': 2, 'docFreq': np.int64(2)}, 'critical': {'id': 205, 'occurences': 3, 'docFreq': np.int64(2)}, 'step': {'id': 206, 'occurences': 3, 'docFreq': np.int64(2)}, 'programmatic': {'id': 207, 'occurences': 1, 'docFreq': np.int64(1)}, 'validation': {'id': 208, 'occurences': 2, 'docFreq': np.int64(2)}, 'rejected': {'id': 209, 'occurences': 1, 'docFreq': np.int64(1)}, 'rejection': {'id': 210, 'occurences': 2, 'docFreq': np.int64(1)}, 'criteria': {'id': 211, 'occurences': 3, 'docFreq': np.int64(3)}, 'schema': {'id': 212, 'occurences': 1, 'docFreq': np.int64(1)}, 'violations': {'id': 213, 'occurences': 2, 'docFreq': np.int64(1)}, 'hallucinated': {'id': 214, 'occurences': 1, 'docFreq': np.int64(1)}, 'constraints': {'id': 215, 'occurences': 1, 'docFreq': np.int64(1)}, 'failures': {'id': 216, 'occurences': 4, 'docFreq': np.int64(1)}, 'final': {'id': 217, 'occurences': 1, 'docFreq': np.int64(1)}, 'training': {'id': 218, 'occurences': 12, 'docFreq': np.int64(5)}, 'set': {'id': 219, 'occurences': 5, 'docFreq': np.int64(3)}, 'student': {'id': 220, 'occurences': 1, 'docFreq': np.int64(1)}, 'comparison': {'id': 221, 'occurences': 1, 'docFreq': np.int64(1)}, 'model': {'id': 222, 'occurences': 28, 'docFreq': np.int64(10)}, 'method': {'id': 223, 'occurences': 5, 'docFreq': np.int64(5)}, 'key': {'id': 224, 'occurences': 4, 'docFreq': np.int64(3)}, 'failure': {'id': 225, 'occurences': 2, 'docFreq': np.int64(1)}, 'mode': {'id': 226, 'occurences': 3, 'docFreq': np.int64(2)}, 'lora': {'id': 227, 'occurences': 2, 'docFreq': np.int64(1)}, 'r': {'id': 228, 'occurences': 2, 'docFreq': np.int64(1)}, 'negative': {'id': 229, 'occurences': 2, 'docFreq': np.int64(1)}, 'constraint': {'id': 230, 'occurences': 3, 'docFreq': np.int64(1)}, 'catastrophic': {'id': 231, 'occurences': 2, 'docFreq': np.int64(1)}, 'forgetting': {'id': 232, 'occurences': 2, 'docFreq': np.int64(1)}, 'parse': {'id': 233, 'occurences': 2, 'docFreq': np.int64(1)}, 'api': {'id': 234, 'occurences': 2, 'docFreq': np.int64(2)}, 'tune': {'id': 235, 'occurences': 1, 'docFreq': np.int64(1)}, 'role': {'id': 236, 'occurences': 2, 'docFreq': np.int64(1)}, 'specificity': {'id': 237, 'occurences': 2, 'docFreq': np.int64(1)}, 'weakness': {'id': 238, 'occurences': 2, 'docFreq': np.int64(1)}, 'results': {'id': 239, 'occurences': 3, 'docFreq': np.int64(3)}, 'performance': {'id': 240, 'occurences': 14, 'docFreq': np.int64(7)}, 'reduction': {'id': 241, 'occurences': 1, 'docFreq': np.int64(1)}, 'x': {'id': 242, 'occurences': 1, 'docFreq': np.int64(1)}, 'improvement': {'id': 243, 'occurences': 4, 'docFreq': np.int64(3)}, 'success': {'id': 244, 'occurences': 2, 'docFreq': np.int64(2)}, 'dimension': {'id': 245, 'occurences': 1, 'docFreq': np.int64(1)}, 'exceeds': {'id': 246, 'occurences': 2, 'docFreq': np.int64(1)}, 'matches': {'id': 247, 'occurences': 1, 'docFreq': np.int64(1)}, 'technical': {'id': 248, 'occurences': 4, 'docFreq': np.int64(3)}, 'accuracy': {'id': 249, 'occurences': 5, 'docFreq': np.int64(4)}, 'company': {'id': 250, 'occurences': 1, 'docFreq': np.int64(1)}, 'relevance': {'id': 251, 'occurences': 2, 'docFreq': np.int64(2)}, 'scenario': {'id': 252, 'occurences': 1, 'docFreq': np.int64(1)}, 'realism': {'id': 253, 'occurences': 1, 'docFreq': np.int64(1)}, 'insights': {'id': 254, 'occurences': 2, 'docFreq': np.int64(2)}, 'size': {'id': 255, 'occurences': 3, 'docFreq': np.int64(2)}, 'rumored': {'id': 256, 'occurences': 1, 'docFreq': np.int64(1)}, 'parameters': {'id': 257, 'occurences': 2, 'docFreq': np.int64(2)}, 'beat': {'id': 258, 'occurences': 1, 'docFreq': np.int64(1)}, 'points': {'id': 259, 'occurences': 3, 'docFreq': np.int64(3)}, 'pre': {'id': 260, 'occurences': 4, 'docFreq': np.int64(2)}, 'matters': {'id': 261, 'occurences': 1, 'docFreq': np.int64(1)}, 'more': {'id': 262, 'occurences': 12, 'docFreq': np.int64(8)}, 'than': {'id': 263, 'occurences': 2, 'docFreq': np.int64(2)}, 'count': {'id': 264, 'occurences': 1, 'docFreq': np.int64(1)}, 'rate': {'id': 265, 'occurences': 1, 'docFreq': np.int64(1)}, 'was': {'id': 266, 'occurences': 1, 'docFreq': np.int64(1)}, 'essential': {'id': 267, 'occurences': 2, 'docFreq': np.int64(2)}, 'without': {'id': 268, 'occurences': 3, 'docFreq': np.int64(2)}, 'filtering': {'id': 269, 'occurences': 2, 'docFreq': np.int64(1)}, 'jumped': {'id': 270, 'occurences': 1, 'docFreq': np.int64(1)}, 'increase': {'id': 271, 'occurences': 2, 'docFreq': np.int64(2)}, 'code': {'id': 272, 'occurences': 4, 'docFreq': np.int64(2)}, 'completion': {'id': 273, 'occurences': 2, 'docFreq': np.int64(1)}, 'toward': {'id': 274, 'occurences': 1, 'docFreq': np.int64(1)}, 'interfered': {'id': 275, 'occurences': 1, 'docFreq': np.int64(1)}, 'strict': {'id': 276, 'occurences': 2, 'docFreq': np.int64(1)}, 'adherence': {'id': 277, 'occurences': 2, 'docFreq': np.int64(1)}, 'couldn': {'id': 278, 'occurences': 1, 'docFreq': np.int64(1)}, 't': {'id': 279, 'occurences': 6, 'docFreq': np.int64(5)}, 'syntax': {'id': 280, 'occurences': 1, 'docFreq': np.int64(1)}, 'knowledge': {'id': 281, 'occurences': 2, 'docFreq': np.int64(2)}, 'learning': {'id': 282, 'occurences': 47, 'docFreq': np.int64(12)}, 'task': {'id': 283, 'occurences': 1, 'docFreq': np.int64(1)}, 'setup': {'id': 284, 'occurences': 1, 'docFreq': np.int64(1)}, 'tuning': {'id': 285, 'occurences': 1, 'docFreq': np.int64(1)}, 'break': {'id': 286, 'occurences': 1, 'docFreq': np.int64(1)}, 'even': {'id': 287, 'occurences': 5, 'docFreq': np.int64(4)}, 'inferences': {'id': 288, 'occurences': 1, 'docFreq': np.int64(1)}, 'weeks': {'id': 289, 'occurences': 1, 'docFreq': np.int64(1)}, 'month': {'id': 290, 'occurences': 2, 'docFreq': np.int64(1)}, 'cumulative': {'id': 291, 'occurences': 1, 'docFreq': np.int64(1)}, 'savings': {'id': 292, 'occurences': 1, 'docFreq': np.int64(1)}, 'volume': {'id': 293, 'occurences': 1, 'docFreq': np.int64(1)}, 'scaling': {'id': 294, 'occurences': 1, 'docFreq': np.int64(1)}, 'how': {'id': 295, 'occurences': 8, 'docFreq': np.int64(5)}, 'handle': {'id': 296, 'occurences': 1, 'docFreq': np.int64(1)}, 'when': {'id': 297, 'occurences': 3, 'docFreq': np.int64(3)}, 'architectural': {'id': 298, 'occurences': 1, 'docFreq': np.int64(1)}, 'techniques': {'id': 299, 'occurences': 4, 'docFreq': np.int64(3)}, 'improve': {'id': 300, 'occurences': 4, 'docFreq': np.int64(4)}, 'models': {'id': 301, 'occurences': 24, 'docFreq': np.int64(9)}, 'why': {'id': 302, 'occurences': 1, 'docFreq': np.int64(1)}, 'specialized': {'id': 303, 'occurences': 1, 'docFreq': np.int64(1)}, 'struggle': {'id': 304, 'occurences': 1, 'docFreq': np.int64(1)}, 'reproducibility': {'id': 305, 'occurences': 1, 'docFreq': np.int64(1)}, 'methodology': {'id': 306, 'occurences': 3, 'docFreq': np.int64(2)}, 'charts': {'id': 307, 'occurences': 1, 'docFreq': np.int64(1)}, 'https': {'id': 308, 'occurences': 8, 'docFreq': np.int64(4)}, 'www': {'id': 309, 'occurences': 2, 'docFreq': np.int64(1)}, 'algoirl': {'id': 310, 'occurences': 2, 'docFreq': np.int64(1)}, 'ai': {'id': 311, 'occurences': 6, 'docFreq': np.int64(4)}, 'engineering': {'id': 312, 'occurences': 2, 'docFreq': np.int64(1)}, 'notes': {'id': 313, 'occurences': 2, 'docFreq': np.int64(1)}, 'distilling': {'id': 314, 'occurences': 2, 'docFreq': np.int64(1)}, 'intelligence': {'id': 315, 'occurences': 2, 'docFreq': np.int64(1)}, 'happy': {'id': 316, 'occurences': 2, 'docFreq': np.int64(2)}, 'discuss': {'id': 317, 'occurences': 3, 'docFreq': np.int64(3)}, 'details': {'id': 318, 'occurences': 2, 'docFreq': np.int64(2)}, 'modes': {'id': 319, 'occurences': 1, 'docFreq': np.int64(1)}, 'project': {'id': 320, 'occurences': 1, 'docFreq': np.int64(1)}, 'i': {'id': 321, 'occurences': 23, 'docFreq': np.int64(7)}, 've': {'id': 322, 'occurences': 1, 'docFreq': np.int64(1)}, 'been': {'id': 323, 'occurences': 10, 'docFreq': np.int64(7)}, 'working': {'id': 324, 'occurences': 1, 'docFreq': np.int64(1)}, 'quietly': {'id': 325, 'occurences': 1, 'docFreq': np.int64(1)}, 'finally': {'id': 326, 'occurences': 1, 'docFreq': np.int64(1)}, 'feel': {'id': 327, 'occurences': 2, 'docFreq': np.int64(2)}, 'confident': {'id': 328, 'occurences': 1, 'docFreq': np.int64(1)}, 'enough': {'id': 329, 'occurences': 1, 'docFreq': np.int64(1)}, 'share': {'id': 330, 'occurences': 1, 'docFreq': np.int64(1)}, 'core': {'id': 331, 'occurences': 1, 'docFreq': np.int64(1)}, 'idea': {'id': 332, 'occurences': 2, 'docFreq': np.int64(2)}, 'lightweight': {'id': 333, 'occurences': 1, 'docFreq': np.int64(1)}, 'verification': {'id': 334, 'occurences': 4, 'docFreq': np.int64(1)}, 'pipeline': {'id': 335, 'occurences': 5, 'docFreq': np.int64(2)}, 'designed': {'id': 336, 'occurences': 1, 'docFreq': np.int64(1)}, 'make': {'id': 337, 'occurences': 2, 'docFreq': np.int64(2)}, 'small': {'id': 338, 'occurences': 6, 'docFreq': np.int64(4)}, 'local': {'id': 339, 'occurences': 2, 'docFreq': np.int64(1)}, 'behave': {'id': 340, 'occurences': 2, 'docFreq': np.int64(1)}, 'much': {'id': 341, 'occurences': 2, 'docFreq': np.int64(2)}, 'reliably': {'id': 342, 'occurences': 1, 'docFreq': np.int64(1)}, 'giving': {'id': 343, 'occurences': 1, 'docFreq': np.int64(1)}, 'them': {'id': 344, 'occurences': 5, 'docFreq': np.int64(5)}, 'structure': {'id': 345, 'occurences': 2, 'docFreq': np.int64(2)}, 'scale': {'id': 346, 'occurences': 2, 'docFreq': np.int64(2)}, 'architecture': {'id': 347, 'occurences': 6, 'docFreq': np.int64(4)}, 'has': {'id': 348, 'occurences': 11, 'docFreq': np.int64(8)}, 'three': {'id': 349, 'occurences': 4, 'docFreq': np.int64(4)}, 'parts': {'id': 350, 'occurences': 1, 'docFreq': np.int64(1)}, 'intent': {'id': 351, 'occurences': 2, 'docFreq': np.int64(1)}, 'understanding': {'id': 352, 'occurences': 3, 'docFreq': np.int64(3)}, 'before': {'id': 353, 'occurences': 1, 'docFreq': np.int64(1)}, 'does': {'id': 354, 'occurences': 1, 'docFreq': np.int64(1)}, 'anything': {'id': 355, 'occurences': 3, 'docFreq': np.int64(2)}, 'classifier': {'id': 356, 'occurences': 1, 'docFreq': np.int64(1)}, 'figures': {'id': 357, 'occurences': 1, 'docFreq': np.int64(1)}, 'out': {'id': 358, 'occurences': 2, 'docFreq': np.int64(2)}, 'type': {'id': 359, 'occurences': 3, 'docFreq': np.int64(3)}, 'request': {'id': 360, 'occurences': 1, 'docFreq': np.int64(1)}, 'user': {'id': 361, 'occurences': 3, 'docFreq': np.int64(2)}, 'making': {'id': 362, 'occurences': 13, 'docFreq': np.int64(8)}, 'news': {'id': 363, 'occurences': 9, 'docFreq': np.int64(2)}, 'explanation': {'id': 364, 'occurences': 1, 'docFreq': np.int64(1)}, 'solving': {'id': 365, 'occurences': 2, 'docFreq': np.int64(1)}, 'treating': {'id': 366, 'occurences': 1, 'docFreq': np.int64(1)}, 'all': {'id': 367, 'occurences': 2, 'docFreq': np.int64(1)}, 'prompts': {'id': 368, 'occurences': 1, 'docFreq': np.int64(1)}, 'same': {'id': 369, 'occurences': 1, 'docFreq': np.int64(1)}, 'routed': {'id': 370, 'occurences': 1, 'docFreq': np.int64(1)}, 'correct': {'id': 371, 'occurences': 1, 'docFreq': np.int64(1)}, 'beginning': {'id': 372, 'occurences': 1, 'docFreq': np.int64(1)}, 'execution': {'id': 373, 'occurences': 1, 'docFreq': np.int64(1)}, 'paths': {'id': 374, 'occurences': 1, 'docFreq': np.int64(1)}, 'each': {'id': 375, 'occurences': 3, 'docFreq': np.int64(3)}, 'its': {'id': 376, 'occurences': 8, 'docFreq': np.int64(5)}, 'own': {'id': 377, 'occurences': 1, 'docFreq': np.int64(1)}, 'multi': {'id': 378, 'occurences': 2, 'docFreq': np.int64(2)}, 'search': {'id': 379, 'occurences': 1, 'docFreq': np.int64(1)}, 'aggregation': {'id': 380, 'occurences': 1, 'docFreq': np.int64(1)}, 'explanations': {'id': 381, 'occurences': 1, 'docFreq': np.int64(1)}, 'layered': {'id': 382, 'occurences': 1, 'docFreq': np.int64(1)}, 'chain': {'id': 383, 'occurences': 1, 'docFreq': np.int64(1)}, 'logic': {'id': 384, 'occurences': 2, 'docFreq': np.int64(1)}, 'symbolic': {'id': 385, 'occurences': 1, 'docFreq': np.int64(1)}, 'checks': {'id': 386, 'occurences': 2, 'docFreq': np.int64(1)}, 'removes': {'id': 387, 'occurences': 1, 'docFreq': np.int64(1)}, 'ambiguity': {'id': 388, 'occurences': 1, 'docFreq': np.int64(1)}, 'forces': {'id': 389, 'occurences': 1, 'docFreq': np.int64(1)}, 'predictable': {'id': 390, 'occurences': 1, 'docFreq': np.int64(1)}, 'behavior': {'id': 391, 'occurences': 1, 'docFreq': np.int64(1)}, 'big': {'id': 392, 'occurences': 1, 'docFreq': np.int64(1)}, 'deal': {'id': 393, 'occurences': 1, 'docFreq': np.int64(1)}, 'automatic': {'id': 394, 'occurences': 1, 'docFreq': np.int64(1)}, 'correction': {'id': 395, 'occurences': 1, 'docFreq': np.int64(1)}, 'generating': {'id': 396, 'occurences': 1, 'docFreq': np.int64(1)}, 'answer': {'id': 397, 'occurences': 5, 'docFreq': np.int64(2)}, 'verifies': {'id': 398, 'occurences': 1, 'docFreq': np.int64(1)}, 'against': {'id': 399, 'occurences': 1, 'docFreq': np.int64(1)}, 'external': {'id': 400, 'occurences': 1, 'docFreq': np.int64(1)}, 'signals': {'id': 401, 'occurences': 2, 'docFreq': np.int64(2)}, 'cross': {'id': 402, 'occurences': 3, 'docFreq': np.int64(2)}, 'consistency': {'id': 403, 'occurences': 2, 'docFreq': np.int64(2)}, 'internal': {'id': 404, 'occurences': 1, 'docFreq': np.int64(1)}, 'coherence': {'id': 405, 'occurences': 1, 'docFreq': np.int64(1)}, 'pattern': {'id': 406, 'occurences': 1, 'docFreq': np.int64(1)}, 'based': {'id': 407, 'occurences': 10, 'docFreq': np.int64(7)}, 'self': {'id': 408, 'occurences': 1, 'docFreq': np.int64(1)}, 'fails': {'id': 409, 'occurences': 1, 'docFreq': np.int64(1)}, 'automatically': {'id': 410, 'occurences': 1, 'docFreq': np.int64(1)}, 'regenerates': {'id': 411, 'occurences': 1, 'docFreq': np.int64(1)}, 'corrected': {'id': 412, 'occurences': 1, 'docFreq': np.int64(1)}, 'goal': {'id': 413, 'occurences': 2, 'docFreq': np.int64(1)}, 'isn': {'id': 414, 'occurences': 2, 'docFreq': np.int64(2)}, 'trick': {'id': 415, 'occurences': 1, 'docFreq': np.int64(1)}, 'smart': {'id': 416, 'occurences': 1, 'docFreq': np.int64(1)}, 'give': {'id': 417, 'occurences': 3, 'docFreq': np.int64(3)}, 'software': {'id': 418, 'occurences': 1, 'docFreq': np.int64(1)}, 'they': {'id': 419, 'occurences': 8, 'docFreq': np.int64(4)}, 'need': {'id': 420, 'occurences': 2, 'docFreq': np.int64(2)}, 'bigger': {'id': 421, 'occurences': 2, 'docFreq': np.int64(2)}, 'dedicated': {'id': 422, 'occurences': 1, 'docFreq': np.int64(1)}, 'routes': {'id': 423, 'occurences': 1, 'docFreq': np.int64(1)}, 'clear': {'id': 424, 'occurences': 1, 'docFreq': np.int64(1)}, 'second': {'id': 425, 'occurences': 1, 'docFreq': np.int64(1)}, 'layer': {'id': 426, 'occurences': 6, 'docFreq': np.int64(2)}, 'control': {'id': 427, 'occurences': 1, 'docFreq': np.int64(1)}, 'early': {'id': 428, 'occurences': 3, 'docFreq': np.int64(2)}, 'testers': {'id': 429, 'occurences': 1, 'docFreq': np.int64(1)}, 'reported': {'id': 430, 'occurences': 1, 'docFreq': np.int64(1)}, 'basic': {'id': 431, 'occurences': 1, 'docFreq': np.int64(1)}, 'felt': {'id': 432, 'occurences': 1, 'docFreq': np.int64(1)}, 'noticeably': {'id': 433, 'occurences': 1, 'docFreq': np.int64(1)}, 'run': {'id': 434, 'occurences': 2, 'docFreq': np.int64(2)}, 'through': {'id': 435, 'occurences': 2, 'docFreq': np.int64(2)}, 'because': {'id': 436, 'occurences': 2, 'docFreq': np.int64(1)}, 'changed': {'id': 437, 'occurences': 1, 'docFreq': np.int64(1)}, 'surrounding': {'id': 438, 'occurences': 1, 'docFreq': np.int64(1)}, 'system': {'id': 439, 'occurences': 1, 'docFreq': np.int64(1)}, 'did': {'id': 440, 'occurences': 1, 'docFreq': np.int64(1)}, 'll': {'id': 441, 'occurences': 2, 'docFreq': np.int64(2)}, 'benchmarks': {'id': 442, 'occurences': 1, 'docFreq': np.int64(1)}, 'first': {'id': 443, 'occurences': 4, 'docFreq': np.int64(4)}, 'comment': {'id': 444, 'occurences': 1, 'docFreq': np.int64(1)}, 'comply': {'id': 445, 'occurences': 1, 'docFreq': np.int64(1)}, 'rule': {'id': 446, 'occurences': 1, 'docFreq': np.int64(1)}, 'anyone': {'id': 447, 'occurences': 3, 'docFreq': np.int64(3)}, 'tries': {'id': 448, 'occurences': 1, 'docFreq': np.int64(1)}, 'd': {'id': 449, 'occurences': 6, 'docFreq': np.int64(4)}, 'genuinely': {'id': 450, 'occurences': 1, 'docFreq': np.int64(1)}, 'love': {'id': 451, 'occurences': 3, 'docFreq': np.int64(3)}, 'know': {'id': 452, 'occurences': 1, 'docFreq': np.int64(1)}, 'behaves': {'id': 453, 'occurences': 1, 'docFreq': np.int64(1)}, 'setups': {'id': 454, 'occurences': 1, 'docFreq': np.int64(1)}, 'feedback': {'id': 455, 'occurences': 2, 'docFreq': np.int64(2)}, 'improvements': {'id': 456, 'occurences': 1, 'docFreq': np.int64(1)}, 'edge': {'id': 457, 'occurences': 1, 'docFreq': np.int64(1)}, 'cases': {'id': 458, 'occurences': 1, 'docFreq': np.int64(1)}, 'are': {'id': 459, 'occurences': 14, 'docFreq': np.int64(10)}, 'welcome': {'id': 460, 'occurences': 1, 'docFreq': np.int64(1)}, 'about': {'id': 461, 'occurences': 1, 'docFreq': np.int64(1)}, 'routing': {'id': 462, 'occurences': 1, 'docFreq': np.int64(1)}, 'design': {'id': 463, 'occurences': 1, 'docFreq': np.int64(1)}, 'implementation': {'id': 464, 'occurences': 2, 'docFreq': np.int64(2)}, 'repo': {'id': 465, 'occurences': 1, 'docFreq': np.int64(1)}, 'mit': {'id': 466, 'occurences': 1, 'docFreq': np.int64(1)}, 'github': {'id': 467, 'occurences': 4, 'docFreq': np.int64(2)}, 'com': {'id': 468, 'occurences': 4, 'docFreq': np.int64(2)}, 'clowerweb': {'id': 469, 'occurences': 2, 'docFreq': np.int64(1)}, 'relational': {'id': 470, 'occurences': 5, 'docFreq': np.int64(1)}, 'attention': {'id': 471, 'occurences': 3, 'docFreq': np.int64(2)}, 'quick': {'id': 472, 'occurences': 2, 'docFreq': np.int64(2)}, 'rundown': {'id': 473, 'occurences': 1, 'docFreq': np.int64(1)}, 'novel': {'id': 474, 'occurences': 4, 'docFreq': np.int64(4)}, 'neural': {'id': 475, 'occurences': 1, 'docFreq': np.int64(1)}, 'few': {'id': 476, 'occurences': 2, 'docFreq': np.int64(2)}, 'shot': {'id': 477, 'occurences': 1, 'docFreq': np.int64(1)}, 'transformations': {'id': 478, 'occurences': 1, 'docFreq': np.int64(1)}, 'outperforms': {'id': 479, 'occurences': 1, 'docFreq': np.int64(1)}, 'standard': {'id': 480, 'occurences': 4, 'docFreq': np.int64(1)}, 'transformers': {'id': 481, 'occurences': 1, 'docFreq': np.int64(1)}, 'relative': {'id': 482, 'occurences': 1, 'docFreq': np.int64(1)}, 'being': {'id': 483, 'occurences': 2, 'docFreq': np.int64(2)}, 'faster': {'id': 484, 'occurences': 1, 'docFreq': np.int64(1)}, 'unseen': {'id': 485, 'occurences': 2, 'docFreq': np.int64(1)}, 'speed': {'id': 486, 'occurences': 1, 'docFreq': np.int64(1)}, 'gap': {'id': 487, 'occurences': 1, 'docFreq': np.int64(1)}, 'ours': {'id': 488, 'occurences': 1, 'docFreq': np.int64(1)}, 'transformer': {'id': 489, 'occurences': 1, 'docFreq': np.int64(1)}, 'baseline': {'id': 490, 'occurences': 1, 'docFreq': np.int64(1)}, 'per': {'id': 491, 'occurences': 1, 'docFreq': np.int64(1)}, 'transform': {'id': 492, 'occurences': 3, 'docFreq': np.int64(1)}, 'breakdown': {'id': 493, 'occurences': 1, 'docFreq': np.int64(1)}, 'flip_vertical': {'id': 494, 'occurences': 1, 'docFreq': np.int64(1)}, 'rotate_': {'id': 495, 'occurences': 1, 'docFreq': np.int64(1)}, 'translate_down': {'id': 496, 'occurences': 1, 'docFreq': np.int64(1)}, 'invert_colors': {'id': 497, 'occurences': 1, 'docFreq': np.int64(1)}, 'excels': {'id': 498, 'occurences': 1, 'docFreq': np.int64(1)}, 'at': {'id': 499, 'occurences': 5, 'docFreq': np.int64(4)}, 'spatial': {'id': 500, 'occurences': 1, 'docFreq': np.int64(1)}, 'maintaining': {'id': 501, 'occurences': 1, 'docFreq': np.int64(1)}, 'strong': {'id': 502, 'occurences': 1, 'docFreq': np.int64(1)}, 'color': {'id': 503, 'occurences': 1, 'docFreq': np.int64(1)}, 'm': {'id': 504, 'occurences': 3, 'docFreq': np.int64(3)}, 'params': {'id': 505, 'occurences': 1, 'docFreq': np.int64(1)}, 'scores': {'id': 506, 'occurences': 1, 'docFreq': np.int64(1)}, 'epoch': {'id': 507, 'occurences': 1, 'docFreq': np.int64(1)}, 'epochs': {'id': 508, 'occurences': 2, 'docFreq': np.int64(1)}, 'arc': {'id': 509, 'occurences': 2, 'docFreq': np.int64(1)}, 'agi': {'id': 510, 'occurences': 2, 'docFreq': np.int64(1)}, 'starts': {'id': 511, 'occurences': 1, 'docFreq': np.int64(1)}, 'slip': {'id': 512, 'occurences': 1, 'docFreq': np.int64(1)}, 'likely': {'id': 513, 'occurences': 1, 'docFreq': np.int64(1)}, 'overfitting': {'id': 514, 'occurences': 1, 'docFreq': np.int64(1)}, 'think': {'id': 515, 'occurences': 2, 'docFreq': np.int64(2)}, 'just': {'id': 516, 'occurences': 1, 'docFreq': np.int64(1)}, 'too': {'id': 517, 'occurences': 2, 'docFreq': np.int64(2)}, 'don': {'id': 518, 'occurences': 2, 'docFreq': np.int64(2)}, 'have': {'id': 519, 'occurences': 12, 'docFreq': np.int64(8)}, 'hardware': {'id': 520, 'occurences': 2, 'docFreq': np.int64(1)}, 'also': {'id': 521, 'occurences': 9, 'docFreq': np.int64(7)}, 'see': {'id': 522, 'occurences': 1, 'docFreq': np.int64(1)}, 'algorithm': {'id': 523, 'occurences': 8, 'docFreq': np.int64(4)}, 'might': {'id': 524, 'occurences': 1, 'docFreq': np.int64(1)}, 'llms': {'id': 525, 'occurences': 2, 'docFreq': np.int64(2)}, 'may': {'id': 526, 'occurences': 3, 'docFreq': np.int64(2)}, 'train': {'id': 527, 'occurences': 1, 'docFreq': np.int64(1)}, 'tinystories': {'id': 528, 'occurences': 1, 'docFreq': np.int64(1)}, 'slm': {'id': 529, 'occurences': 1, 'docFreq': np.int64(1)}, 'over': {'id': 530, 'occurences': 3, 'docFreq': np.int64(3)}, 'weekend': {'id': 531, 'occurences': 1, 'docFreq': np.int64(1)}, 'probably': {'id': 532, 'occurences': 1, 'docFreq': np.int64(1)}, 'take': {'id': 533, 'occurences': 1, 'docFreq': np.int64(1)}, 'several': {'id': 534, 'occurences': 5, 'docFreq': np.int64(5)}, 'days': {'id': 535, 'occurences': 1, 'docFreq': np.int64(1)}, 'my': {'id': 536, 'occurences': 5, 'docFreq': np.int64(3)}, 'welcoming': {'id': 537, 'occurences': 1, 'docFreq': np.int64(1)}, 'where': {'id': 538, 'occurences': 4, 'docFreq': np.int64(4)}, 'sample': {'id': 539, 'occurences': 1, 'docFreq': np.int64(1)}, 'matrix': {'id': 540, 'occurences': 1, 'docFreq': np.int64(1)}, 'mapping': {'id': 541, 'occurences': 1, 'docFreq': np.int64(1)}, 'two': {'id': 542, 'occurences': 3, 'docFreq': np.int64(3)}, 'values': {'id': 543, 'occurences': 1, 'docFreq': np.int64(1)}, 'output': {'id': 544, 'occurences': 2, 'docFreq': np.int64(1)}, 'regression': {'id': 545, 'occurences': 1, 'docFreq': np.int64(1)}, 'find': {'id': 546, 'occurences': 2, 'docFreq': np.int64(2)}, 'which': {'id': 547, 'occurences': 8, 'docFreq': np.int64(7)}, 'max': {'id': 548, 'occurences': 4, 'docFreq': np.int64(1)}, 'contains': {'id': 549, 'occurences': 1, 'docFreq': np.int64(1)}, 'conv': {'id': 550, 'occurences': 2, 'docFreq': np.int64(1)}, 'dense': {'id': 551, 'occurences': 3, 'docFreq': np.int64(1)}, 'nodes': {'id': 552, 'occurences': 1, 'docFreq': np.int64(1)}, 'er': {'id': 553, 'occurences': 1, 'docFreq': np.int64(1)}, 'won': {'id': 554, 'occurences': 1, 'docFreq': np.int64(1)}, 'pruning': {'id': 555, 'occurences': 1, 'docFreq': np.int64(1)}, 'overparameterized': {'id': 556, 'occurences': 1, 'docFreq': np.int64(1)}, 'help': {'id': 557, 'occurences': 7, 'docFreq': np.int64(5)}, 'fix': {'id': 558, 'occurences': 1, 'docFreq': np.int64(1)}, 'fitting': {'id': 559, 'occurences': 1, 'docFreq': np.int64(1)}, 'decide': {'id': 560, 'occurences': 1, 'docFreq': np.int64(1)}, 'many': {'id': 561, 'occurences': 4, 'docFreq': np.int64(3)}, 'needed': {'id': 562, 'occurences': 2, 'docFreq': np.int64(1)}, 'using': {'id': 563, 'occurences': 3, 'docFreq': np.int64(3)}, 'nas': {'id': 564, 'occurences': 3, 'docFreq': np.int64(1)}, 'coz': {'id': 565, 'occurences': 1, 'docFreq': np.int64(1)}, 'slightest': {'id': 566, 'occurences': 1, 'docFreq': np.int64(1)}, 'number': {'id': 567, 'occurences': 8, 'docFreq': np.int64(5)}, 'cov': {'id': 568, 'occurences': 1, 'docFreq': np.int64(1)}, 'layers': {'id': 569, 'occurences': 2, 'docFreq': np.int64(1)}, 'select': {'id': 570, 'occurences': 5, 'docFreq': np.int64(3)}, 'highest': {'id': 571, 'occurences': 2, 'docFreq': np.int64(2)}, 'attribute': {'id': 572, 'occurences': 2, 'docFreq': np.int64(1)}, 'approx': {'id': 573, 'occurences': 1, 'docFreq': np.int64(1)}, 'attributes': {'id': 574, 'occurences': 2, 'docFreq': np.int64(2)}, 'very': {'id': 575, 'occurences': 4, 'docFreq': np.int64(4)}, 'high': {'id': 576, 'occurences': 1, 'docFreq': np.int64(1)}, 'drop': {'id': 577, 'occurences': 1, 'docFreq': np.int64(1)}, 'frequency': {'id': 578, 'occurences': 1, 'docFreq': np.int64(1)}, 'compared': {'id': 579, 'occurences': 2, 'docFreq': np.int64(2)}, 'hi': {'id': 580, 'occurences': 1, 'docFreq': np.int64(1)}, 'folks': {'id': 581, 'occurences': 1, 'docFreq': np.int64(1)}, 'maintainers': {'id': 582, 'occurences': 1, 'docFreq': np.int64(1)}, 'pixeltable': {'id': 583, 'occurences': 4, 'docFreq': np.int64(1)}, 'provide': {'id': 584, 'occurences': 5, 'docFreq': np.int64(4)}, 'built': {'id': 585, 'occurences': 1, 'docFreq': np.int64(1)}, 'support': {'id': 586, 'occurences': 2, 'docFreq': np.int64(2)}, 'sam': {'id': 587, 'occurences': 2, 'docFreq': np.int64(1)}, 'facebookresearch': {'id': 588, 'occurences': 1, 'docFreq': np.int64(1)}, 'segment': {'id': 589, 'occurences': 2, 'docFreq': np.int64(1)}, 'chat': {'id': 590, 'occurences': 1, 'docFreq': np.int64(1)}, 'people': {'id': 591, 'occurences': 2, 'docFreq': np.int64(2)}, 'daily': {'id': 592, 'occurences': 1, 'docFreq': np.int64(1)}, 'weekly': {'id': 593, 'occurences': 1, 'docFreq': np.int64(1)}, 'basis': {'id': 594, 'occurences': 1, 'docFreq': np.int64(1)}, 'workflows': {'id': 595, 'occurences': 1, 'docFreq': np.int64(1)}, 'look': {'id': 596, 'occurences': 1, 'docFreq': np.int64(1)}, 'quite': {'id': 597, 'occurences': 1, 'docFreq': np.int64(1)}, 'unique': {'id': 598, 'occurences': 2, 'docFreq': np.int64(1)}, 'way': {'id': 599, 'occurences': 1, 'docFreq': np.int64(1)}, 'can': {'id': 600, 'occurences': 12, 'docFreq': np.int64(8)}, 'dataframe': {'id': 601, 'occurences': 1, 'docFreq': np.int64(1)}, 'engine': {'id': 602, 'occurences': 1, 'docFreq': np.int64(1)}, 'manipulate': {'id': 603, 'occurences': 1, 'docFreq': np.int64(1)}, 'video': {'id': 604, 'occurences': 1, 'docFreq': np.int64(1)}, 'frames': {'id': 605, 'occurences': 1, 'docFreq': np.int64(1)}, 'arrays': {'id': 606, 'occurences': 1, 'docFreq': np.int64(1)}, 'class': {'id': 607, 'occurences': 2, 'docFreq': np.int64(2)}, 'types': {'id': 608, 'occurences': 2, 'docFreq': np.int64(2)}, 'among': {'id': 609, 'occurences': 1, 'docFreq': np.int64(1)}, 'other': {'id': 610, 'occurences': 5, 'docFreq': np.int64(2)}, 'things': {'id': 611, 'occurences': 1, 'docFreq': np.int64(1)}, 'makes': {'id': 612, 'occurences': 2, 'docFreq': np.int64(2)}, 'programmatically': {'id': 613, 'occurences': 1, 'docFreq': np.int64(1)}, 'outputs': {'id': 614, 'occurences': 1, 'docFreq': np.int64(1)}, 'masks': {'id': 615, 'occurences': 1, 'docFreq': np.int64(1)}, 'free': {'id': 616, 'occurences': 1, 'docFreq': np.int64(1)}, 'reply': {'id': 617, 'occurences': 1, 'docFreq': np.int64(1)}, 'dm': {'id': 618, 'occurences': 1, 'docFreq': np.int64(1)}, 'me': {'id': 619, 'occurences': 2, 'docFreq': np.int64(2)}, 'thanks': {'id': 620, 'occurences': 1, 'docFreq': np.int64(1)}, 'really': {'id': 621, 'occurences': 2, 'docFreq': np.int64(2)}, 'appreciated': {'id': 622, 'occurences': 1, 'docFreq': np.int64(1)}, 'pretty': {'id': 623, 'occurences': 1, 'docFreq': np.int64(1)}, 'crazy': {'id': 624, 'occurences': 1, 'docFreq': np.int64(1)}, 'feat': {'id': 625, 'occurences': 1, 'docFreq': np.int64(1)}, 'zelo': {'id': 626, 'occurences': 1, 'docFreq': np.int64(1)}, 'super': {'id': 627, 'occurences': 1, 'docFreq': np.int64(1)}, 'impressive': {'id': 628, 'occurences': 1, 'docFreq': np.int64(1)}, 'thoughts': {'id': 629, 'occurences': 1, 'docFreq': np.int64(1)}, 'tensorpool': {'id': 630, 'occurences': 2, 'docFreq': np.int64(1)}, 'dev': {'id': 631, 'occurences': 2, 'docFreq': np.int64(1)}, 'blog': {'id': 632, 'occurences': 2, 'docFreq': np.int64(1)}, 'zeroentropy': {'id': 633, 'occurences': 2, 'docFreq': np.int64(1)}, 'zerank': {'id': 634, 'occurences': 2, 'docFreq': np.int64(1)}, 'utm': {'id': 635, 'occurences': 1, 'docFreq': np.int64(1)}, '_source': {'id': 636, 'occurences': 1, 'docFreq': np.int64(1)}, 'reddit': {'id': 637, 'occurences': 2, 'docFreq': np.int64(1)}, 'utm_source': {'id': 638, 'occurences': 1, 'docFreq': np.int64(1)}, 'used': {'id': 639, 'occurences': 3, 'docFreq': np.int64(3)}, 'pp': {'id': 640, 'occurences': 1, 'docFreq': np.int64(1)}, 'ocr': {'id': 641, 'occurences': 1, 'docFreq': np.int64(1)}, 'vl': {'id': 642, 'occurences': 1, 'docFreq': np.int64(1)}, 'could': {'id': 643, 'occurences': 1, 'docFreq': np.int64(1)}, 'installation': {'id': 644, 'occurences': 1, 'docFreq': np.int64(1)}, 'tried': {'id': 645, 'occurences': 3, 'docFreq': np.int64(1)}, 'times': {'id': 646, 'occurences': 1, 'docFreq': np.int64(1)}, 'different': {'id': 647, 'occurences': 7, 'docFreq': np.int64(3)}, 'ways': {'id': 648, 'occurences': 1, 'docFreq': np.int64(1)}, 'faced': {'id': 649, 'occurences': 1, 'docFreq': np.int64(1)}, 'lot': {'id': 650, 'occurences': 4, 'docFreq': np.int64(3)}, 'issues': {'id': 651, 'occurences': 2, 'docFreq': np.int64(1)}, 'solve': {'id': 652, 'occurences': 1, 'docFreq': np.int64(1)}, 'created': {'id': 653, 'occurences': 1, 'docFreq': np.int64(1)}, 'environment': {'id': 654, 'occurences': 1, 'docFreq': np.int64(1)}, 'failed': {'id': 655, 'occurences': 2, 'docFreq': np.int64(1)}, 'colab': {'id': 656, 'occurences': 1, 'docFreq': np.int64(1)}, 'aws': {'id': 657, 'occurences': 1, 'docFreq': np.int64(1)}, 'ec': {'id': 658, 'occurences': 1, 'docFreq': np.int64(1)}, 'there': {'id': 659, 'occurences': 2, 'docFreq': np.int64(2)}, 'understandable': {'id': 660, 'occurences': 1, 'docFreq': np.int64(1)}, 'machine': {'id': 661, 'occurences': 28, 'docFreq': np.int64(11)}, 'ubuntu': {'id': 662, 'occurences': 1, 'docFreq': np.int64(1)}, 'gtx': {'id': 663, 'occurences': 1, 'docFreq': np.int64(1)}, 'ti': {'id': 664, 'occurences': 1, 'docFreq': np.int64(1)}, 'gb': {'id': 665, 'occurences': 1, 'docFreq': np.int64(1)}, 'ram': {'id': 666, 'occurences': 1, 'docFreq': np.int64(1)}, 'appreciate': {'id': 667, 'occurences': 1, 'docFreq': np.int64(1)}, 'rant': {'id': 668, 'occurences': 1, 'docFreq': np.int64(1)}, 'noticed': {'id': 669, 'occurences': 1, 'docFreq': np.int64(1)}, 'experienced': {'id': 670, 'occurences': 1, 'docFreq': np.int64(1)}, 'reviews': {'id': 671, 'occurences': 7, 'docFreq': np.int64(2)}, 'large': {'id': 672, 'occurences': 4, 'docFreq': np.int64(4)}, 'conferences': {'id': 673, 'occurences': 2, 'docFreq': np.int64(1)}, 'such': {'id': 674, 'occurences': 7, 'docFreq': np.int64(4)}, 'iclr': {'id': 675, 'occurences': 1, 'docFreq': np.int64(1)}, 'icml': {'id': 676, 'occurences': 1, 'docFreq': np.int64(1)}, 'aaai': {'id': 677, 'occurences': 1, 'docFreq': np.int64(1)}, 'nips': {'id': 678, 'occurences': 1, 'docFreq': np.int64(1)}, 'generally': {'id': 679, 'occurences': 1, 'docFreq': np.int64(1)}, 'inconsistent': {'id': 680, 'occurences': 1, 'docFreq': np.int64(1)}, 'getting': {'id': 681, 'occurences': 1, 'docFreq': np.int64(1)}, 'low': {'id': 682, 'occurences': 1, 'docFreq': np.int64(1)}, 'written': {'id': 683, 'occurences': 2, 'docFreq': np.int64(1)}, 'shocking': {'id': 684, 'occurences': 1, 'docFreq': np.int64(1)}, 'given': {'id': 685, 'occurences': 2, 'docFreq': np.int64(2)}, 'submissions': {'id': 686, 'occurences': 1, 'docFreq': np.int64(1)}, 'lack': {'id': 687, 'occurences': 1, 'docFreq': np.int64(1)}, 'reviewers': {'id': 688, 'occurences': 4, 'docFreq': np.int64(2)}, 'changes': {'id': 689, 'occurences': 2, 'docFreq': np.int64(2)}, 'made': {'id': 690, 'occurences': 2, 'docFreq': np.int64(2)}, 'general': {'id': 691, 'occurences': 1, 'docFreq': np.int64(1)}, 'consensus': {'id': 692, 'occurences': 1, 'docFreq': np.int64(1)}, 'researchers': {'id': 693, 'occurences': 3, 'docFreq': np.int64(2)}, 'aistats': {'id': 694, 'occurences': 2, 'docFreq': np.int64(1)}, 'ml': {'id': 695, 'occurences': 7, 'docFreq': np.int64(3)}, 'conference': {'id': 696, 'occurences': 1, 'docFreq': np.int64(1)}, 'reviewing': {'id': 697, 'occurences': 2, 'docFreq': np.int64(1)}, 'sense': {'id': 698, 'occurences': 1, 'docFreq': np.int64(1)}, 'similar': {'id': 699, 'occurences': 2, 'docFreq': np.int64(2)}, 'scientific': {'id': 700, 'occurences': 1, 'docFreq': np.int64(1)}, 'fields': {'id': 701, 'occurences': 2, 'docFreq': np.int64(2)}, 'believe': {'id': 702, 'occurences': 2, 'docFreq': np.int64(2)}, 'should': {'id': 703, 'occurences': 3, 'docFreq': np.int64(1)}, 'learn': {'id': 704, 'occurences': 1, 'docFreq': np.int64(1)}, 'example': {'id': 705, 'occurences': 1, 'docFreq': np.int64(1)}, 'dont': {'id': 706, 'occurences': 1, 'docFreq': np.int64(1)}, 'allow': {'id': 707, 'occurences': 1, 'docFreq': np.int64(1)}, 'writing': {'id': 708, 'occurences': 1, 'docFreq': np.int64(1)}, 'flag': {'id': 709, 'occurences': 1, 'docFreq': np.int64(1)}, 'chance': {'id': 710, 'occurences': 1, 'docFreq': np.int64(1)}, 'everyone': {'id': 711, 'occurences': 1, 'docFreq': np.int64(1)}, 'follow': {'id': 712, 'occurences': 1, 'docFreq': np.int64(1)}, 'format': {'id': 713, 'occurences': 1, 'docFreq': np.int64(1)}, 'easier': {'id': 714, 'occurences': 2, 'docFreq': np.int64(1)}, 'compare': {'id': 715, 'occurences': 2, 'docFreq': np.int64(2)}, 'typically': {'id': 716, 'occurences': 1, 'docFreq': np.int64(1)}, 'shorter': {'id': 717, 'occurences': 1, 'docFreq': np.int64(1)}, 'focus': {'id': 718, 'occurences': 4, 'docFreq': np.int64(3)}, 'concerns': {'id': 719, 'occurences': 1, 'docFreq': np.int64(1)}, 'pin': {'id': 720, 'occurences': 1, 'docFreq': np.int64(1)}, 'point': {'id': 721, 'occurences': 1, 'docFreq': np.int64(1)}, 'adress': {'id': 722, 'occurences': 1, 'docFreq': np.int64(1)}, 'perfect': {'id': 723, 'occurences': 1, 'docFreq': np.int64(1)}, 'feels': {'id': 724, 'occurences': 1, 'docFreq': np.int64(1)}, 'less': {'id': 725, 'occurences': 1, 'docFreq': np.int64(1)}, 'random': {'id': 726, 'occurences': 1, 'docFreq': np.int64(1)}, 'venues': {'id': 727, 'occurences': 1, 'docFreq': np.int64(1)}, 'usually': {'id': 728, 'occurences': 3, 'docFreq': np.int64(2)}, 'sure': {'id': 729, 'occurences': 1, 'docFreq': np.int64(1)}, 'actually': {'id': 730, 'occurences': 1, 'docFreq': np.int64(1)}, 'read': {'id': 731, 'occurences': 1, 'docFreq': np.int64(1)}, 'misunderstandingd': {'id': 732, 'occurences': 1, 'docFreq': np.int64(1)}, 'acceptable': {'id': 733, 'occurences': 1, 'docFreq': np.int64(1)}, 'science': {'id': 734, 'occurences': 3, 'docFreq': np.int64(1)}, 'become': {'id': 735, 'occurences': 2, 'docFreq': np.int64(2)}, 'increasingly': {'id': 736, 'occurences': 1, 'docFreq': np.int64(1)}, 'official': {'id': 737, 'occurences': 4, 'docFreq': np.int64(1)}, 'statistics': {'id': 738, 'occurences': 5, 'docFreq': np.int64(1)}, 'enables': {'id': 739, 'occurences': 2, 'docFreq': np.int64(1)}, 'automated': {'id': 740, 'occurences': 1, 'docFreq': np.int64(1)}, 'collection': {'id': 741, 'occurences': 1, 'docFreq': np.int64(1)}, 'processing': {'id': 742, 'occurences': 1, 'docFreq': np.int64(1)}, 'analysis': {'id': 743, 'occurences': 2, 'docFreq': np.int64(2)}, 'amounts': {'id': 744, 'occurences': 2, 'docFreq': np.int64(2)}, 'practices': {'id': 745, 'occurences': 1, 'docFreq': np.int64(1)}, 'place': {'id': 746, 'occurences': 1, 'docFreq': np.int64(1)}, 'timely': {'id': 747, 'occurences': 1, 'docFreq': np.int64(1)}, 'insightful': {'id': 748, 'occurences': 1, 'docFreq': np.int64(1)}, 'flexible': {'id': 749, 'occurences': 1, 'docFreq': np.int64(1)}, 'reporting': {'id': 750, 'occurences': 2, 'docFreq': np.int64(1)}, 'however': {'id': 751, 'occurences': 4, 'docFreq': np.int64(4)}, 'integrity': {'id': 752, 'occurences': 2, 'docFreq': np.int64(1)}, 'driven': {'id': 753, 'occurences': 1, 'docFreq': np.int64(1)}, 'rely': {'id': 754, 'occurences': 1, 'docFreq': np.int64(1)}, 'reliability': {'id': 755, 'occurences': 2, 'docFreq': np.int64(1)}, 'sources': {'id': 756, 'occurences': 5, 'docFreq': np.int64(1)}, 'particular': {'id': 757, 'occurences': 1, 'docFreq': np.int64(1)}, 'inevitable': {'id': 758, 'occurences': 1, 'docFreq': np.int64(1)}, 'occur': {'id': 759, 'occurences': 1, 'docFreq': np.int64(1)}, 'pose': {'id': 760, 'occurences': 1, 'docFreq': np.int64(1)}, 'significant': {'id': 761, 'occurences': 2, 'docFreq': np.int64(2)}, 'risks': {'id': 762, 'occurences': 2, 'docFreq': np.int64(1)}, 'crucial': {'id': 763, 'occurences': 1, 'docFreq': np.int64(1)}, 'address': {'id': 764, 'occurences': 2, 'docFreq': np.int64(2)}, 'context': {'id': 765, 'occurences': 3, 'docFreq': np.int64(2)}, 'paper': {'id': 766, 'occurences': 5, 'docFreq': np.int64(4)}, 'gives': {'id': 767, 'occurences': 1, 'docFreq': np.int64(1)}, 'liabilities': {'id': 768, 'occurences': 1, 'docFreq': np.int64(1)}, 'uncertainties': {'id': 769, 'occurences': 1, 'docFreq': np.int64(1)}, 'associated': {'id': 770, 'occurences': 2, 'docFreq': np.int64(2)}, 'changing': {'id': 771, 'occurences': 3, 'docFreq': np.int64(1)}, 'checklist': {'id': 772, 'occurences': 1, 'docFreq': np.int64(1)}, 'most': {'id': 773, 'occurences': 10, 'docFreq': np.int64(5)}, 'prevalent': {'id': 774, 'occurences': 1, 'docFreq': np.int64(1)}, 'origins': {'id': 775, 'occurences': 1, 'docFreq': np.int64(1)}, 'causes': {'id': 776, 'occurences': 1, 'docFreq': np.int64(1)}, 'only': {'id': 777, 'occurences': 3, 'docFreq': np.int64(3)}, 'level': {'id': 778, 'occurences': 1, 'docFreq': np.int64(1)}, 'regarding': {'id': 779, 'occurences': 1, 'docFreq': np.int64(1)}, 'ownership': {'id': 780, 'occurences': 1, 'docFreq': np.int64(1)}, 'ethics': {'id': 781, 'occurences': 1, 'docFreq': np.int64(1)}, 'regulation': {'id': 782, 'occurences': 1, 'docFreq': np.int64(1)}, 'public': {'id': 783, 'occurences': 2, 'docFreq': np.int64(1)}, 'perception': {'id': 784, 'occurences': 1, 'docFreq': np.int64(1)}, 'highlight': {'id': 785, 'occurences': 1, 'docFreq': np.int64(1)}, 'repercussions': {'id': 786, 'occurences': 1, 'docFreq': np.int64(1)}, 'statistical': {'id': 787, 'occurences': 3, 'docFreq': np.int64(1)}, 'these': {'id': 788, 'occurences': 9, 'docFreq': np.int64(7)}, 'include': {'id': 789, 'occurences': 1, 'docFreq': np.int64(1)}, 'effects': {'id': 790, 'occurences': 1, 'docFreq': np.int64(1)}, 'concept': {'id': 791, 'occurences': 2, 'docFreq': np.int64(2)}, 'drift': {'id': 792, 'occurences': 1, 'docFreq': np.int64(1)}, 'availability': {'id': 793, 'occurences': 2, 'docFreq': np.int64(2)}, 'validity': {'id': 794, 'occurences': 1, 'docFreq': np.int64(1)}, 'completeness': {'id': 795, 'occurences': 1, 'docFreq': np.int64(1)}, 'neutrality': {'id': 796, 'occurences': 1, 'docFreq': np.int64(1)}, 'potential': {'id': 797, 'occurences': 2, 'docFreq': np.int64(2)}, 'discontinuation': {'id': 798, 'occurences': 1, 'docFreq': np.int64(1)}, 'offering': {'id': 799, 'occurences': 2, 'docFreq': np.int64(2)}, 'offer': {'id': 800, 'occurences': 2, 'docFreq': np.int64(2)}, 'important': {'id': 801, 'occurences': 4, 'docFreq': np.int64(3)}, 'precautionary': {'id': 802, 'occurences': 1, 'docFreq': np.int64(1)}, 'measures': {'id': 803, 'occurences': 1, 'docFreq': np.int64(1)}, 'enhancing': {'id': 804, 'occurences': 2, 'docFreq': np.int64(2)}, 'robustness': {'id': 805, 'occurences': 2, 'docFreq': np.int64(2)}, 'sourcing': {'id': 806, 'occurences': 1, 'docFreq': np.int64(1)}, 'thorough': {'id': 807, 'occurences': 1, 'docFreq': np.int64(1)}, 'monitoring': {'id': 808, 'occurences': 1, 'docFreq': np.int64(1)}, 'doing': {'id': 809, 'occurences': 1, 'docFreq': np.int64(1)}, 'policy': {'id': 810, 'occurences': 1, 'docFreq': np.int64(1)}, 'decision': {'id': 811, 'occurences': 10, 'docFreq': np.int64(6)}, 'discourse': {'id': 812, 'occurences': 1, 'docFreq': np.int64(1)}, 'modern': {'id': 813, 'occurences': 1, 'docFreq': np.int64(1)}, 'biology': {'id': 814, 'occurences': 2, 'docFreq': np.int64(1)}, 'frequently': {'id': 815, 'occurences': 1, 'docFreq': np.int64(1)}, 'relies': {'id': 816, 'occurences': 1, 'docFreq': np.int64(1)}, 'predictions': {'id': 817, 'occurences': 1, 'docFreq': np.int64(1)}, 'processes': {'id': 818, 'occurences': 1, 'docFreq': np.int64(1)}, 'recent': {'id': 819, 'occurences': 3, 'docFreq': np.int64(3)}, 'calls': {'id': 820, 'occurences': 1, 'docFreq': np.int64(1)}, 'scrutiny': {'id': 821, 'occurences': 1, 'docFreq': np.int64(1)}, 'possible': {'id': 822, 'occurences': 1, 'docFreq': np.int64(1)}, 'limitations': {'id': 823, 'occurences': 3, 'docFreq': np.int64(2)}, 'present': {'id': 824, 'occurences': 1, 'docFreq': np.int64(1)}, 'wide': {'id': 825, 'occurences': 1, 'docFreq': np.int64(1)}, 'recommendations': {'id': 826, 'occurences': 2, 'docFreq': np.int64(1)}, 'aiming': {'id': 827, 'occurences': 2, 'docFreq': np.int64(2)}, 'establish': {'id': 828, 'occurences': 1, 'docFreq': np.int64(1)}, 'standards': {'id': 829, 'occurences': 1, 'docFreq': np.int64(1)}, 'supervised': {'id': 830, 'occurences': 1, 'docFreq': np.int64(1)}, 'adopting': {'id': 831, 'occurences': 1, 'docFreq': np.int64(1)}, 'methods': {'id': 832, 'occurences': 4, 'docFreq': np.int64(2)}, 'description': {'id': 833, 'occurences': 1, 'docFreq': np.int64(1)}, 'optimization': {'id': 834, 'occurences': 1, 'docFreq': np.int64(1)}, 'dome': {'id': 835, 'occurences': 1, 'docFreq': np.int64(1)}, 'aim': {'id': 836, 'occurences': 2, 'docFreq': np.int64(2)}, 'readers': {'id': 837, 'occurences': 1, 'docFreq': np.int64(1)}, 'better': {'id': 838, 'occurences': 2, 'docFreq': np.int64(2)}, 'understand': {'id': 839, 'occurences': 1, 'docFreq': np.int64(1)}, 'assess': {'id': 840, 'occurences': 4, 'docFreq': np.int64(4)}, 'outcome': {'id': 841, 'occurences': 1, 'docFreq': np.int64(1)}, 'formulated': {'id': 842, 'occurences': 1, 'docFreq': np.int64(1)}, 'wishing': {'id': 843, 'occurences': 1, 'docFreq': np.int64(1)}, 'pursue': {'id': 844, 'occurences': 1, 'docFreq': np.int64(1)}, 'answers': {'id': 845, 'occurences': 1, 'docFreq': np.int64(1)}, 'easily': {'id': 846, 'occurences': 1, 'docFreq': np.int64(1)}, 'included': {'id': 847, 'occurences': 1, 'docFreq': np.int64(1)}, 'supplementary': {'id': 848, 'occurences': 1, 'docFreq': np.int64(1)}, 'material': {'id': 849, 'occurences': 1, 'docFreq': np.int64(1)}, 'published': {'id': 850, 'occurences': 1, 'docFreq': np.int64(1)}, 'papers': {'id': 851, 'occurences': 2, 'docFreq': np.int64(2)}, 'curves': {'id': 852, 'occurences': 4, 'docFreq': np.int64(1)}, 'social': {'id': 853, 'occurences': 3, 'docFreq': np.int64(3)}, 'sciences': {'id': 854, 'occurences': 3, 'docFreq': np.int64(2)}, 'adopted': {'id': 855, 'occurences': 1, 'docFreq': np.int64(1)}, 'respect': {'id': 856, 'occurences': 1, 'docFreq': np.int64(1)}, 'certain': {'id': 857, 'occurences': 3, 'docFreq': np.int64(1)}, 'resource': {'id': 858, 'occurences': 1, 'docFreq': np.int64(1)}, 'e': {'id': 859, 'occurences': 3, 'docFreq': np.int64(3)}, 'g': {'id': 860, 'occurences': 2, 'docFreq': np.int64(2)}, 'iterations': {'id': 861, 'occurences': 1, 'docFreq': np.int64(1)}, 'applications': {'id': 862, 'occurences': 3, 'docFreq': np.int64(3)}, 'contexts': {'id': 863, 'occurences': 1, 'docFreq': np.int64(1)}, 'notably': {'id': 864, 'occurences': 1, 'docFreq': np.int64(1)}, 'acquisition': {'id': 865, 'occurences': 1, 'docFreq': np.int64(1)}, 'stopping': {'id': 866, 'occurences': 1, 'docFreq': np.int64(1)}, 'instance': {'id': 867, 'occurences': 1, 'docFreq': np.int64(1)}, 'combination': {'id': 868, 'occurences': 2, 'docFreq': np.int64(2)}, 'hyperparameter': {'id': 869, 'occurences': 1, 'docFreq': np.int64(1)}, 'configuration': {'id': 870, 'occurences': 1, 'docFreq': np.int64(1)}, 'providing': {'id': 871, 'occurences': 1, 'docFreq': np.int64(1)}, 'suitability': {'id': 872, 'occurences': 1, 'docFreq': np.int64(1)}, 'stage': {'id': 873, 'occurences': 1, 'docFreq': np.int64(1)}, 'often': {'id': 874, 'occurences': 1, 'docFreq': np.int64(1)}, 'expediting': {'id': 875, 'occurences': 1, 'docFreq': np.int64(1)}, 'process': {'id': 876, 'occurences': 1, 'docFreq': np.int64(1)}, 'various': {'id': 877, 'occurences': 4, 'docFreq': np.int64(4)}, 'curve': {'id': 878, 'occurences': 4, 'docFreq': np.int64(1)}, 'proposed': {'id': 879, 'occurences': 5, 'docFreq': np.int64(3)}, 'some': {'id': 880, 'occurences': 1, 'docFreq': np.int64(1)}, 'binary': {'id': 881, 'occurences': 1, 'docFreq': np.int64(1)}, 'question': {'id': 882, 'occurences': 3, 'docFreq': np.int64(2)}, 'whether': {'id': 883, 'occurences': 1, 'docFreq': np.int64(1)}, 'budget': {'id': 884, 'occurences': 1, 'docFreq': np.int64(1)}, 'outperform': {'id': 885, 'occurences': 1, 'docFreq': np.int64(1)}, 'reference': {'id': 886, 'occurences': 1, 'docFreq': np.int64(1)}, 'whereas': {'id': 887, 'occurences': 1, 'docFreq': np.int64(1)}, 'complex': {'id': 888, 'occurences': 1, 'docFreq': np.int64(1)}, 'predict': {'id': 889, 'occurences': 1, 'docFreq': np.int64(1)}, 'entire': {'id': 890, 'occurences': 2, 'docFreq': np.int64(2)}, 'contribute': {'id': 891, 'occurences': 1, 'docFreq': np.int64(1)}, 'framework': {'id': 892, 'occurences': 3, 'docFreq': np.int64(2)}, 'categorises': {'id': 893, 'occurences': 1, 'docFreq': np.int64(1)}, 'approaches': {'id': 894, 'occurences': 7, 'docFreq': np.int64(4)}, 'situation': {'id': 895, 'occurences': 1, 'docFreq': np.int64(1)}, 'intrinsic': {'id': 896, 'occurences': 3, 'docFreq': np.int64(3)}, 'resources': {'id': 897, 'occurences': 1, 'docFreq': np.int64(1)}, 'survey': {'id': 898, 'occurences': 5, 'docFreq': np.int64(2)}, 'literature': {'id': 899, 'occurences': 3, 'docFreq': np.int64(3)}, 'classify': {'id': 900, 'occurences': 1, 'docFreq': np.int64(1)}, 'online': {'id': 901, 'occurences': 2, 'docFreq': np.int64(1)}, 'active': {'id': 902, 'occurences': 5, 'docFreq': np.int64(1)}, 'paradigm': {'id': 903, 'occurences': 1, 'docFreq': np.int64(1)}, 'aims': {'id': 904, 'occurences': 2, 'docFreq': np.int64(1)}, 'informative': {'id': 905, 'occurences': 3, 'docFreq': np.int64(1)}, 'label': {'id': 906, 'occurences': 1, 'docFreq': np.int64(1)}, 'stream': {'id': 907, 'occurences': 3, 'docFreq': np.int64(1)}, 'minimizing': {'id': 908, 'occurences': 1, 'docFreq': np.int64(1)}, 'collecting': {'id': 909, 'occurences': 1, 'docFreq': np.int64(1)}, 'labeled': {'id': 910, 'occurences': 2, 'docFreq': np.int64(1)}, 'observations': {'id': 911, 'occurences': 5, 'docFreq': np.int64(1)}, 'gained': {'id': 912, 'occurences': 1, 'docFreq': np.int64(1)}, 'years': {'id': 913, 'occurences': 1, 'docFreq': np.int64(1)}, 'particularly': {'id': 914, 'occurences': 1, 'docFreq': np.int64(1)}, 'real': {'id': 915, 'occurences': 4, 'docFreq': np.int64(3)}, 'world': {'id': 916, 'occurences': 3, 'docFreq': np.int64(3)}, 'available': {'id': 917, 'occurences': 1, 'docFreq': np.int64(1)}, 'unlabeled': {'id': 918, 'occurences': 2, 'docFreq': np.int64(1)}, 'form': {'id': 919, 'occurences': 1, 'docFreq': np.int64(1)}, 'annotating': {'id': 920, 'occurences': 1, 'docFreq': np.int64(1)}, 'observation': {'id': 921, 'occurences': 1, 'docFreq': np.int64(1)}, 'consuming': {'id': 922, 'occurences': 1, 'docFreq': np.int64(1)}, 'costly': {'id': 923, 'occurences': 1, 'docFreq': np.int64(1)}, 'difficult': {'id': 924, 'occurences': 1, 'docFreq': np.int64(1)}, 'obtain': {'id': 925, 'occurences': 1, 'docFreq': np.int64(1)}, 'overcome': {'id': 926, 'occurences': 1, 'docFreq': np.int64(1)}, 'issue': {'id': 927, 'occurences': 1, 'docFreq': np.int64(1)}, 'strategies': {'id': 928, 'occurences': 1, 'docFreq': np.int64(1)}, 'last': {'id': 929, 'occurences': 1, 'docFreq': np.int64(1)}, 'decades': {'id': 930, 'occurences': 1, 'docFreq': np.int64(1)}, 'labeling': {'id': 931, 'occurences': 2, 'docFreq': np.int64(1)}, 'order': {'id': 932, 'occurences': 2, 'docFreq': np.int64(2)}, 'broadly': {'id': 933, 'occurences': 1, 'docFreq': np.int64(1)}, 'divided': {'id': 934, 'occurences': 1, 'docFreq': np.int64(1)}, 'categories': {'id': 935, 'occurences': 1, 'docFreq': np.int64(1)}, 'static': {'id': 936, 'occurences': 1, 'docFreq': np.int64(1)}, 'pool': {'id': 937, 'occurences': 3, 'docFreq': np.int64(1)}, 'involves': {'id': 938, 'occurences': 2, 'docFreq': np.int64(1)}, 'selecting': {'id': 939, 'occurences': 3, 'docFreq': np.int64(1)}, 'subset': {'id': 940, 'occurences': 1, 'docFreq': np.int64(1)}, 'closed': {'id': 941, 'occurences': 1, 'docFreq': np.int64(1)}, 'surveys': {'id': 942, 'occurences': 1, 'docFreq': np.int64(1)}, 'growing': {'id': 943, 'occurences': 1, 'docFreq': np.int64(1)}, 'streams': {'id': 944, 'occurences': 2, 'docFreq': np.int64(1)}, 'led': {'id': 945, 'occurences': 1, 'docFreq': np.int64(1)}, 'continuously': {'id': 946, 'occurences': 1, 'docFreq': np.int64(1)}, 'arrive': {'id': 947, 'occurences': 1, 'docFreq': np.int64(1)}, 'recently': {'id': 948, 'occurences': 2, 'docFreq': np.int64(2)}, 'review': {'id': 949, 'occurences': 4, 'docFreq': np.int64(2)}, 'strengths': {'id': 950, 'occurences': 1, 'docFreq': np.int64(1)}, 'well': {'id': 951, 'occurences': 1, 'docFreq': np.int64(1)}, 'challenges': {'id': 952, 'occurences': 3, 'docFreq': np.int64(3)}, 'opportunities': {'id': 953, 'occurences': 2, 'docFreq': np.int64(2)}, 'exist': {'id': 954, 'occurences': 2, 'docFreq': np.int64(2)}, 'area': {'id': 955, 'occurences': 1, 'docFreq': np.int64(1)}, 'research': {'id': 956, 'occurences': 8, 'docFreq': np.int64(5)}, 'ability': {'id': 957, 'occurences': 2, 'docFreq': np.int64(2)}, 'explain': {'id': 958, 'occurences': 1, 'docFreq': np.int64(1)}, 'decisions': {'id': 959, 'occurences': 1, 'docFreq': np.int64(1)}, 'remains': {'id': 960, 'occurences': 1, 'docFreq': np.int64(1)}, 'hurdles': {'id': 961, 'occurences': 1, 'docFreq': np.int64(1)}, 'widespread': {'id': 962, 'occurences': 1, 'docFreq': np.int64(1)}, 'adoption': {'id': 963, 'occurences': 1, 'docFreq': np.int64(1)}, 'highly': {'id': 964, 'occurences': 1, 'docFreq': np.int64(1)}, 'sensitive': {'id': 965, 'occurences': 2, 'docFreq': np.int64(2)}, 'areas': {'id': 966, 'occurences': 1, 'docFreq': np.int64(1)}, 'medicine': {'id': 967, 'occurences': 1, 'docFreq': np.int64(1)}, 'cybersecurity': {'id': 968, 'occurences': 1, 'docFreq': np.int64(1)}, 'autonomous': {'id': 969, 'occurences': 1, 'docFreq': np.int64(1)}, 'driving': {'id': 970, 'occurences': 1, 'docFreq': np.int64(1)}, 'great': {'id': 971, 'occurences': 1, 'docFreq': np.int64(1)}, 'interest': {'id': 972, 'occurences': 1, 'docFreq': np.int64(1)}, 'features': {'id': 973, 'occurences': 3, 'docFreq': np.int64(1)}, 'input': {'id': 974, 'occurences': 2, 'docFreq': np.int64(1)}, 'prompt': {'id': 975, 'occurences': 1, 'docFreq': np.int64(1)}, 'contribution': {'id': 976, 'occurences': 1, 'docFreq': np.int64(1)}, 'propose': {'id': 977, 'occurences': 2, 'docFreq': np.int64(2)}, 'identify': {'id': 978, 'occurences': 4, 'docFreq': np.int64(2)}, 'relevant': {'id': 979, 'occurences': 1, 'docFreq': np.int64(1)}, 'inspired': {'id': 980, 'occurences': 1, 'docFreq': np.int64(1)}, 'energy': {'id': 981, 'occurences': 2, 'docFreq': np.int64(1)}, 'landscapes': {'id': 982, 'occurences': 3, 'docFreq': np.int64(1)}, 'field': {'id': 983, 'occurences': 1, 'docFreq': np.int64(1)}, 'developed': {'id': 984, 'occurences': 1, 'docFreq': np.int64(1)}, 'physical': {'id': 985, 'occurences': 1, 'docFreq': np.int64(1)}, 'identifying': {'id': 986, 'occurences': 1, 'docFreq': np.int64(1)}, 'conserved': {'id': 987, 'occurences': 1, 'docFreq': np.int64(1)}, 'weights': {'id': 988, 'occurences': 1, 'docFreq': np.int64(1)}, 'within': {'id': 989, 'occurences': 1, 'docFreq': np.int64(1)}, 'groups': {'id': 990, 'occurences': 1, 'docFreq': np.int64(1)}, 'minima': {'id': 991, 'occurences': 1, 'docFreq': np.int64(1)}, 'loss': {'id': 992, 'occurences': 2, 'docFreq': np.int64(1)}, 'drivers': {'id': 993, 'occurences': 1, 'docFreq': np.int64(1)}, 'analogues': {'id': 994, 'occurences': 1, 'docFreq': np.int64(1)}, 'molecular': {'id': 995, 'occurences': 1, 'docFreq': np.int64(1)}, 'coordinate': {'id': 996, 'occurences': 1, 'docFreq': np.int64(1)}, 'invariants': {'id': 997, 'occurences': 1, 'docFreq': np.int64(1)}, 'employed': {'id': 998, 'occurences': 1, 'docFreq': np.int64(1)}, 'molecule': {'id': 999, 'occurences': 1, 'docFreq': np.int64(1)}, 'no': {'id': 1000, 'occurences': 1, 'docFreq': np.int64(1)}, 'demonstrate': {'id': 1001, 'occurences': 2, 'docFreq': np.int64(2)}, 'applicability': {'id': 1002, 'occurences': 1, 'docFreq': np.int64(1)}, 'landscape': {'id': 1003, 'occurences': 2, 'docFreq': np.int64(2)}, 'interpretable': {'id': 1004, 'occurences': 1, 'docFreq': np.int64(1)}, 'shown': {'id': 1005, 'occurences': 1, 'docFreq': np.int64(1)}, 'tremendous': {'id': 1006, 'occurences': 1, 'docFreq': np.int64(1)}, 'modeling': {'id': 1007, 'occurences': 1, 'docFreq': np.int64(1)}, 'healthcare': {'id': 1008, 'occurences': 3, 'docFreq': np.int64(1)}, 'prediction': {'id': 1009, 'occurences': 1, 'docFreq': np.int64(1)}, 'ranging': {'id': 1010, 'occurences': 1, 'docFreq': np.int64(1)}, 'disease': {'id': 1011, 'occurences': 1, 'docFreq': np.int64(1)}, 'diagnosis': {'id': 1012, 'occurences': 1, 'docFreq': np.int64(1)}, 'prognosis': {'id': 1013, 'occurences': 1, 'docFreq': np.int64(1)}, 'patient': {'id': 1014, 'occurences': 1, 'docFreq': np.int64(1)}, 'treatment': {'id': 1015, 'occurences': 1, 'docFreq': np.int64(1)}, 'nature': {'id': 1016, 'occurences': 1, 'docFreq': np.int64(1)}, 'medical': {'id': 1017, 'occurences': 1, 'docFreq': np.int64(1)}, 'privacy': {'id': 1018, 'occurences': 3, 'docFreq': np.int64(1)}, 'must': {'id': 1019, 'occurences': 1, 'docFreq': np.int64(1)}, 'considered': {'id': 1020, 'occurences': 1, 'docFreq': np.int64(1)}, 'along': {'id': 1021, 'occurences': 2, 'docFreq': np.int64(2)}, 'conduct': {'id': 1022, 'occurences': 1, 'docFreq': np.int64(1)}, 'concerning': {'id': 1023, 'occurences': 1, 'docFreq': np.int64(1)}, 'preserving': {'id': 1024, 'occurences': 2, 'docFreq': np.int64(1)}, 'ppml': {'id': 1025, 'occurences': 1, 'docFreq': np.int64(1)}, 'service': {'id': 1026, 'occurences': 1, 'docFreq': np.int64(1)}, 'perform': {'id': 1027, 'occurences': 2, 'docFreq': np.int64(2)}, 'comprehensive': {'id': 1028, 'occurences': 2, 'docFreq': np.int64(2)}, 'existing': {'id': 1029, 'occurences': 2, 'docFreq': np.int64(2)}, 'trends': {'id': 1030, 'occurences': 1, 'docFreq': np.int64(1)}, 'future': {'id': 1031, 'occurences': 2, 'docFreq': np.int64(2)}, 'directions': {'id': 1032, 'occurences': 2, 'docFreq': np.int64(2)}, 'guide': {'id': 1033, 'occurences': 1, 'docFreq': np.int64(1)}, 'development': {'id': 1034, 'occurences': 1, 'docFreq': np.int64(1)}, 'private': {'id': 1035, 'occurences': 1, 'docFreq': np.int64(1)}, 'efficient': {'id': 1036, 'occurences': 3, 'docFreq': np.int64(2)}, 'prospects': {'id': 1037, 'occurences': 1, 'docFreq': np.int64(1)}, 'translating': {'id': 1038, 'occurences': 1, 'docFreq': np.int64(1)}, 'efforts': {'id': 1039, 'occurences': 1, 'docFreq': np.int64(1)}, 'settings': {'id': 1040, 'occurences': 1, 'docFreq': np.int64(1)}, 'proliferation': {'id': 1041, 'occurences': 1, 'docFreq': np.int64(1)}, 'fake': {'id': 1042, 'occurences': 5, 'docFreq': np.int64(1)}, 'propagation': {'id': 1043, 'occurences': 1, 'docFreq': np.int64(1)}, 'media': {'id': 1044, 'occurences': 1, 'docFreq': np.int64(1)}, 'major': {'id': 1045, 'occurences': 1, 'docFreq': np.int64(1)}, 'concern': {'id': 1046, 'occurences': 1, 'docFreq': np.int64(1)}, 'devastating': {'id': 1047, 'occurences': 1, 'docFreq': np.int64(1)}, 'impacts': {'id': 1048, 'occurences': 1, 'docFreq': np.int64(1)}, 'suggested': {'id': 1049, 'occurences': 1, 'docFreq': np.int64(1)}, 'detect': {'id': 1050, 'occurences': 1, 'docFreq': np.int64(1)}, 'focused': {'id': 1051, 'occurences': 2, 'docFreq': np.int64(2)}, 'specific': {'id': 1052, 'occurences': 1, 'docFreq': np.int64(1)}, 'political': {'id': 1053, 'occurences': 1, 'docFreq': np.int64(1)}, 'leads': {'id': 1054, 'occurences': 1, 'docFreq': np.int64(1)}, 'us': {'id': 1055, 'occurences': 1, 'docFreq': np.int64(1)}, 'dataset': {'id': 1056, 'occurences': 2, 'docFreq': np.int64(1)}, 'conducted': {'id': 1057, 'occurences': 1, 'docFreq': np.int64(1)}, 'benchmark': {'id': 1058, 'occurences': 2, 'docFreq': np.int64(1)}, 'study': {'id': 1059, 'occurences': 2, 'docFreq': np.int64(1)}, 'applicable': {'id': 1060, 'occurences': 1, 'docFreq': np.int64(1)}, 'datasets': {'id': 1061, 'occurences': 2, 'docFreq': np.int64(2)}, 'accumulated': {'id': 1062, 'occurences': 1, 'docFreq': np.int64(1)}, 'largest': {'id': 1063, 'occurences': 1, 'docFreq': np.int64(1)}, 'diversified': {'id': 1064, 'occurences': 1, 'docFreq': np.int64(1)}, 'explored': {'id': 1065, 'occurences': 1, 'docFreq': np.int64(1)}, 'advanced': {'id': 1066, 'occurences': 1, 'docFreq': np.int64(1)}, 'trained': {'id': 1067, 'occurences': 2, 'docFreq': np.int64(1)}, 'language': {'id': 1068, 'occurences': 1, 'docFreq': np.int64(1)}, 'detection': {'id': 1069, 'occurences': 3, 'docFreq': np.int64(1)}, 'traditional': {'id': 1070, 'occurences': 1, 'docFreq': np.int64(1)}, 'deep': {'id': 1071, 'occurences': 1, 'docFreq': np.int64(1)}, 'ones': {'id': 1072, 'occurences': 1, 'docFreq': np.int64(1)}, 'performances': {'id': 1073, 'occurences': 1, 'docFreq': np.int64(1)}, 'aspects': {'id': 1074, 'occurences': 1, 'docFreq': np.int64(1)}, 'best': {'id': 1075, 'occurences': 2, 'docFreq': np.int64(1)}, 'our': {'id': 1076, 'occurences': 2, 'docFreq': np.int64(2)}, 'bert': {'id': 1077, 'occurences': 1, 'docFreq': np.int64(1)}, 'especially': {'id': 1078, 'occurences': 1, 'docFreq': np.int64(1)}, 'hence': {'id': 1079, 'occurences': 1, 'docFreq': np.int64(1)}, 'significantly': {'id': 1080, 'occurences': 1, 'docFreq': np.int64(1)}, 'option': {'id': 1081, 'occurences': 1, 'docFreq': np.int64(1)}, 'languages': {'id': 1082, 'occurences': 1, 'docFreq': np.int64(1)}, 'limited': {'id': 1083, 'occurences': 1, 'docFreq': np.int64(1)}, 'electronic': {'id': 1084, 'occurences': 1, 'docFreq': np.int64(1)}, 'contents': {'id': 1085, 'occurences': 1, 'docFreq': np.int64(1)}, 'carried': {'id': 1086, 'occurences': 1, 'docFreq': np.int64(1)}, 'article': {'id': 1087, 'occurences': 3, 'docFreq': np.int64(2)}, 'topic': {'id': 1088, 'occurences': 1, 'docFreq': np.int64(1)}, 'length': {'id': 1089, 'occurences': 1, 'docFreq': np.int64(1)}, 'discussed': {'id': 1090, 'occurences': 1, 'docFreq': np.int64(1)}, 'lessons': {'id': 1091, 'occurences': 1, 'docFreq': np.int64(1)}, 'learned': {'id': 1092, 'occurences': 1, 'docFreq': np.int64(1)}, 'explore': {'id': 1093, 'occurences': 1, 'docFreq': np.int64(1)}, 'further': {'id': 1094, 'occurences': 1, 'docFreq': np.int64(1)}, 'sites': {'id': 1095, 'occurences': 1, 'docFreq': np.int64(1)}, 'appropriate': {'id': 1096, 'occurences': 1, 'docFreq': np.int64(1)}, 'provides': {'id': 1097, 'occurences': 3, 'docFreq': np.int64(1)}, 'computational': {'id': 1098, 'occurences': 2, 'docFreq': np.int64(1)}, 'emotion': {'id': 1099, 'occurences': 6, 'docFreq': np.int64(1)}, 'reinforcement': {'id': 1100, 'occurences': 1, 'docFreq': np.int64(1)}, 'rl': {'id': 1101, 'occurences': 9, 'docFreq': np.int64(1)}, 'agents': {'id': 1102, 'occurences': 4, 'docFreq': np.int64(1)}, 'focuses': {'id': 1103, 'occurences': 1, 'docFreq': np.int64(1)}, 'agent': {'id': 1104, 'occurences': 4, 'docFreq': np.int64(1)}, 'robot': {'id': 1105, 'occurences': 2, 'docFreq': np.int64(1)}, 'emotions': {'id': 1106, 'occurences': 9, 'docFreq': np.int64(1)}, 'mostly': {'id': 1107, 'occurences': 1, 'docFreq': np.int64(1)}, 'ignores': {'id': 1108, 'occurences': 1, 'docFreq': np.int64(1)}, 'human': {'id': 1109, 'occurences': 2, 'docFreq': np.int64(1)}, 'recognized': {'id': 1110, 'occurences': 1, 'docFreq': np.int64(1)}, 'functional': {'id': 1111, 'occurences': 1, 'docFreq': np.int64(1)}, 'influencing': {'id': 1112, 'occurences': 1, 'docFreq': np.int64(1)}, 'motivation': {'id': 1113, 'occurences': 2, 'docFreq': np.int64(1)}, 'action': {'id': 1114, 'occurences': 1, 'docFreq': np.int64(1)}, 'therefore': {'id': 1115, 'occurences': 1, 'docFreq': np.int64(1)}, 'grounded': {'id': 1116, 'occurences': 1, 'docFreq': np.int64(1)}, 'subclass': {'id': 1117, 'occurences': 1, 'docFreq': np.int64(1)}, 'studying': {'id': 1118, 'occurences': 1, 'docFreq': np.int64(1)}, 'useful': {'id': 1119, 'occurences': 2, 'docFreq': np.int64(1)}, 'efficiency': {'id': 1120, 'occurences': 2, 'docFreq': np.int64(1)}, 'interactive': {'id': 1121, 'occurences': 1, 'docFreq': np.int64(1)}, 'interaction': {'id': 1122, 'occurences': 1, 'docFreq': np.int64(1)}, 'hri': {'id': 1123, 'occurences': 1, 'docFreq': np.int64(1)}, 'communicate': {'id': 1124, 'occurences': 1, 'docFreq': np.int64(1)}, 'state': {'id': 1125, 'occurences': 1, 'docFreq': np.int64(1)}, 'enhance': {'id': 1126, 'occurences': 1, 'docFreq': np.int64(1)}, 'investment': {'id': 1127, 'occurences': 1, 'docFreq': np.int64(1)}, 'lastly': {'id': 1128, 'occurences': 1, 'docFreq': np.int64(1)}, 'allows': {'id': 1129, 'occurences': 2, 'docFreq': np.int64(2)}, 'affective': {'id': 1130, 'occurences': 1, 'docFreq': np.int64(1)}, 'modelling': {'id': 1131, 'occurences': 1, 'docFreq': np.int64(1)}, 'am': {'id': 1132, 'occurences': 1, 'docFreq': np.int64(1)}, 'investigate': {'id': 1133, 'occurences': 1, 'docFreq': np.int64(1)}, 'theories': {'id': 1134, 'occurences': 1, 'docFreq': np.int64(1)}, 'successful': {'id': 1135, 'occurences': 1, 'docFreq': np.int64(1)}, 'background': {'id': 1136, 'occurences': 1, 'docFreq': np.int64(1)}, 'theory': {'id': 1137, 'occurences': 4, 'docFreq': np.int64(2)}, 'systematically': {'id': 1138, 'occurences': 2, 'docFreq': np.int64(1)}, 'addresses': {'id': 1139, 'occurences': 1, 'docFreq': np.int64(1)}, 'underlying': {'id': 1140, 'occurences': 2, 'docFreq': np.int64(2)}, 'homeostasis': {'id': 1141, 'occurences': 1, 'docFreq': np.int64(1)}, 'appraisal': {'id': 1142, 'occurences': 1, 'docFreq': np.int64(1)}, 'derived': {'id': 1143, 'occurences': 2, 'docFreq': np.int64(1)}, 'modelled': {'id': 1144, 'occurences': 1, 'docFreq': np.int64(1)}, 'either': {'id': 1145, 'occurences': 1, 'docFreq': np.int64(1)}, 'influence': {'id': 1146, 'occurences': 1, 'docFreq': np.int64(1)}, 'draw': {'id': 1147, 'occurences': 1, 'docFreq': np.int64(1)}, 'connections': {'id': 1148, 'occurences': 1, 'docFreq': np.int64(1)}, 'sub': {'id': 1149, 'occurences': 1, 'docFreq': np.int64(1)}, 'domains': {'id': 1150, 'occurences': 1, 'docFreq': np.int64(1)}, 'short': {'id': 1151, 'occurences': 1, 'docFreq': np.int64(1)}, 'practical': {'id': 1152, 'occurences': 1, 'docFreq': np.int64(1)}, 'engineers': {'id': 1153, 'occurences': 1, 'docFreq': np.int64(1)}, 'wanting': {'id': 1154, 'occurences': 1, 'docFreq': np.int64(1)}, 'implement': {'id': 1155, 'occurences': 1, 'docFreq': np.int64(1)}, 'identifies': {'id': 1156, 'occurences': 1, 'docFreq': np.int64(1)}, 'approximation': {'id': 1157, 'occurences': 1, 'docFreq': np.int64(1)}, 'lies': {'id': 1158, 'occurences': 1, 'docFreq': np.int64(1)}, 'heart': {'id': 1159, 'occurences': 1, 'docFreq': np.int64(1)}, 'robust': {'id': 1160, 'occurences': 1, 'docFreq': np.int64(1)}, 'maximum': {'id': 1161, 'occurences': 1, 'docFreq': np.int64(1)}, 'entropy': {'id': 1162, 'occurences': 4, 'docFreq': np.int64(2)}, 'capable': {'id': 1163, 'occurences': 1, 'docFreq': np.int64(1)}, 'dealing': {'id': 1164, 'occurences': 1, 'docFreq': np.int64(1)}, 'hundreds': {'id': 1165, 'occurences': 1, 'docFreq': np.int64(1)}, 'moments': {'id': 1166, 'occurences': 1, 'docFreq': np.int64(1)}, 'computationally': {'id': 1167, 'occurences': 1, 'docFreq': np.int64(1)}, 'approximations': {'id': 1168, 'occurences': 1, 'docFreq': np.int64(1)}, 'showcase': {'id': 1169, 'occurences': 1, 'docFreq': np.int64(1)}, 'usefulness': {'id': 1170, 'occurences': 1, 'docFreq': np.int64(1)}, 'equivalence': {'id': 1171, 'occurences': 1, 'docFreq': np.int64(1)}, 'constrained': {'id': 1172, 'occurences': 1, 'docFreq': np.int64(1)}, 'bayesian': {'id': 1173, 'occurences': 2, 'docFreq': np.int64(1)}, 'variational': {'id': 1174, 'occurences': 1, 'docFreq': np.int64(1)}, 'superiority': {'id': 1175, 'occurences': 1, 'docFreq': np.int64(1)}, 'namely': {'id': 1176, 'occurences': 1, 'docFreq': np.int64(1)}, 'fast': {'id': 1177, 'occurences': 1, 'docFreq': np.int64(1)}, 'log': {'id': 1178, 'occurences': 1, 'docFreq': np.int64(1)}, 'determinant': {'id': 1179, 'occurences': 1, 'docFreq': np.int64(1)}, 'estimation': {'id': 1180, 'occurences': 1, 'docFreq': np.int64(1)}, 'information': {'id': 1181, 'occurences': 2, 'docFreq': np.int64(2)}, 'theoretic': {'id': 1182, 'occurences': 1, 'docFreq': np.int64(1)}, 'optimisation': {'id': 1183, 'occurences': 1, 'docFreq': np.int64(1)}, 'delves': {'id': 1184, 'occurences': 1, 'docFreq': np.int64(1)}, 'innovative': {'id': 1185, 'occurences': 1, 'docFreq': np.int64(1)}, 'integration': {'id': 1186, 'occurences': 1, 'docFreq': np.int64(1)}, 'shannon': {'id': 1187, 'occurences': 2, 'docFreq': np.int64(1)}, 'rough': {'id': 1188, 'occurences': 3, 'docFreq': np.int64(1)}, 'presenting': {'id': 1189, 'occurences': 1, 'docFreq': np.int64(1)}, 'generalize': {'id': 1190, 'occurences': 1, 'docFreq': np.int64(1)}, 'conventional': {'id': 1191, 'occurences': 1, 'docFreq': np.int64(1)}, 'application': {'id': 1192, 'occurences': 2, 'docFreq': np.int64(1)}, 'uncertainty': {'id': 1193, 'occurences': 2, 'docFreq': np.int64(1)}, 'extended': {'id': 1194, 'occurences': 1, 'docFreq': np.int64(1)}, 'deeper': {'id': 1195, 'occurences': 1, 'docFreq': np.int64(1)}, 'insight': {'id': 1196, 'occurences': 1, 'docFreq': np.int64(1)}, 'interpretability': {'id': 1197, 'occurences': 1, 'docFreq': np.int64(1)}, 'introduce': {'id': 1198, 'occurences': 1, 'docFreq': np.int64(1)}, 'synergizes': {'id': 1199, 'occurences': 1, 'docFreq': np.int64(1)}, 'granularity': {'id': 1200, 'occurences': 1, 'docFreq': np.int64(1)}, 'quantification': {'id': 1201, 'occurences': 1, 'docFreq': np.int64(1)}, 'applied': {'id': 1202, 'occurences': 1, 'docFreq': np.int64(1)}, 'spectrum': {'id': 1203, 'occurences': 1, 'docFreq': np.int64(1)}, 'algorithms': {'id': 1204, 'occurences': 1, 'docFreq': np.int64(1)}, 'rigorously': {'id': 1205, 'occurences': 1, 'docFreq': np.int64(1)}, 'showcasing': {'id': 1206, 'occurences': 1, 'docFreq': np.int64(1)}, 'capability': {'id': 1207, 'occurences': 1, 'docFreq': np.int64(1)}, 'predictive': {'id': 1208, 'occurences': 1, 'docFreq': np.int64(1)}, 'illuminate': {'id': 1209, 'occurences': 1, 'docFreq': np.int64(1)}, 'complexity': {'id': 1210, 'occurences': 1, 'docFreq': np.int64(1)}, 'underscore': {'id': 1211, 'occurences': 1, 'docFreq': np.int64(1)}, 'utility': {'id': 1212, 'occurences': 1, 'docFreq': np.int64(1)}, 'integrated': {'id': 1213, 'occurences': 1, 'docFreq': np.int64(1)}, 'faceted': {'id': 1214, 'occurences': 1, 'docFreq': np.int64(1)}, 'perspective': {'id': 1215, 'occurences': 2, 'docFreq': np.int64(1)}, 'balances': {'id': 1216, 'occurences': 1, 'docFreq': np.int64(1)}, 'profound': {'id': 1217, 'occurences': 1, 'docFreq': np.int64(1)}, 'dynamics': {'id': 1218, 'occurences': 1, 'docFreq': np.int64(1)}, 'contributes': {'id': 1219, 'occurences': 1, 'docFreq': np.int64(1)}, 'groundbreaking': {'id': 1220, 'occurences': 1, 'docFreq': np.int64(1)}, 'proposing': {'id': 1221, 'occurences': 1, 'docFreq': np.int64(1)}, 'encapsulates': {'id': 1222, 'occurences': 1, 'docFreq': np.int64(1)}, 'holistic': {'id': 1223, 'occurences': 1, 'docFreq': np.int64(1)}, 'view': {'id': 1224, 'occurences': 1, 'docFreq': np.int64(1)}, 'thereby': {'id': 1225, 'occurences': 1, 'docFreq': np.int64(1)}, 'facilitating': {'id': 1226, 'occurences': 1, 'docFreq': np.int64(1)}, 'informed': {'id': 1227, 'occurences': 1, 'docFreq': np.int64(1)}, 'loremmmmmmmmmm': {'id': 1228, 'occurences': 1, 'docFreq': np.int64(1)}, 'je': {'id': 1229, 'occurences': 2, 'docFreq': np.int64(2)}, 'fais': {'id': 1230, 'occurences': 2, 'docFreq': np.int64(2)}, 'le': {'id': 1231, 'occurences': 2, 'docFreq': np.int64(2)}, 'tour': {'id': 1232, 'occurences': 2, 'docFreq': np.int64(2)}, 'du': {'id': 1233, 'occurences': 4, 'docFreq': np.int64(2)}, 'monde': {'id': 1234, 'occurences': 2, 'docFreq': np.int64(2)}, 'texte': {'id': 1235, 'occurences': 2, 'docFreq': np.int64(2)}, 'lom': {'id': 1236, 'occurences': 2, 'docFreq': np.int64(2)}}\n"
          ]
        }
      ],
      "source": [
        "vocab = corpus.freq_mots()\n",
        "vocab = vocab[0]\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BQGkwG2GRko",
        "outputId": "4625bd10-26d3-4354-efd0-ed5a9fcd4b2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tappez 'stop' pour terminer la recherche...\n",
            "\n",
            "5 Résultats :\n",
            "\n",
            "\n",
            "<Document.Document object at 0x00000162CDB5B5C0>\n",
            "Val : 0.10332787555729644\n",
            "\n",
            "\n",
            "<Document.Document object at 0x00000162CDBD3430>\n",
            "Val : 0.05581127454196056\n",
            "\n",
            "\n",
            "<Document.Document object at 0x00000162CDBCC410>\n",
            "Val : 0.04021448898487182\n",
            "\n",
            "\n",
            "<Document.Document object at 0x00000162CDB3E030>\n",
            "Val : 0.03613862028589827\n",
            "\n",
            "\n",
            "<Document.Document object at 0x00000162E200D550>\n",
            "Val : 0.03169351454602337\n"
          ]
        }
      ],
      "source": [
        "vocab = corpus.freq_mots()\n",
        "vocab = vocab[0]\n",
        "corpus.moteurRecherche(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<module 'SearchEngine' from 'c:\\\\Users\\\\albar\\\\Desktop\\\\TD-Python\\\\Projet_Python\\\\TD7\\\\TD7\\\\SearchEngine.py'>"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Importer le module\n",
        "import SearchEngine\n",
        "\n",
        "# Recharger le module si nécessaire (utile si déjà importé avant)\n",
        "import importlib\n",
        "importlib.reload(SearchEngine)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instancier un moteur de recherche sur le corpus qu'on avait deja en amont(Machine Learning)\n",
        "search = SearchEngine.SearchEngine(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tappez 'stop' pour terminer la recherche...\n",
            "   id                                              titre        date     score                                              texte\n",
            "0  22                                Le tour du monde IA  2025-11-25  0.209061       je fais le tour du monde Texte du post.lom..\n",
            "1  21                               Le sujet du monde IA  2025-11-25  0.112922  loremmmmmmmmmm, je fais le tour du monde Texte...\n",
            "2  14         Active learning for data streams: a survey  2023-02-17  0.070499  Online active learning is a paradigm in machin...\n",
            "3  11  Changing Data Sources in the Age of Machine Le...  2023-06-07  0.049056  Data science has become increasingly essential...\n",
            "4   9  [R] Struggle with PaddlePaddle OCR Vision Lang...  2025-11-24  0.048094  If anyone used PP-OCR VL could you help me wit...\n",
            "Aucun document trouvé pour cette requête.\n",
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n",
            "   id                                              titre        date     score                                              texte\n",
            "0   7       [P] Feedback/Usage of SAM (Segment Anything)  2025-11-25  0.010217  Hi folks!  I'm one of the maintainers of [Pixe...\n",
            "1   5  [R] Novel Relational Cross-Attention appears t...  2025-11-25  0.005771  Repo (MIT): [https://github.com/clowerweb/rela...\n",
            "2   4  [D] I built a reasoning pipeline that boosts 8...  2025-11-25  0.003152  This is a project I’ve been working on quietly...\n",
            "3   1                          [D] Self-Promotion Thread  2025-11-02  0.000000  Please post your personal projects, startups, ...\n",
            "4   2  [D] Monthly Who's Hiring and Who wants to be H...  2025-10-31  0.000000  **For Job Postings** please use this template ...\n",
            "   id                                              titre        date     score                                              texte\n",
            "0   4  [D] I built a reasoning pipeline that boosts 8...  2025-11-25  0.003152  This is a project I’ve been working on quietly...\n",
            "1   1                          [D] Self-Promotion Thread  2025-11-02  0.000000  Please post your personal projects, startups, ...\n",
            "2   2  [D] Monthly Who's Hiring and Who wants to be H...  2025-10-31  0.000000  **For Job Postings** please use this template ...\n",
            "3   3  [P] Knowledge Distillation: 97% Cost Reduction...  2025-11-25  0.000000  **TL DR**: Fine-tuned GPT-4.1-nano achieved 98...\n",
            "4   5  [R] Novel Relational Cross-Attention appears t...  2025-11-25  0.000000  Repo (MIT): [https://github.com/clowerweb/rela...\n",
            "   id                                              titre        date     score                                              texte\n",
            "0   1                          [D] Self-Promotion Thread  2025-11-02  0.011147  Please post your personal projects, startups, ...\n",
            "1   2  [D] Monthly Who's Hiring and Who wants to be H...  2025-10-31  0.000000  **For Job Postings** please use this template ...\n",
            "2   3  [P] Knowledge Distillation: 97% Cost Reduction...  2025-11-25  0.000000  **TL DR**: Fine-tuned GPT-4.1-nano achieved 98...\n",
            "3   4  [D] I built a reasoning pipeline that boosts 8...  2025-11-25  0.000000  This is a project I’ve been working on quietly...\n",
            "4   5  [R] Novel Relational Cross-Attention appears t...  2025-11-25  0.000000  Repo (MIT): [https://github.com/clowerweb/rela...\n",
            "   id                                              titre        date     score                                              texte\n",
            "0  19  MEMe: An Accurate Maximum Entropy Method for E...  2019-06-04  0.008063  Efficient approximation lies at the heart of l...\n",
            "1   6  [R] is there a way to decide on a model archit...  2025-11-25  0.006040  I have a data of size 16k where each sample is...\n",
            "2  18  Emotion in Reinforcement Learning Agents and R...  2017-05-15  0.005662  This article provides the first survey of comp...\n",
            "3  16  Privacy-preserving machine learning for health...  2023-03-27  0.004581  Machine Learning (ML) has recently shown treme...\n",
            "4  15  Physics-Inspired Interpretability Of Machine L...  2023-04-05  0.003946  The ability to explain decisions made by machi...\n"
          ]
        }
      ],
      "source": [
        "#Faire la recherche\n",
        "search.recherche()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
