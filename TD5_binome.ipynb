{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KxfHH4z4trML"
      },
      "outputs": [],
      "source": [
        "#Requirements :\n",
        "!pip install praw\n",
        "!pip install xmltodict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqCQ4cg0pghw"
      },
      "source": [
        "Le sujet choisi est : Machine Learning\n",
        "\n",
        "Partie 1  :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VePKbQON_JCq",
        "outputId": "ed7165b7-0163-450b-d97e-4ee6802454df"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'Document' from '/content/Document.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "# Importer le module\n",
        "import Document\n",
        "\n",
        "# Recharger le module si nécessaire (utile si déjà importé avant)\n",
        "import importlib\n",
        "importlib.reload(Document)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4D-zmju5trMO",
        "outputId": "f6518a30-e36a-43ae-eb01-573a278b11cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Reddit\n",
        "import praw\n",
        "from datetime import datetime\n",
        "import Author\n",
        "\n",
        "#instanciation de l'authentification aux outils API de Reddit\n",
        "reddit = praw.Reddit(client_id='WhOeUX8xCa_LSqqzHogeNA', client_secret='IouRe-5putFpKpyr11rOW7Wzh2Rpmw', user_agent='Web Scrapping')\n",
        "\n",
        "#Les 10 posts les plus tendance du subreddit MachineLearning :\n",
        "hot_posts = reddit.subreddit('MachineLearning').hot(limit=10)\n",
        "\n",
        "#variables principales\n",
        "docs = []                 #stocke les données d'un article (indice 0 → titre, 1 → auteur, 2 → date de publication, 3 → url, 4 → contenu textuel)\n",
        "origines = []             #tableau indiquant l'origine du document (Reddit ou Arxiv)\n",
        "id2doc = {}               #Clés : id du document, Valeurs : objet Document\n",
        "id2aut = {}               #Clés : noms des auteurs, Valeurs : id des documents publiés\n",
        "id = 1                    #id du document\n",
        "\n",
        "#pour chaque post Reddit\n",
        "for posts in hot_posts:\n",
        "  docs.append(posts.title)                #ajout du titre dans docs\n",
        "  auteurs = posts.author.name             #ajout du nom d'auteur dans docs\n",
        "  docs.append(auteurs)\n",
        "\n",
        "  texte = posts.selftext                  #contenu texte\n",
        "  texte = texte.replace(\"\\n\", \" \")                #formatage du texte pour remplacer les sauts de ligne \\n par un espace\n",
        "  texte = texte.replace(\"\\t\", \" \")\n",
        "  texte = texte.replace(\";\", \" \")\n",
        "  texte = texte.replace(\"&#x200B\", \"\")\n",
        "\n",
        "  if auteurs not in id2aut.keys():        #vérification de la présence de l'auteur dans id2aut\n",
        "    id2aut[auteurs] = Author.Author(auteurs)  #instanciation de l'objet Author avec le nouvel auteur\n",
        "    id2aut[auteurs].add(id, texte)\n",
        "  else:\n",
        "    id2aut[auteurs].add(id, texte)           #ajout du document à la production de l'auteur\n",
        "\n",
        "  dateP = posts.created_utc               #ajout de la date dans docs au format unix\n",
        "  docs.append(datetime.fromtimestamp(dateP))\n",
        "\n",
        "  docs.append(posts.url)                  #ajout de l'url dans docs\n",
        "\n",
        "  docs.append(texte)                      #ajout du contenu texte dans docs\n",
        "  docs.append(posts.num_comments)         #ajout du nombre commentaire dans docs\n",
        "\n",
        "  docs.append(\"Reddit\")                   #récupération de l'origine du post\n",
        "\n",
        "  #Une fois toutes les données du post récupérées, on instancie l'objet Document dans id2doc avec un id unique\n",
        "  id2doc[id] = Document.RedditDocument(docs[0], docs[1], docs[2], docs[3], docs[4], docs[5], docs[6])\n",
        "  id+=1                                   #incrémentation de l'id\n",
        "  docs = []                               #on vide le tableau pour pouvoir ajouter les données du document suivant\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqDPvnS7trMQ"
      },
      "source": [
        "Champs disponibles pour Reddit API :\n",
        "* 'title' (titre du post)\n",
        "* 'score' (nombre d'upvotes)\n",
        "* 'id' (identifiant du post, contenu dans l'url)\n",
        "* 'subreddit' (communauté)\n",
        "* 'url' (lien dans le post)\n",
        "* 'num_comments' (nb commentaires)\n",
        "* 'body' (contenu commentaire)\n",
        "* 'selftext' (contenu du post si c'est textuel)\n",
        "* 'author' (nom de l'utilisateur auteur du post)\n",
        "* created_utc (heure de publication du post)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "diMBjea_trMS"
      },
      "outputs": [],
      "source": [
        "#Arxiv\n",
        "import requests\n",
        "import xmltodict\n",
        "import dateutil.parser as dp\n",
        "\n",
        "#url contenant la requete (ici, 10 articles résultant de la recherche des mots Machine et Learning).\n",
        "url = 'http://export.arxiv.org/api/query?search_query=all:machine+learning&start=0&max_results=10'\n",
        "\n",
        "data = requests.get(url)                      #Obtention des données au format XML à partir de la requête\n",
        "articles = xmltodict.parse(data.text)         #données converties du format XML à dictionnaire\n",
        "\n",
        "#parcours des articles\n",
        "entries = articles['feed']['entry']\n",
        "for entry in entries:\n",
        "  a2 = [] #tableau des coauteurs\n",
        "  docs.append(entry.get(\"title\"))             #ajout du titre dans docs\n",
        "\n",
        "  summary = entry['summary']                  #contenu textuel\n",
        "  summary = summary.replace(\"\\n\", \" \")\n",
        "  summary = summary.replace(\"\\t\", \" \")                  #formatage du texte pour remplacer les sauts de ligne \\n par un espace\n",
        "  summary = summary.replace(\";\", \" \")\n",
        "  summary = summary.replace(\"&#x200B\", \"\")\n",
        "\n",
        "  authors = entry.get(\"author\", [])           #ajout de l'auteur dans docs\n",
        "  if type(authors) is dict:                   #si un seul auteur\n",
        "    a = authors.get(\"name\") #a = premier auteur\n",
        "    if a not in id2aut.keys():                #vérification de la présence de l'auteur dans id2aut\n",
        "      id2aut[a] = Author.Author(a)  #instanciation de l'objet AUthor avec le nouvel auteur\n",
        "      id2aut[a].add(id, summary)\n",
        "    else:\n",
        "      id2aut[a].add(id, summary)\n",
        "    docs.append(a)\n",
        "  else:                                       #si plusieurs auteurs\n",
        "    docs.append(authors[0].get(\"name\"))\n",
        "    for au in authors:\n",
        "      a2.append(au.get(\"name\"))\n",
        "      if au.get(\"name\") not in id2aut.keys(): #vérification de la présence de l'auteur dans id2aut\n",
        "        id2aut[au.get(\"name\")] = Author.Author(a2)\n",
        "        id2aut[au.get(\"name\")].add(id, summary)\n",
        "      else:\n",
        "        id2aut[au.get(\"name\")].add(id, summary)\n",
        "    a2 = a2[1:]                              #on met la lste des co-auteurs dans a2 (donc tous sauf le premier element)\n",
        "\n",
        "  dateP = entry.get(\"published\")              #ajout de la date de publication dans docs au format unix\n",
        "  dateP = dp.parse(dateP)\n",
        "  dateP = dateP.timestamp()\n",
        "  docs.append(datetime.fromtimestamp(dateP))\n",
        "\n",
        "  url = entry.get(\"id\")                       #ajout de l'url dans docs\n",
        "  docs.append(url)\n",
        "\n",
        "  docs.append(summary)                        #ajout du contenu textuel dans docs\n",
        "\n",
        "  docs.append(a2)\n",
        "\n",
        "  docs.append(\"Arxiv\")                        #récupération de l'origine du post\n",
        "\n",
        "  #Une fois toutes les données du post récupérées, on instancie l'objet Document dans id2doc avec un id unique\n",
        "  id2doc[id] = Document.ArxivDocument(docs[0], docs[1], docs[2], docs[3], docs[4], docs[5], docs[6])\n",
        "  docs = []                                   ##on vide le tableau pour pouvoir ajouter les données du document suivant\n",
        "  id+=1                                       #incrémentation de l'id\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjmzF6OrK084"
      },
      "source": [
        "Champs disponibles pour Arxiv :\n",
        "\n",
        "* author/name (nom d'un auteur de l'article)\n",
        "* title (titre de l'article)\n",
        "* id (identifiant de l'article)\n",
        "* published (date de première publication)\n",
        "* link (lien de l'article)\n",
        "* update (date de mise à jour de l'article)\n",
        "* summary (sommaire du contenu de l'article)\n",
        "\n",
        "\n",
        "Classe Document :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQYY1I5s-u3H",
        "outputId": "53ff3187-3a5d-46a6-f000-7095d6c395a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Titre : Minimax deviation strategies for machine learning and recognition with\n",
            "  short learning samples\n",
            "Auteur : Michail Schlesinger\n",
            "Date : 2017-07-16\n",
            "Url : http://arxiv.org/abs/1707.04849v1\n",
            "Texte : The article is devoted to the problem of small learning samples in machine learning. The flaws of ma...\n",
            "Evgeniy Vodolazskiy\n",
            "Michail Schlesinger\n"
          ]
        }
      ],
      "source": [
        "d1 = id2doc[13]     #test avec le 13ème document sur 20\n",
        "print()\n",
        "d1.__str__()   #affiche les attributs de l'objet\n",
        "d1.getType()\n",
        "print(d1.coAuteur)\n",
        "print(d1.auteur)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bHwaU5sNA-4"
      },
      "source": [
        "Classe Author :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "id": "2cm9iL1d3pLY",
        "outputId": "fb9c416f-e6be-417d-ec5f-a846caf7a261"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Liste complète des auteurs :\n",
            "- AutoModerator\n",
            "- Rajivrocks\n",
            "- SublimeSupernova\n",
            "- AgeOfEmpires4AOE4\n",
            "- Extension-Aspect9977\n",
            "- ashz8888\n",
            "- Pranav_999\n",
            "- DataPastor\n",
            "- Fair-Rain3366\n",
            "- Elad Hazan\n",
            "- Xiaojin Zhu\n",
            "- Michail Schlesinger\n",
            "- Evgeniy Vodolazskiy\n",
            "- Wei-Hung Weng\n",
            "- Samiyuru Menik\n",
            "- Lakshmish Ramaswamy\n",
            "- Amnon Shashua\n",
            "- Ayaz Akram\n",
            "- Jason Lowe-Power\n",
            "- Randy J. Chase\n",
            "- David R. Harrison\n",
            "- Amanda Burke\n",
            "- Gary M. Lackmann\n",
            "- Amy McGovern\n",
            "- Jindong Gu\n",
            "- Daniela Oelke\n",
            "- Dustin Juliano\n",
            "\n",
            "Entrez un nom d'auteur parmis ceux proposés : AutoModerator\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Nombre de documents produits : 2 - Taille moyenne des documents : 594 caractères'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "print(\"Liste complète des auteurs :\")                                #affiche la liste des noms d'auteurs\n",
        "for key in id2aut:\n",
        "  print(\"-\", key)\n",
        "\n",
        "verif = False\n",
        "print()\n",
        "while verif is False:                                                #boucle pour que l'utilisateur entre un nom d'auteur valide\n",
        "  nom  = str(input(\"Entrez un nom d'auteur parmis ceux proposés : \"))\n",
        "  if nom in id2aut.keys():\n",
        "    verif = True\n",
        "  else:\n",
        "    verif = False\n",
        "    print(\"Veuillez entrer un nom d'auteur valide parmi la liste proposée.\")\n",
        "    print()\n",
        "\n",
        "a1 = id2aut[nom]                                             #instanciation de l'auteur choisi\n",
        "a1.get_taille_moyenne_documents()                            #affichage des informations concernées"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhA9-yPy_JCt",
        "outputId": "9bb6f5f1-5253-443b-98a4-e100654758ca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'Corpus' from '/content/Corpus.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "# Importer le module\n",
        "import Corpus\n",
        "\n",
        "# Recharger le module si nécessaire (utile si déjà importé avant)\n",
        "import importlib\n",
        "importlib.reload(Corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtyrR4uQgJrt",
        "outputId": "9a852c66-47e6-44e3-eedd-9adcc9f5609a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tri par date :\n",
            "- 2025-11-10 | [D] ML Pipelines completely in Notebooks within Databricks, thoughts? (Rajivrocks)\n",
            "- 2025-11-10 | [D] Information geometry, anyone? (SublimeSupernova)\n",
            "- 2025-11-10 | [P] SDLArch-RL is now compatible with Citra!!!! And we'll be training Street Fighter 6!!! (AgeOfEmpires4AOE4)\n",
            "- 2025-11-10 | [D] AAAI-26 Student Scholar Volunteer Program (Extension-Aspect9977)\n",
            "- 2025-11-10 | Unsure about submitting to TMLR[R] (Pranav_999)\n",
            "- 2025-11-10 | [R] AlphaEvolve: Breaking 56 Years of Mathematical Stagnation (Fair-Rain3366)\n",
            "- 2025-11-09 | [P] RLHF (SFT, RM, PPO) with GPT-2 in Notebooks (ashz8888)\n",
            "- 2025-11-09 | [D] Which programming languages have you used to ship ML/AI projects in the last 3 years? (DataPastor)\n",
            "- 2025-11-02 | [D] Self-Promotion Thread (AutoModerator)\n",
            "- 2025-10-31 | [D] Monthly Who's Hiring and Who wants to be Hired? (AutoModerator)\n",
            "- 2023-01-23 | Towards Modular Machine Learning Solution Development: Benefits and\n",
            "  Trade-offs (Samiyuru Menik)\n",
            "- 2022-04-15 | A Machine Learning Tutorial for Operational Meteorology, Part I:\n",
            "  Traditional Machine Learning (Randy J. Chase)\n",
            "- 2020-12-07 | The Tribes of Machine Learning and the Realm of Computer Architecture (Ayaz Akram)\n",
            "- 2019-11-12 | Position Paper: Towards Transparent Machine Learning (Dustin Juliano)\n",
            "- 2019-09-19 | Machine Learning for Clinical Predictive Analytics (Wei-Hung Weng)\n",
            "- 2019-09-08 | Lecture Notes: Optimization for Machine Learning (Elad Hazan)\n",
            "- 2019-09-02 | Understanding Bias in Machine Learning (Jindong Gu)\n",
            "- 2018-11-11 | An Optimal Control View of Adversarial Machine Learning (Xiaojin Zhu)\n",
            "- 2017-07-16 | Minimax deviation strategies for machine learning and recognition with\n",
            "  short learning samples (Michail Schlesinger)\n",
            "\n",
            "Tri par titre\n",
            "- [D] AAAI-26 Student Scholar Volunteer Program (Extension-Aspect9977, 2025-11-10)\n",
            "- [D] Information geometry, anyone? (SublimeSupernova, 2025-11-10)\n",
            "- [D] ML Pipelines completely in Notebooks within Databricks, thoughts? (Rajivrocks, 2025-11-10)\n",
            "- [D] Monthly Who's Hiring and Who wants to be Hired? (AutoModerator, 2025-10-31)\n",
            "- [D] Self-Promotion Thread (AutoModerator, 2025-11-02)\n",
            "- [D] Which programming languages have you used to ship ML/AI projects in the last 3 years? (DataPastor, 2025-11-09)\n",
            "- [P] RLHF (SFT, RM, PPO) with GPT-2 in Notebooks (ashz8888, 2025-11-09)\n",
            "- [P] SDLArch-RL is now compatible with Citra!!!! And we'll be training Street Fighter 6!!! (AgeOfEmpires4AOE4, 2025-11-10)\n",
            "- [R] AlphaEvolve: Breaking 56 Years of Mathematical Stagnation (Fair-Rain3366, 2025-11-10)\n",
            "- A Machine Learning Tutorial for Operational Meteorology, Part I:\n",
            "  Traditional Machine Learning (Randy J. Chase, 2022-04-15)\n",
            "- An Optimal Control View of Adversarial Machine Learning (Xiaojin Zhu, 2018-11-11)\n",
            "- Introduction to Machine Learning: Class Notes 67577 (Amnon Shashua, 2009-04-23)\n",
            "- Lecture Notes: Optimization for Machine Learning (Elad Hazan, 2019-09-08)\n",
            "- Machine Learning for Clinical Predictive Analytics (Wei-Hung Weng, 2019-09-19)\n",
            "- Minimax deviation strategies for machine learning and recognition with\n",
            "  short learning samples (Michail Schlesinger, 2017-07-16)\n",
            "- Position Paper: Towards Transparent Machine Learning (Dustin Juliano, 2019-11-12)\n",
            "- The Tribes of Machine Learning and the Realm of Computer Architecture (Ayaz Akram, 2020-12-07)\n",
            "- Towards Modular Machine Learning Solution Development: Benefits and\n",
            "  Trade-offs (Samiyuru Menik, 2023-01-23)\n",
            "- Understanding Bias in Machine Learning (Jindong Gu, 2019-09-02)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Instanciation de corpus\n",
        "corpus = Corpus.Corpus(\"Machine Learning\", id2aut, id2doc)\n",
        "\n",
        "corpus.save(\"data.csv\")\n",
        "\n",
        "corpus.afficher_par_date(19)\n",
        "print()\n",
        "corpus.afficher_par_titre(19)\n",
        "print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JL-v6EtR_JCu"
      },
      "source": [
        "Partie 4: Tester le singleton"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8-z9aKT_JCu",
        "outputId": "4e8f6e2f-81a9-4e05-f605-f72dc317178e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "corpus2 = Corpus.Corpus(\"Machine_Learning_2\", id2aut, id2doc)\n",
        "print(corpus is corpus2) # retourne True si la même instance est crée (verifie qu'il est singleton)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-J2curF_JCu"
      },
      "source": [
        "Générateur de documents grˆace `a un patron de conception d’usine (factory pattern)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvZqTVCV_JCu",
        "outputId": "83203a63-3705-4498-8b04-d1bdb4c8738b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'DocumentFactory' from '/content/DocumentFactory.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "# Importer le module\n",
        "import DocumentFactory\n",
        "\n",
        "# Recharger le module si nécessaire (utile si déjà importé avant)\n",
        "import importlib\n",
        "importlib.reload(DocumentFactory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2yXFbJG_JCu"
      },
      "source": [
        "Tester le generateur avec reddit et arxiv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "GakETHBc_JCu"
      },
      "outputs": [],
      "source": [
        "from DocumentFactory import DocumentFactory  # importe la classe\n",
        "\n",
        "id2doc[id] = DocumentFactory.createDocument(\"arxiv\", titre=\"Le sujet du monde IA\", auteur=\"Aissatou\", date=datetime.today(),url=\"https://arxiv.com/xyz\", texte=\"loremmmmmmmmmm, je fais le tour du monde Texte du post.lom..\", extra=[\"Bob\", \"Charlie\"])\n",
        "\n",
        "id+=1                                       #incrémentation de l'id\n",
        "\n",
        "id2doc[id] = DocumentFactory.createDocument(\"reddit\", titre=\"Le tour du monde IA\", auteur=\"Barry\", date=datetime.today(),url=\"https://reddit.com/xyz\", texte=\"je fais le tour du monde Texte du post.lom..\", extra=50)\n",
        "\n",
        "id+=1                                       #incrémentation de l'id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "rzxYlgrA_JCv",
        "outputId": "5a55c6de-3080-4984-c870-61a8705ab8b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Titre : Le tour du monde IA\n",
            "Auteur : Barry\n",
            "Date : 2025-11-10\n",
            "Url : https://reddit.com/xyz\n",
            "Texte : je fais le tour du monde Texte du post.lom.....\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Reddit'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "d1 = id2doc[22]     #test avec le 22ème document sur 24\n",
        "print()\n",
        "d1.__str__()   #affiche les attributs de l'objet\n",
        "d1.getType()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}