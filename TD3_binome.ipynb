{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KxfHH4z4trML",
    "outputId": "71944b8e-0c27-42e0-af31-30755e737abd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in c:\\users\\albar\\anaconda3\\lib\\site-packages (7.8.1)\n",
      "Requirement already satisfied: prawcore<3,>=2.4 in c:\\users\\albar\\anaconda3\\lib\\site-packages (from praw) (2.4.0)\n",
      "Requirement already satisfied: update_checker>=0.18 in c:\\users\\albar\\anaconda3\\lib\\site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in c:\\users\\albar\\anaconda3\\lib\\site-packages (from praw) (1.8.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in c:\\users\\albar\\anaconda3\\lib\\site-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\albar\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\albar\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\albar\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\albar\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.4.26)\n",
      "Requirement already satisfied: xmltodict in c:\\users\\albar\\anaconda3\\lib\\site-packages (1.0.2)\n"
     ]
    }
   ],
   "source": [
    "#Requirements :\n",
    "!pip install praw\n",
    "!pip install xmltodict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqCQ4cg0pghw"
   },
   "source": [
    "Le sujet choisi est : Machine Learning\n",
    "\n",
    "Partie 1  :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4D-zmju5trMO",
    "outputId": "ea330261-2260-43bc-c017-8c2745dec16a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posts :\n",
      "Please post your personal projects, startups, product placements, collaboration needs, blogs etc.  Please mention the payment and pricing requirements for products and services.  Please do not post link shorteners, link aggregator websites , or auto-subscribe links.  \\--  Any abuse of trust will lead to bans.  Encourage others who create new posts for questions to post here instead!  Thread will stay alive until next one so keep posting after the date in the title.  \\--  Meta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads.\n",
      "\n",
      "**For Job Postings** please use this template  >Hiring: \\[Location\\], Salary:\\[\\], \\[Remote | Relocation\\], \\[Full Time | Contract | Part Time\\]    and \\[Brief overview, what you're looking for\\]  **For Those looking for jobs** please use this template  >Want to be Hired: \\[Location\\], Salary Expectation:\\[\\], \\[Remote | Relocation\\], \\[Full Time | Contract | Part Time\\]  Resume: \\[Link to resume\\] and \\[Brief overview, what you're looking for\\]     Please remember that this community is geared towards those with experience.\n",
      "\n",
      "I built and trained this very simple MoE \\[ [Beens-MiniMax](https://github.com/Abinesh-Mathivanan/beens-minimax) \\] from scratch in a span of 5 days. You could read more in the [report](https://github.com/Abinesh-Mathivanan/beens-minimax/blob/main/Beens_MiniMax__How_not_to_Build_an_LLM.pdf) here.\n",
      "\n",
      "Hi everyone,  I'm hoping to get a sense of what ML/AI fields are the focus of active research and development in the private sector today.  I currently work as a Data Scientist (finished my Ph.D. two years ago) and am looking to transition into a more research-focused role. To guide my efforts, I'm trying to understand which fields are in demand and what knowledge would make me a stronger candidate for these positions.  My background is strong in classical ML and statistics, so not much of NLP or CV, even though I did learn the basics of both at some point. While I enjoy these classical areas, my impression is that they might not be in the spotlight for *new* research roles at the moment. I would be very happy to be proven wrong!  If you work in an industry research or applied science role, I'd love to hear your perspective. What areas are you seeing the investment and hiring in? Are there any surprising or niche fields that still have demand?  Thanks in advance for your insights!\n",
      "\n",
      "**TL DR:** Tool-call accuracy in LLMs can be significantly improved by using natural language instead of JSON-defined schemas (\\~+18 percentage points across 6,400 trials and 10 models), while simultaneously reducing variance by 70% and token overhead by 31%. We introduce Natural Language Tools (NLT), a simple framework that decouples tool selection from response generation and eliminates programmatic format constraints and extends tool calling to models even without tool-call support.  **Resources:** [Paper](https://arxiv.org/abs/2510.14453)  **Authors:** Reid T. Johnson, Michelle D. Pain, Jordan D. West  # The Problem  Current LLMs use structured JSON/XML for tool calling, requiring outputs like:      {       \"tool_calls\": [{         \"name\": \"check_talk_to_a_human\",         \"description\": \"Used when the user requests...\"       }]     }  This structured approach creates three  bottlenecks:  1. **Task interference**: Models must simultaneously handle multiple tasks, such as understanding queries, select tools, maintaining format constraints, and  generating responses. 2. **Format burden**: Research demonstrates that the more structured a model's output, the more its performance tends to degrade ([a great paper by Tam on the subject](https://arxiv.org/abs/2408.02442)). 3. **Context bloat**: Structured schemas increase token usage, since you define not only the tool name and description, but surrounding JSON or XML syntax.  Even when tool selection is separated from response generation, probability mass is diverted toward maintaining correct formatting rather than selecting the right tools.  # Method: Natural Language Tools (NLT)  We introduce a simple three-stage framework that replaces JSON with natural language:  [Example NLT architecture with Selector \\> Parser \\> Output](https://preview.redd.it/o80vloo1ylvf1.jpg?width=2259&format=pjpg&auto=webp&s=3c75d8e6986fd499c61ebb364acb4c69abbaf157)  **Stage 1 - Tool Selection:** Model thinks through if any tools are relevant, then lists each tool with a YES/NO determination:      Thinking: (brief reasoning)     Example Tool 1 - YES/NO     Example Tool 2 - YES/NO     Example Tool 3 - YES/NO     Assessment finished.  **Stage 2 - Tool Execution:** Parser reads YES/NO decisions and executes relevant tools  **Stage 3 - Response:** Output module receives tool results and generates final response  **Evaluation:** 6,400 trials across two domains (Mental Health & Customer Service), 16 inputs per domain, 5 repetitions per input. Both original and perturbed inputs were tested to control for prompt engineering effects.  # Results  We find that NLT significantly improves tool-call performance, boosting accuracy by more than 18 percentage points (69.1% to 87.5%). Variance overall fell dramatically, falling more than 70% from .0411 to .0121 when switching from structured tool calling to NLT.  DeepSeek-V3 was a standout example, jumping from 78.4% to 94.7% accuracy while its variance dropped from 0.023 to 0.0016, going from among the least stable to the most consistent performer.  While we couldn't compare relative gain, NLT extends tool calling to models without native tool calling support (DeepSeek-R1: 94.1% accuracy).  # Basic NLT Template  **Basic NLT Prompt Template:**      You are an assistant to [Agent Name], [context].          Your mission is to identify if any of the following topics have      been brought up or are relevant:          - Tool 1 (description of when to use it)     - Tool 2 (description of when to use it)     ...          Your output should begin by thinking whether any of these are      relevant, then include the name of every tool followed by YES or NO.      End with \"Assessment finished.\"          Format:     Thinking: (reasoning)     Tool 1 - YES/NO     Tool 2 - YES/NO     ...     Assessment finished.  Full prompts and implementation details in [Appendix A](https://arxiv.org/abs/2510.14453). Works immediately with any LLM with no API changes or fine-tuning needed.  # Limitations  **Latency considerations:** NLT requires minimum two model calls per response (selector + output), whereas structured approaches can respond immediately when no tool is needed.  **Evaluation scope:**  We examined single-turn, parameterless tool selection. While less complex than existing multi-turn benchmarks, it proved sufficiently rigorous -- no model achieved 100% accuracy in either condition.  A full discussion on limitations and areas for further research can be found in section 5.9 of the paper!  # Discussion & Implications  We propose five mechanisms for these improvements:  1. **Reduced format burden**: Requiring structured outputs (e.g. JSON) may divert the model's probability mass toward syntax control rather than task accuracy 2. **Reduced task interference**: By separating the tool selection into its own distinct stage, task interference can be  sidestepped. 3. **Training alignment**: The majority of model training is on outputting human-readable text, and NLT better aligns with this training paradigm. This is further supported by our results, as open-weight models see more pronounced gains. This makes intuitive sense, as open-weight models typically have fewer resources to invest in structured tool-call training. 4. **Explicit full-catalog consideration**: Requiring the model to explicitly include each tool name in its output avoids positional bias, allowing the model to \"recollect\" each tool right before it makes a determination. 5. **Reduced context length**: Even minor increases in tokens can degrade performance, and NLT used 47.4% fewer input tokens on average than its structured tool call counterpart (largely due to removing JSON boilerplate).  For agentic systems, the NLT approach could significantly boost tool selection and accuracy, particularly for open-source models. This may be especially relevant for systems-critical tool call capabilities (i.e. safety).  For model trainers, training efforts currently devoted to SFT and RLHF for structured tool calls may be better directed toward natural-language approaches. This is less clear, as there may be cross-training effects.  One of the authors here, happy to answer any questions about experimental design, implementation, or discuss implications! What do you think?\n",
      "\n",
      "Hi guys,  I just released the source code of my most recent project: a DQN network controlling the radiator power of a house to maintain a perfect temperature when occupants are home while saving energy.  I created a custom gymnasium environment for this project that relies on thermal transfer equation, so that it recreates exactly the behavior of a real house.  The action space is discrete number between 0 and max\\_power.  The state space given is :  \\- Temperature in the inside,  \\- Temperature of the outside,  \\- Radiator state,  \\- Occupant presence,  \\- Time of day.  I am really open to suggestion and feedback, don't hesitate to contribute to this project !  [https://github.com/mp-mech-ai/radiator-rl](https://github.com/mp-mech-ai/radiator-rl)  EDIT: I am aware that for this linear behavior a statistical model would be sufficient, however I see this project as a template for more general physical behavior that could include high non-linearity or randomness.\n",
      "\n",
      "Hi all  I have a dilemma I really need help with. My old macbook pro died and I need a new one ASAP, but could probably hold off for a few weeks/months for the macbook pro 5 pro/max. I reserved the Nvidia DGX months ago, and I have the opportunity to buy it, but the last date I can buy it is tomorrow. I can also buy GCP credits.  Next year my research projects will mainly be inference of open source and closed source LLMs, with a few projects where I develop some multimodal models (likely small language models, unsure of how many parameters).  What do you think would be best for my goals?\n",
      "\n",
      "I haven't received any review assignments for ICLR yet, is that normal? I'm concerned that my paper might be desk rejected due to some kind of error.\n",
      "\n",
      "You may know that [Mila in Quebec](https://x.com/Mila_Quebec/status/1978415562276692370) is opening applications for PhD students recently, and I am considering for applying. I have searched relevent key words here, but it seems that there are not so many recent posts on studying and working experience at Mila, *so I was wondering how do you like your experience here and/or in Montreal in general? For instance, how do you like your work-life balance, Montreal's winter/weather aspects, supervisors?* To be more specific, I am interested in DL/LLM theory, AI / foundational models for (formal) math (e.g., [Goedel-Prover-V2](https://blog.goedel-prover.com/)), and/or post-training.  Thank you!\n",
      "\n",
      "I built this deep learning framework,\\[[ go-torch](https://github.com/Abinesh-Mathivanan/go-torch) \\] from scratch to learn the internals of Torch-like frameworks. You could learn from this \\[[ blog](https://abinesh-mathivanan.vercel.app/en/posts/post-5/) \\] post.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Reddit\n",
    "import praw\n",
    "\n",
    "#instanciation de l'authentification aux outils API de Reddit\n",
    "reddit = praw.Reddit(client_id='WhOeUX8xCa_LSqqzHogeNA', client_secret='IouRe-5putFpKpyr11rOW7Wzh2Rpmw', user_agent='Web Scrapping')\n",
    "\n",
    "#Les 10 posts les plus tendance du subreddit MachineLearning :\n",
    "hot_posts = reddit.subreddit('MachineLearning').hot(limit=10)\n",
    "\n",
    "#Liste des documents extraits :\n",
    "docs = []\n",
    "origines = []\n",
    "#pour chaque post Reddit\n",
    "for posts in hot_posts:\n",
    "    docs.append(posts.selftext) #ajout du contenu texte du post dans docs\n",
    "    origines.append(\"reddit\") #ajout de l'origine du post dans origines\n",
    "\n",
    "#parcours du tableau docs pour remplacer les sauts de ligne \\n par un espace\n",
    "for i in range(len(docs)):\n",
    "    docs[i] = docs[i].replace(\"\\n\", \" \")\n",
    "    docs[i] = docs[i].replace(\";\", \" \")\n",
    "    docs[i] = docs[i].replace(\"&#x200B\", \"\")\n",
    "\n",
    "\n",
    "#affichage du résultat\n",
    "print(\"Posts :\")\n",
    "for post in docs:\n",
    "  print(post)\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqDPvnS7trMQ"
   },
   "source": [
    "Champs disponibles pour Reddit API :\n",
    "* 'title' (titre du post)\n",
    "* 'score' (nombre d'upvotes)\n",
    "* 'id' (identifiant du post, contenu dans l'url)\n",
    "* 'subreddit' (communauté)\n",
    "* 'url' (lien dans le post)\n",
    "* 'num_comments' (nb commentaires)\n",
    "* 'body' (contenu commentaire)\n",
    "* 'selftext' (contenu du post si c'est textuel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "diMBjea_trMS",
    "outputId": "e42d7358-34b3-4b43-c679-d4c3c679f632"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sommaires :\n",
      "Please post your personal projects, startups, product placements, collaboration needs, blogs etc.  Please mention the payment and pricing requirements for products and services.  Please do not post link shorteners, link aggregator websites , or auto-subscribe links.  \\--  Any abuse of trust will lead to bans.  Encourage others who create new posts for questions to post here instead!  Thread will stay alive until next one so keep posting after the date in the title.  \\--  Meta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads.\n",
      "\n",
      "**For Job Postings** please use this template  >Hiring: \\[Location\\], Salary:\\[\\], \\[Remote | Relocation\\], \\[Full Time | Contract | Part Time\\]    and \\[Brief overview, what you're looking for\\]  **For Those looking for jobs** please use this template  >Want to be Hired: \\[Location\\], Salary Expectation:\\[\\], \\[Remote | Relocation\\], \\[Full Time | Contract | Part Time\\]  Resume: \\[Link to resume\\] and \\[Brief overview, what you're looking for\\]     Please remember that this community is geared towards those with experience.\n",
      "\n",
      "I built and trained this very simple MoE \\[ [Beens-MiniMax](https://github.com/Abinesh-Mathivanan/beens-minimax) \\] from scratch in a span of 5 days. You could read more in the [report](https://github.com/Abinesh-Mathivanan/beens-minimax/blob/main/Beens_MiniMax__How_not_to_Build_an_LLM.pdf) here.\n",
      "\n",
      "Hi everyone,  I'm hoping to get a sense of what ML/AI fields are the focus of active research and development in the private sector today.  I currently work as a Data Scientist (finished my Ph.D. two years ago) and am looking to transition into a more research-focused role. To guide my efforts, I'm trying to understand which fields are in demand and what knowledge would make me a stronger candidate for these positions.  My background is strong in classical ML and statistics, so not much of NLP or CV, even though I did learn the basics of both at some point. While I enjoy these classical areas, my impression is that they might not be in the spotlight for *new* research roles at the moment. I would be very happy to be proven wrong!  If you work in an industry research or applied science role, I'd love to hear your perspective. What areas are you seeing the investment and hiring in? Are there any surprising or niche fields that still have demand?  Thanks in advance for your insights!\n",
      "\n",
      "**TL DR:** Tool-call accuracy in LLMs can be significantly improved by using natural language instead of JSON-defined schemas (\\~+18 percentage points across 6,400 trials and 10 models), while simultaneously reducing variance by 70% and token overhead by 31%. We introduce Natural Language Tools (NLT), a simple framework that decouples tool selection from response generation and eliminates programmatic format constraints and extends tool calling to models even without tool-call support.  **Resources:** [Paper](https://arxiv.org/abs/2510.14453)  **Authors:** Reid T. Johnson, Michelle D. Pain, Jordan D. West  # The Problem  Current LLMs use structured JSON/XML for tool calling, requiring outputs like:      {       \"tool_calls\": [{         \"name\": \"check_talk_to_a_human\",         \"description\": \"Used when the user requests...\"       }]     }  This structured approach creates three  bottlenecks:  1. **Task interference**: Models must simultaneously handle multiple tasks, such as understanding queries, select tools, maintaining format constraints, and  generating responses. 2. **Format burden**: Research demonstrates that the more structured a model's output, the more its performance tends to degrade ([a great paper by Tam on the subject](https://arxiv.org/abs/2408.02442)). 3. **Context bloat**: Structured schemas increase token usage, since you define not only the tool name and description, but surrounding JSON or XML syntax.  Even when tool selection is separated from response generation, probability mass is diverted toward maintaining correct formatting rather than selecting the right tools.  # Method: Natural Language Tools (NLT)  We introduce a simple three-stage framework that replaces JSON with natural language:  [Example NLT architecture with Selector \\> Parser \\> Output](https://preview.redd.it/o80vloo1ylvf1.jpg?width=2259&format=pjpg&auto=webp&s=3c75d8e6986fd499c61ebb364acb4c69abbaf157)  **Stage 1 - Tool Selection:** Model thinks through if any tools are relevant, then lists each tool with a YES/NO determination:      Thinking: (brief reasoning)     Example Tool 1 - YES/NO     Example Tool 2 - YES/NO     Example Tool 3 - YES/NO     Assessment finished.  **Stage 2 - Tool Execution:** Parser reads YES/NO decisions and executes relevant tools  **Stage 3 - Response:** Output module receives tool results and generates final response  **Evaluation:** 6,400 trials across two domains (Mental Health & Customer Service), 16 inputs per domain, 5 repetitions per input. Both original and perturbed inputs were tested to control for prompt engineering effects.  # Results  We find that NLT significantly improves tool-call performance, boosting accuracy by more than 18 percentage points (69.1% to 87.5%). Variance overall fell dramatically, falling more than 70% from .0411 to .0121 when switching from structured tool calling to NLT.  DeepSeek-V3 was a standout example, jumping from 78.4% to 94.7% accuracy while its variance dropped from 0.023 to 0.0016, going from among the least stable to the most consistent performer.  While we couldn't compare relative gain, NLT extends tool calling to models without native tool calling support (DeepSeek-R1: 94.1% accuracy).  # Basic NLT Template  **Basic NLT Prompt Template:**      You are an assistant to [Agent Name], [context].          Your mission is to identify if any of the following topics have      been brought up or are relevant:          - Tool 1 (description of when to use it)     - Tool 2 (description of when to use it)     ...          Your output should begin by thinking whether any of these are      relevant, then include the name of every tool followed by YES or NO.      End with \"Assessment finished.\"          Format:     Thinking: (reasoning)     Tool 1 - YES/NO     Tool 2 - YES/NO     ...     Assessment finished.  Full prompts and implementation details in [Appendix A](https://arxiv.org/abs/2510.14453). Works immediately with any LLM with no API changes or fine-tuning needed.  # Limitations  **Latency considerations:** NLT requires minimum two model calls per response (selector + output), whereas structured approaches can respond immediately when no tool is needed.  **Evaluation scope:**  We examined single-turn, parameterless tool selection. While less complex than existing multi-turn benchmarks, it proved sufficiently rigorous -- no model achieved 100% accuracy in either condition.  A full discussion on limitations and areas for further research can be found in section 5.9 of the paper!  # Discussion & Implications  We propose five mechanisms for these improvements:  1. **Reduced format burden**: Requiring structured outputs (e.g. JSON) may divert the model's probability mass toward syntax control rather than task accuracy 2. **Reduced task interference**: By separating the tool selection into its own distinct stage, task interference can be  sidestepped. 3. **Training alignment**: The majority of model training is on outputting human-readable text, and NLT better aligns with this training paradigm. This is further supported by our results, as open-weight models see more pronounced gains. This makes intuitive sense, as open-weight models typically have fewer resources to invest in structured tool-call training. 4. **Explicit full-catalog consideration**: Requiring the model to explicitly include each tool name in its output avoids positional bias, allowing the model to \"recollect\" each tool right before it makes a determination. 5. **Reduced context length**: Even minor increases in tokens can degrade performance, and NLT used 47.4% fewer input tokens on average than its structured tool call counterpart (largely due to removing JSON boilerplate).  For agentic systems, the NLT approach could significantly boost tool selection and accuracy, particularly for open-source models. This may be especially relevant for systems-critical tool call capabilities (i.e. safety).  For model trainers, training efforts currently devoted to SFT and RLHF for structured tool calls may be better directed toward natural-language approaches. This is less clear, as there may be cross-training effects.  One of the authors here, happy to answer any questions about experimental design, implementation, or discuss implications! What do you think?\n",
      "\n",
      "Hi guys,  I just released the source code of my most recent project: a DQN network controlling the radiator power of a house to maintain a perfect temperature when occupants are home while saving energy.  I created a custom gymnasium environment for this project that relies on thermal transfer equation, so that it recreates exactly the behavior of a real house.  The action space is discrete number between 0 and max\\_power.  The state space given is :  \\- Temperature in the inside,  \\- Temperature of the outside,  \\- Radiator state,  \\- Occupant presence,  \\- Time of day.  I am really open to suggestion and feedback, don't hesitate to contribute to this project !  [https://github.com/mp-mech-ai/radiator-rl](https://github.com/mp-mech-ai/radiator-rl)  EDIT: I am aware that for this linear behavior a statistical model would be sufficient, however I see this project as a template for more general physical behavior that could include high non-linearity or randomness.\n",
      "\n",
      "Hi all  I have a dilemma I really need help with. My old macbook pro died and I need a new one ASAP, but could probably hold off for a few weeks/months for the macbook pro 5 pro/max. I reserved the Nvidia DGX months ago, and I have the opportunity to buy it, but the last date I can buy it is tomorrow. I can also buy GCP credits.  Next year my research projects will mainly be inference of open source and closed source LLMs, with a few projects where I develop some multimodal models (likely small language models, unsure of how many parameters).  What do you think would be best for my goals?\n",
      "\n",
      "I haven't received any review assignments for ICLR yet, is that normal? I'm concerned that my paper might be desk rejected due to some kind of error.\n",
      "\n",
      "You may know that [Mila in Quebec](https://x.com/Mila_Quebec/status/1978415562276692370) is opening applications for PhD students recently, and I am considering for applying. I have searched relevent key words here, but it seems that there are not so many recent posts on studying and working experience at Mila, *so I was wondering how do you like your experience here and/or in Montreal in general? For instance, how do you like your work-life balance, Montreal's winter/weather aspects, supervisors?* To be more specific, I am interested in DL/LLM theory, AI / foundational models for (formal) math (e.g., [Goedel-Prover-V2](https://blog.goedel-prover.com/)), and/or post-training.  Thank you!\n",
      "\n",
      "I built this deep learning framework,\\[[ go-torch](https://github.com/Abinesh-Mathivanan/go-torch) \\] from scratch to learn the internals of Torch-like frameworks. You could learn from this \\[[ blog](https://abinesh-mathivanan.vercel.app/en/posts/post-5/) \\] post.\n",
      "\n",
      "Lecture notes on optimization for machine learning, derived from a course at Princeton University and tutorials given in MLSS, Buenos Aires, as well as Simons Foundation, Berkeley.\n",
      "\n",
      "I describe an optimal control view of adversarial machine learning, where the dynamical system is the machine learner, the input are adversarial actions, and the control costs are defined by the adversary's goals to do harm and be hard to detect. This view encompasses many types of adversarial machine learning, including test-item attacks, training-data poisoning, and adversarial reward shaping. The view encourages adversarial machine learning researcher to utilize advances in control theory and reinforcement learning.\n",
      "\n",
      "The article is devoted to the problem of small learning samples in machine learning. The flaws of maximum likelihood learning and minimax learning are looked into and the concept of minimax deviation learning is introduced that is free of those flaws.\n",
      "\n",
      "In this chapter, we provide a brief overview of applying machine learning techniques for clinical prediction tasks. We begin with a quick introduction to the concepts of machine learning and outline some of the most common machine learning algorithms. Next, we demonstrate how to apply the algorithms with appropriate toolkits to conduct machine learning experiments for clinical prediction tasks. The objectives of this chapter are to (1) understand the basics of machine learning techniques and the reasons behind why they are useful for solving clinical prediction problems, (2) understand the intuition behind some machine learning models, including regression, decision trees, and support vector machines, and (3) understand how to apply these models to clinical prediction problems using publicly available datasets via case studies.\n",
      "\n",
      "Machine learning technologies have demonstrated immense capabilities in various domains. They play a key role in the success of modern businesses. However, adoption of machine learning technologies has a lot of untouched potential. Cost of developing custom machine learning solutions that solve unique business problems is a major inhibitor to far-reaching adoption of machine learning technologies. We recognize that the monolithic nature prevalent in today's machine learning applications stands in the way of efficient and cost effective customized machine learning solution development. In this work we explore the benefits of modular machine learning solutions and discuss how modular machine learning solutions can overcome some of the major solution engineering limitations of monolithic machine learning solutions. We analyze the trade-offs between modular and monolithic machine learning solutions through three deep learning problems; one text based and the two image based. Our experimental results show that modular machine learning solutions have a promising potential to reap the solution engineering advantages of modularity while gaining performance and data advantages in a way the monolithic machine learning solutions do not permit.\n",
      "\n",
      "Introduction to Machine learning covering Statistical Inference (Bayes, EM, ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering), and PAC learning (the Formal model, VC dimension, Double Sampling theorem).\n",
      "\n",
      "Machine learning techniques have influenced the field of computer architecture like many other fields. This paper studies how the fundamental machine learning techniques can be applied towards computer architecture problems. We also provide a detailed survey of computer architecture research that employs different machine learning methods. Finally, we present some future opportunities and the outstanding challenges that need to be overcome to exploit full potential of machine learning for computer architecture.\n",
      "\n",
      "Recently, the use of machine learning in meteorology has increased greatly. While many machine learning methods are not new, university classes on machine learning are largely unavailable to meteorology students and are not required to become a meteorologist. The lack of formal instruction has contributed to perception that machine learning methods are 'black boxes' and thus end-users are hesitant to apply the machine learning methods in their every day workflow. To reduce the opaqueness of machine learning methods and lower hesitancy towards machine learning in meteorology, this paper provides a survey of some of the most common machine learning methods. A familiar meteorological example is used to contextualize the machine learning methods while also discussing machine learning topics using plain language. The following machine learning methods are demonstrated: linear regression; logistic regression; decision trees; random forest; gradient boosted decision trees; naive Bayes; and support vector machines. Beyond discussing the different methods, the paper also contains discussions on the general machine learning process as well as best practices to enable readers to apply machine learning to their own datasets. Furthermore, all code (in the form of Jupyter notebooks and Google Colaboratory notebooks) used to make the examples in the paper is provided in an effort to catalyse the use of machine learning in meteorology.\n",
      "\n",
      "Bias is known to be an impediment to fair decisions in many domains such as human resources, the public sector, health care etc. Recently, hope has been expressed that the use of machine learning methods for taking such decisions would diminish or even resolve the problem. At the same time, machine learning experts warn that machine learning models can be biased as well. In this article, our goal is to explain the issue of bias in machine learning from a technical perspective and to illustrate the impact that biased data can have on a machine learning model. To reach such a goal, we develop interactive plots to visualizing the bias learned from synthetic data.\n",
      "\n",
      "Transparent machine learning is introduced as an alternative form of machine learning, where both the model and the learning system are represented in source code form. The goal of this project is to enable direct human understanding of machine learning models, giving us the ability to learn, verify, and refine them as programs. If solved, this technology could represent a best-case scenario for the safety and security of AI systems going forward.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Arxiv\n",
    "import requests\n",
    "import xmltodict\n",
    "\n",
    "#url contenant la requete (ici, 10 articles résultant de la recherche des mots Machine et Learning.\n",
    "url = 'http://export.arxiv.org/api/query?search_query=all:machine+learning&start=0&max_results=10'\n",
    "\n",
    "#Obtention des données au format XML à partir de la requête\n",
    "data = requests.get(url)\n",
    "\n",
    "#données converties du format XML à dictionnaire\n",
    "articles = xmltodict.parse(data.text)\n",
    "\n",
    "#parcours des articles obtenus pour en extraire le sommaire\n",
    "entries = articles['feed']['entry']\n",
    "for entry in entries:\n",
    "  summary = entry['summary']\n",
    "  docs.append(summary) #ajout du contenu texte du sommaire au tableau docs\n",
    "  origines.append(\"arxiv\") #ajout de l'origine du post dans origines\n",
    "\n",
    "#parcours du tableau docs pour remplacer les sauts de ligne \\n par un espace\n",
    "for i in range(len(docs)):\n",
    "    docs[i] = docs[i].replace(\"\\n\", \" \")\n",
    "\n",
    "#Affichage du résultat\n",
    "print(\"Sommaires :\")\n",
    "for elt in docs:\n",
    "  print(elt)\n",
    "  print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjmzF6OrK084"
   },
   "source": [
    "Champs disponibles pour Arxiv :\n",
    "\n",
    "* author/name (nom d'un auteur de l'article)\n",
    "* title (titre de l'article)\n",
    "* id (lien de l'article)\n",
    "* published (date de première publication)\n",
    "* update (date de mise à jour de l'article)\n",
    "* summary (sommaire du contenu de l'article)\n",
    "\n",
    "\n",
    "Partie 2 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3bkh5AJwMq2W",
    "outputId": "19b22e09-093a-431a-bb96-e03c5d9d8a3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id                                              texte origine\n",
      "0    1  Please post your personal projects, startups, ...  reddit\n",
      "1    2  **For Job Postings** please use this template ...  reddit\n",
      "2    3  I built and trained this very simple MoE \\[ [B...  reddit\n",
      "3    4  Hi everyone,  I'm hoping to get a sense of wha...  reddit\n",
      "4    5  **TL DR:** Tool-call accuracy in LLMs can be s...  reddit\n",
      "5    6  Hi guys,  I just released the source code of m...  reddit\n",
      "6    7  Hi all  I have a dilemma I really need help wi...  reddit\n",
      "7    8  I haven't received any review assignments for ...  reddit\n",
      "8    9  You may know that [Mila in Quebec](https://x.c...  reddit\n",
      "9   10  I built this deep learning framework,\\[[ go-to...  reddit\n",
      "10  11  Lecture notes on optimization for machine lear...   arxiv\n",
      "11  12  I describe an optimal control view of adversar...   arxiv\n",
      "12  13  The article is devoted to the problem of small...   arxiv\n",
      "13  14  In this chapter, we provide a brief overview o...   arxiv\n",
      "14  15  Machine learning technologies have demonstrate...   arxiv\n",
      "15  16  Introduction to Machine learning covering Stat...   arxiv\n",
      "16  17  Machine learning techniques have influenced th...   arxiv\n",
      "17  18  Recently, the use of machine learning in meteo...   arxiv\n",
      "18  19  Bias is known to be an impediment to fair deci...   arxiv\n",
      "19  20  Transparent machine learning is introduced as ...   arxiv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "#création du DataFrame\n",
    "df = pd.DataFrame({'id' : range(1, len(docs)+1), 'texte': docs, 'origine': origines})\n",
    "\n",
    "#Conversion au format csv en définissant le séparateur \\t\n",
    "df.to_csv('td3_Binome.csv', sep='\\t', index=False, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "#chargement des données sans passer par les API\n",
    "new_df = pd.read_csv('td3_Binome.csv', sep='\\t', on_bad_lines='warn', quoting=csv.QUOTE_ALL)\n",
    "\n",
    "#affichage\n",
    "print(new_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partie 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "le nombre de docs est: 20\n",
      "    id  nb_mots  nb_phrases\n",
      "0    1      115           9\n",
      "1    2       86           2\n",
      "2    3       27           6\n",
      "3    4      183           9\n",
      "4    5     1042          77\n",
      "5    6      161           8\n",
      "6    7      117           6\n",
      "7    8       27           2\n",
      "8    9       98           8\n",
      "9   10       22           6\n",
      "10  11       27           2\n",
      "11  12       75           4\n",
      "12  13       41           3\n",
      "13  14      123           5\n",
      "14  15      177           9\n",
      "15  16       30           2\n",
      "16  17       71           5\n",
      "17  18      213           9\n",
      "18  19      114           6\n",
      "19  20       72           4\n",
      "Le nouveau nombre de docs est : 20\n",
      "La Chaine de caractere unique contenant tous les documents : \n",
      " Please post your personal projects, startups, product placements, collaboration needs, blogs etc.  Please mention the payment and pricing requirements for products and services.  Please do not post link shorteners, link aggregator websites , or auto-subscribe links.  \\--  Any abuse of trust will lead to bans.  Encourage others who create new posts for questions to post here instead!  Thread will stay alive until next one so keep posting after the date in the title.  \\--  Meta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads. **For Job Postings** please use this template  >Hiring: \\[Location\\], Salary:\\[\\], \\[Remote | Relocation\\], \\[Full Time | Contract | Part Time\\]    and \\[Brief overview, what you're looking for\\]  **For Those looking for jobs** please use this template  >Want to be Hired: \\[Location\\], Salary Expectation:\\[\\], \\[Remote | Relocation\\], \\[Full Time | Contract | Part Time\\]  Resume: \\[Link to resume\\] and \\[Brief overview, what you're looking for\\]     Please remember that this community is geared towards those with experience. I built and trained this very simple MoE \\[ [Beens-MiniMax](https://github.com/Abinesh-Mathivanan/beens-minimax) \\] from scratch in a span of 5 days. You could read more in the [report](https://github.com/Abinesh-Mathivanan/beens-minimax/blob/main/Beens_MiniMax__How_not_to_Build_an_LLM.pdf) here. Hi everyone,  I'm hoping to get a sense of what ML/AI fields are the focus of active research and development in the private sector today.  I currently work as a Data Scientist (finished my Ph.D. two years ago) and am looking to transition into a more research-focused role. To guide my efforts, I'm trying to understand which fields are in demand and what knowledge would make me a stronger candidate for these positions.  My background is strong in classical ML and statistics, so not much of NLP or CV, even though I did learn the basics of both at some point. While I enjoy these classical areas, my impression is that they might not be in the spotlight for *new* research roles at the moment. I would be very happy to be proven wrong!  If you work in an industry research or applied science role, I'd love to hear your perspective. What areas are you seeing the investment and hiring in? Are there any surprising or niche fields that still have demand?  Thanks in advance for your insights! **TL DR:** Tool-call accuracy in LLMs can be significantly improved by using natural language instead of JSON-defined schemas (\\~+18 percentage points across 6,400 trials and 10 models), while simultaneously reducing variance by 70% and token overhead by 31%. We introduce Natural Language Tools (NLT), a simple framework that decouples tool selection from response generation and eliminates programmatic format constraints and extends tool calling to models even without tool-call support.  **Resources:** [Paper](https://arxiv.org/abs/2510.14453)  **Authors:** Reid T. Johnson, Michelle D. Pain, Jordan D. West  # The Problem  Current LLMs use structured JSON/XML for tool calling, requiring outputs like:      {       \"tool_calls\": [{         \"name\": \"check_talk_to_a_human\",         \"description\": \"Used when the user requests...\"       }]     }  This structured approach creates three  bottlenecks:  1. **Task interference**: Models must simultaneously handle multiple tasks, such as understanding queries, select tools, maintaining format constraints, and  generating responses. 2. **Format burden**: Research demonstrates that the more structured a model's output, the more its performance tends to degrade ([a great paper by Tam on the subject](https://arxiv.org/abs/2408.02442)). 3. **Context bloat**: Structured schemas increase token usage, since you define not only the tool name and description, but surrounding JSON or XML syntax.  Even when tool selection is separated from response generation, probability mass is diverted toward maintaining correct formatting rather than selecting the right tools.  # Method: Natural Language Tools (NLT)  We introduce a simple three-stage framework that replaces JSON with natural language:  [Example NLT architecture with Selector \\> Parser \\> Output](https://preview.redd.it/o80vloo1ylvf1.jpg?width=2259&format=pjpg&auto=webp&s=3c75d8e6986fd499c61ebb364acb4c69abbaf157)  **Stage 1 - Tool Selection:** Model thinks through if any tools are relevant, then lists each tool with a YES/NO determination:      Thinking: (brief reasoning)     Example Tool 1 - YES/NO     Example Tool 2 - YES/NO     Example Tool 3 - YES/NO     Assessment finished.  **Stage 2 - Tool Execution:** Parser reads YES/NO decisions and executes relevant tools  **Stage 3 - Response:** Output module receives tool results and generates final response  **Evaluation:** 6,400 trials across two domains (Mental Health & Customer Service), 16 inputs per domain, 5 repetitions per input. Both original and perturbed inputs were tested to control for prompt engineering effects.  # Results  We find that NLT significantly improves tool-call performance, boosting accuracy by more than 18 percentage points (69.1% to 87.5%). Variance overall fell dramatically, falling more than 70% from .0411 to .0121 when switching from structured tool calling to NLT.  DeepSeek-V3 was a standout example, jumping from 78.4% to 94.7% accuracy while its variance dropped from 0.023 to 0.0016, going from among the least stable to the most consistent performer.  While we couldn't compare relative gain, NLT extends tool calling to models without native tool calling support (DeepSeek-R1: 94.1% accuracy).  # Basic NLT Template  **Basic NLT Prompt Template:**      You are an assistant to [Agent Name], [context].          Your mission is to identify if any of the following topics have      been brought up or are relevant:          - Tool 1 (description of when to use it)     - Tool 2 (description of when to use it)     ...          Your output should begin by thinking whether any of these are      relevant, then include the name of every tool followed by YES or NO.      End with \"Assessment finished.\"          Format:     Thinking: (reasoning)     Tool 1 - YES/NO     Tool 2 - YES/NO     ...     Assessment finished.  Full prompts and implementation details in [Appendix A](https://arxiv.org/abs/2510.14453). Works immediately with any LLM with no API changes or fine-tuning needed.  # Limitations  **Latency considerations:** NLT requires minimum two model calls per response (selector + output), whereas structured approaches can respond immediately when no tool is needed.  **Evaluation scope:**  We examined single-turn, parameterless tool selection. While less complex than existing multi-turn benchmarks, it proved sufficiently rigorous -- no model achieved 100% accuracy in either condition.  A full discussion on limitations and areas for further research can be found in section 5.9 of the paper!  # Discussion & Implications  We propose five mechanisms for these improvements:  1. **Reduced format burden**: Requiring structured outputs (e.g. JSON) may divert the model's probability mass toward syntax control rather than task accuracy 2. **Reduced task interference**: By separating the tool selection into its own distinct stage, task interference can be  sidestepped. 3. **Training alignment**: The majority of model training is on outputting human-readable text, and NLT better aligns with this training paradigm. This is further supported by our results, as open-weight models see more pronounced gains. This makes intuitive sense, as open-weight models typically have fewer resources to invest in structured tool-call training. 4. **Explicit full-catalog consideration**: Requiring the model to explicitly include each tool name in its output avoids positional bias, allowing the model to \"recollect\" each tool right before it makes a determination. 5. **Reduced context length**: Even minor increases in tokens can degrade performance, and NLT used 47.4% fewer input tokens on average than its structured tool call counterpart (largely due to removing JSON boilerplate).  For agentic systems, the NLT approach could significantly boost tool selection and accuracy, particularly for open-source models. This may be especially relevant for systems-critical tool call capabilities (i.e. safety).  For model trainers, training efforts currently devoted to SFT and RLHF for structured tool calls may be better directed toward natural-language approaches. This is less clear, as there may be cross-training effects.  One of the authors here, happy to answer any questions about experimental design, implementation, or discuss implications! What do you think? Hi guys,  I just released the source code of my most recent project: a DQN network controlling the radiator power of a house to maintain a perfect temperature when occupants are home while saving energy.  I created a custom gymnasium environment for this project that relies on thermal transfer equation, so that it recreates exactly the behavior of a real house.  The action space is discrete number between 0 and max\\_power.  The state space given is :  \\- Temperature in the inside,  \\- Temperature of the outside,  \\- Radiator state,  \\- Occupant presence,  \\- Time of day.  I am really open to suggestion and feedback, don't hesitate to contribute to this project !  [https://github.com/mp-mech-ai/radiator-rl](https://github.com/mp-mech-ai/radiator-rl)  EDIT: I am aware that for this linear behavior a statistical model would be sufficient, however I see this project as a template for more general physical behavior that could include high non-linearity or randomness. Hi all  I have a dilemma I really need help with. My old macbook pro died and I need a new one ASAP, but could probably hold off for a few weeks/months for the macbook pro 5 pro/max. I reserved the Nvidia DGX months ago, and I have the opportunity to buy it, but the last date I can buy it is tomorrow. I can also buy GCP credits.  Next year my research projects will mainly be inference of open source and closed source LLMs, with a few projects where I develop some multimodal models (likely small language models, unsure of how many parameters).  What do you think would be best for my goals? I haven't received any review assignments for ICLR yet, is that normal? I'm concerned that my paper might be desk rejected due to some kind of error. You may know that [Mila in Quebec](https://x.com/Mila_Quebec/status/1978415562276692370) is opening applications for PhD students recently, and I am considering for applying. I have searched relevent key words here, but it seems that there are not so many recent posts on studying and working experience at Mila, *so I was wondering how do you like your experience here and/or in Montreal in general? For instance, how do you like your work-life balance, Montreal's winter/weather aspects, supervisors?* To be more specific, I am interested in DL/LLM theory, AI / foundational models for (formal) math (e.g., [Goedel-Prover-V2](https://blog.goedel-prover.com/)), and/or post-training.  Thank you! I built this deep learning framework,\\[[ go-torch](https://github.com/Abinesh-Mathivanan/go-torch) \\] from scratch to learn the internals of Torch-like frameworks. You could learn from this \\[[ blog](https://abinesh-mathivanan.vercel.app/en/posts/post-5/) \\] post. Lecture notes on optimization for machine learning, derived from a course at Princeton University and tutorials given in MLSS, Buenos Aires, as well as Simons Foundation, Berkeley. I describe an optimal control view of adversarial machine learning, where the dynamical system is the machine learner, the input are adversarial actions, and the control costs are defined by the adversary's goals to do harm and be hard to detect. This view encompasses many types of adversarial machine learning, including test-item attacks, training-data poisoning, and adversarial reward shaping. The view encourages adversarial machine learning researcher to utilize advances in control theory and reinforcement learning. The article is devoted to the problem of small learning samples in machine learning. The flaws of maximum likelihood learning and minimax learning are looked into and the concept of minimax deviation learning is introduced that is free of those flaws. In this chapter, we provide a brief overview of applying machine learning techniques for clinical prediction tasks. We begin with a quick introduction to the concepts of machine learning and outline some of the most common machine learning algorithms. Next, we demonstrate how to apply the algorithms with appropriate toolkits to conduct machine learning experiments for clinical prediction tasks. The objectives of this chapter are to (1) understand the basics of machine learning techniques and the reasons behind why they are useful for solving clinical prediction problems, (2) understand the intuition behind some machine learning models, including regression, decision trees, and support vector machines, and (3) understand how to apply these models to clinical prediction problems using publicly available datasets via case studies. Machine learning technologies have demonstrated immense capabilities in various domains. They play a key role in the success of modern businesses. However, adoption of machine learning technologies has a lot of untouched potential. Cost of developing custom machine learning solutions that solve unique business problems is a major inhibitor to far-reaching adoption of machine learning technologies. We recognize that the monolithic nature prevalent in today's machine learning applications stands in the way of efficient and cost effective customized machine learning solution development. In this work we explore the benefits of modular machine learning solutions and discuss how modular machine learning solutions can overcome some of the major solution engineering limitations of monolithic machine learning solutions. We analyze the trade-offs between modular and monolithic machine learning solutions through three deep learning problems; one text based and the two image based. Our experimental results show that modular machine learning solutions have a promising potential to reap the solution engineering advantages of modularity while gaining performance and data advantages in a way the monolithic machine learning solutions do not permit. Introduction to Machine learning covering Statistical Inference (Bayes, EM, ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering), and PAC learning (the Formal model, VC dimension, Double Sampling theorem). Machine learning techniques have influenced the field of computer architecture like many other fields. This paper studies how the fundamental machine learning techniques can be applied towards computer architecture problems. We also provide a detailed survey of computer architecture research that employs different machine learning methods. Finally, we present some future opportunities and the outstanding challenges that need to be overcome to exploit full potential of machine learning for computer architecture. Recently, the use of machine learning in meteorology has increased greatly. While many machine learning methods are not new, university classes on machine learning are largely unavailable to meteorology students and are not required to become a meteorologist. The lack of formal instruction has contributed to perception that machine learning methods are 'black boxes' and thus end-users are hesitant to apply the machine learning methods in their every day workflow. To reduce the opaqueness of machine learning methods and lower hesitancy towards machine learning in meteorology, this paper provides a survey of some of the most common machine learning methods. A familiar meteorological example is used to contextualize the machine learning methods while also discussing machine learning topics using plain language. The following machine learning methods are demonstrated: linear regression; logistic regression; decision trees; random forest; gradient boosted decision trees; naive Bayes; and support vector machines. Beyond discussing the different methods, the paper also contains discussions on the general machine learning process as well as best practices to enable readers to apply machine learning to their own datasets. Furthermore, all code (in the form of Jupyter notebooks and Google Colaboratory notebooks) used to make the examples in the paper is provided in an effort to catalyse the use of machine learning in meteorology. Bias is known to be an impediment to fair decisions in many domains such as human resources, the public sector, health care etc. Recently, hope has been expressed that the use of machine learning methods for taking such decisions would diminish or even resolve the problem. At the same time, machine learning experts warn that machine learning models can be biased as well. In this article, our goal is to explain the issue of bias in machine learning from a technical perspective and to illustrate the impact that biased data can have on a machine learning model. To reach such a goal, we develop interactive plots to visualizing the bias learned from synthetic data. Transparent machine learning is introduced as an alternative form of machine learning, where both the model and the learning system are represented in source code form. The goal of this project is to enable direct human understanding of machine learning models, giving us the ability to learn, verify, and refine them as programs. If solved, this technology could represent a best-case scenario for the safety and security of AI systems going forward.\n"
     ]
    }
   ],
   "source": [
    "# Affichage de la taille du corpus\n",
    "nombre_docs = len(new_df)\n",
    "print('le nombre de docs est:',nombre_docs)\n",
    "\n",
    "# Nombre de mots pour chaque docs en utilisant le separateur espace \" \"\n",
    "new_df['nb_mots'] = new_df['texte'].apply(lambda x: len(x.split(\" \")))   \n",
    "\n",
    "# Nombre de phrases pour chaque docs en utilisant le separateur point ' . '\n",
    "new_df['nb_phrases'] = new_df['texte'].apply(lambda x: len(x.split(\".\")))\n",
    "\n",
    "# Affichage du Nombre de mots et de Phrases\n",
    "print(new_df[['id', 'nb_mots', 'nb_phrases']])\n",
    "\n",
    "# Supprimer les documents trop petits(<20 caracteres), sauf que dans notre DataFrame on a pas de document trop petit < 20 carateres \n",
    "new_df = new_df[new_df['texte'].str.len() >= 20]\n",
    "\n",
    "print('Le nouveau nombre de docs est :',len(new_df))\n",
    "\n",
    "#Creer une chaine de caractere unique contenant tous les documents grace a la fonction join et le separateur espace \" \"\n",
    "texte_join = \" \".join(new_df[\"texte\"])\n",
    "print('La Chaine de caractere unique contenant tous les documents : \\n',texte_join)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
